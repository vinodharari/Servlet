id,logged_by,logged_on,to_address,category,subject,description,job_id,farm,status
1,PWARRIER,"2014-05-09 07:00:00",NULL,Others,"Create Shared Repo for SmartClone REL9 ST5 GSI setup","Need to create a share repo from slcai659 and slcai656",,,Closed
2,JKUTTAPP,"2014-05-10 07:00:00",NULL,Others,"Server Pool slcal070 /OVS not configured properly","For the server pool slcal070, Cluster storage (/OVS) is not configured properly. In OVM manager it shows ""No Data available"" for memory value.",,OPC_QA_SHARED,Closed
3,JKUTTAPP,"2014-05-11 07:00:00",NULL,Others,"Server Pool slcai659 /OVS not configured properly","For the shared server pool slcai659, cluster storage (/OVS) is not configured properly. Not able to view the memory of this pool from OVM manager.",,OPC_QA_SHARED,Closed
4,SOSHETTY,"2014-05-12 07:00:00",NULL,Others,testing,"demo to raghu",,OPC_QA_FARM,Closed
5,PWARRIER,"2014-05-14 07:00:00",NULL,Others,"VMs created shows status as Running but is not pinging","REL9 ST5 GSI Master deployment = > FADeployService task is completed and all VMs appear to be  up and running  from OVMM.

 

But during  GetFaServiceDeploymentStatusTask, its unable to connect to slc04sxq.us.oracle.com and is just reiterating.

 

I tried pinging the host and same result. 

 

Post maintenance, we are frequently hitting this issue now, irrespective of ST3, ST5. Can you please look into this?

If you think it's not environment issue we can raise bug accordingly.",,OPC_QA_SHARED,Closed
6,SOSHETTY,"2014-05-14 07:00:00",NULL,Others,"Configure shared repo on slcal040 and slcal041","Configure shared repo on slcal040 and slcal041",,OPC_QA_SHARED,Closed
7,PWARRIER,"2014-05-14 07:00:00",NULL,Others,"VMs created on Hypervisor slcah811 shows status as Running but is not pinging","VMs created on Hypervisor slcah811 shows status as Running but is not pinging. Even manually created VMs also the same behavior. The hypervisor was re-imaged but with the same issue",,OPC_QA_FARM,Closed
12,VNALLU,"2014-05-15 07:00:00",NULL,Others,"Need link to OVM topology Spec from Job page","Hi,

Need to have link on OVM topology displayed on Jobs listing  page.

This can be a low priority task.",,,Closed
8,JKUTTAPP,"2014-05-14 07:00:00",NULL,Others,"VM creation fails in slcal070","In the server pool slcal070 where shared storage is used, vm creation is failing.",,OPC_QA_SHARED,Closed
9,JKUTTAPP,"2014-05-14 07:00:00",NULL,Others,"VM creation fails in slcal040","In the server pool slcal040 where shared storage is used, vm creation is failing.",,OPC_QA_SHARED,Closed
29,JKUTTAPP,"2014-06-02 07:00:00",NULL,Others,"Request to update DB for slc01iyn memory value","Can you please increase the allocated memory of the host ""slc01iyn"" from 8192 to 16384?",19911974,ovm_general,Closed
10,VNALLU,"2014-05-15 07:00:00",NULL,Others,"REgister OVAB templates block need fix","remove the directory only if its empty.

code tries to unmount the /OVS/seed_pool_<dteid_hyp> and then removes any content in it.

if it cannot unmount it just deletes the content causing issues.",,,Work-in-Progress
11,VNALLU,"2014-05-15 07:00:00",NULL,Others,"REgister OVAB templates block need fix","In case Hypervisor is down, mounting wil not happen and then copy would be attempted on local directory which is incorrect.",,,Work-in-Progress
13,VNALLU,"2014-05-16 07:00:00",NULL,Others,"Clone Topology is not working","Looks like after upgrade clone topology is not working as the directory structure is changed, Its not able to identify the resource page.",,,Closed
14,MMAHAPAT,"2014-05-19 07:00:00",NULL,Others,"Reg:Big IP mapping changes to include without ""-""","Currently the bigip mapping is set up as follows :
sdiqa-v0028-crm.vfarm.oraclecorp.com however this is supported for fa request.Currently with the jcs -fa integrtion other cloud services which majorly get triggered through cloud ui does not allow ""-"" and the supported format is sdiqav0028-crm.vfarm.oraclecorp.com

We need support for the following :
1.FA requests to be processed with out a ""-"" changes to the mapping.
2.Big ip mappings without a ""-"" to take care of JCS request.",19525004,,Closed
15,SOSHETTY,"2014-05-20 07:00:00",NULL,Others,"Reg:Big IP mapping changes to include without ""-"" v0072","Currently the bigip mapping is set up as follows : sdiqa-v0072-crm.vfarm.oraclecorp.com however this is supported for fa request.Currently with the jcs -fa integration other cloud services which majorly get triggered through cloud ui does not allow ""-"" and the supported format is sdiqav0072-crm.vfarm.oraclecorp.com We need support for the following : 1.FA requests to be processed with out a ""-"" changes to the mapping. 2.Big ip mappings without a ""-"" to take care of JCS request.",,,Closed
16,VNALLU,"2014-05-20 07:00:00",NULL,Others,"HC not reporting correct status for SDI base job","Hi, for job 19670853, i see that sdi server is in failed not restartable state and i cannot perform FA deployment but HC did not return error state and it shows LIVE. Also on clicking LIVE link am not able to see the report",,,Closed
17,CSBERNAR,"2014-05-20 07:00:00",NULL,Others,"Storage cleanup script picks only 1 matching share for a DTE job","Reported by SOSHETTY:
Storage cleanup did not pick up multiple shares of DTE ID 19186216. Only picked 19186216REL9ST3GSI.
Storage: slcnas570 / sdiqa",19186216,OPC_QA_FARM,Closed
18,MMAHAPAT,"2014-05-20 07:00:00",NULL,Others,"big ip configuration for JCS setup","We would need bigip configuration for jcs and https to work for jcs here are the details :
java ohs :=slc03yzc.us.oracle.com 
sdi host :slc05ven",19326480,,Closed
19,JKUTTAPP,"2014-05-21 07:00:00",NULL,Others,"Please make a ServerPool using adcgdc16,adcgdc17","Please create a ServerPool combining adcgdc16 and adcgdc17",,ovm_general,Closed
20,JKUTTAPP,"2014-05-21 07:00:00",NULL,Others,"Please make a ServerPool using adcgdb03,adcgdb06,adcgbf23","Please create a ServerPool using adcgdb03,adcgdb06,adcgbf23",,ovm_general,Closed
21,MMAHAPAT,"2014-05-22 07:00:00",NULL,Others,"big ip configuration for JCS setup- env 2","As we had the big ip mapping for the other environment ,please map the following 

Rule is changes to *v0028-jcs.vfarm*   { pool v0028-external   } and member with is created for v0028-external

java ohs host :slc03yzf.us.oracle.com",19525004,OPC_QA_FARM,Closed
22,CSBERNAR,"2014-05-22 07:00:00",NULL,Others,"UCF Batch2 - Add hypervisors to QAFARM","Add the following hypervisors to QAFARM
slcan515
slcan516
slcan517
slcan518
slcan519
slcan520
slcan521
slcan522
slcan523",,,Closed
23,CSBERNAR,"2014-05-22 07:00:00",NULL,Others,"UCF Batch2 - Create shared repo and add hypervisors to OPC_QA_SHARED","Create shared repo and add the following hypervisors to OPC_QA_SHARED

slcan524
slcan525
slcan526 (master)",,,Closed
24,SWSUDHAK,"2014-05-22 07:00:00",NULL,Others,"slcai054 vm issue","We faced issues with slcai054 Hypervisor. During rehydration vm creation was not smooth and shown as ""Creating"" in OVM UI and timeout exception occurred and vm creation failed.",19670853,OPC_QA_FARM,Closed
25,PWARRIER,"2014-05-23 07:00:00",NULL,Others,"VM not responding on slcai659. Need to restart hypervisor","Issue resolved. SR to track the request.",,,Closed
26,SWSUDHAK,"2014-05-23 07:00:00",NULL,Others,"Add few Ip's to ohs_reserved","Please add few Ip's to farm ohs_reserved. we need to use slcai055 for RAC db creation",19670853,,Closed
27,SWSUDHAK,"2014-05-24 07:00:00",NULL,Others,"Unable to shutdown slc08dpm VM","During SDI cancel request i notice slc08dpm VM is not shutting down. Please take a look and resolve. Once resolved we will cancel and re-trigger master.",19670853,OPC_QA_SHARED,Closed
28,MMAHAPAT,"2014-05-27 07:00:00",NULL,Others,"big ip configuration for fa extrenal urls not working","ROHS Host :slc03yvo
url :https://sdiqav0072-fs.vfarm.oraclecorp.com/homePage/faces/AtkHomePageWelcome

Rule is changes to *v0072-jcs.vfarm*   { pool v0072-external   } and member with 10.244.160.195:7777 is created for v0072-external",,,Closed
30,HUKUMAR,"2014-06-02 07:00:00",NULL,Others,"confiure java /fa ohs on same host","ROHS:- slc03yvk.us.oracle.com
Deployer host:- aime@slc03wcf
vncdetails:- slc03wcf:2/welcome",19525004,OPC_QA_FARM,Closed
31,SOSHETTY,"2014-06-03 07:00:00",NULL,Others,"ocfs2 shared repo on slcai659 slcai656","ocfs2 shared repo on slcai659 slcai656",,OPC_QA_SHARED,Closed
32,AMATHURA,"2014-06-04 07:00:00",NULL,Others,"Job status shows as WAITING_FOR_FREE_HV even though Free HV is available","I tried submitting the GSI_HA_Rel9_St6 setup which needs ""744451 MB RAM"" in fastha pool:

perl ovm_farm_manager.pl -o listhv -farm fastha
host_name       avl_mem tot_mem avl_cpu tot_cpu avl_vm  max_vm  gate_way        farm            datacenter      status
--------------------------------------------------------------------------------------------------------------------
slcaj502        750000  750000  1000    1000    48      48      10.241.112.1    fastha  slc     UNLOCKED",19974926,fastha,Closed
33,VIKUKUMA,"2014-06-10 07:00:00",NULL,Others,"slcai658 ->DomUVol-OVS unmounted after reboot","In Hyps slcai658 ,DomUVol-OVS got unmounted after reboot .

Could you please help to mount it back.",,OPC_QA_FARM,Closed
34,SWSUDHAK,"2014-06-10 07:00:00",NULL,Others,"Could not release slcai656","Tried to release vm slcai656 with force
/ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o release -host_name slc04sxu -force
Debug: need to stop and deregister from OVS
ERROR: unknown VM status: Server pool ""slcai656"" does not exist.","19670853 ",OPC_QA_SHARED,Closed
35,KRAMANJA,"2014-06-11 07:00:00",NULL,Others,"Job - 20069785 is waiting for resources and eventually cleaned up after timeout","I have added three hyp to warroom sub farm, two of them are setup shared pool.
Hostname   -   pool name
slcaj503      -   slcaj503(Master)
slcaj504      -   slcaj503
slcai550      -   slcai550

Triggered SDI 14.1.3 HCM with HA topo, still it fails with "" watiing for resources"" and then after timeout it gets cleaned.

Can you please look into this?
Job id - 20069785",20069785,warroom,Closed
36,PWARRIER,"2014-06-11 07:00:00",NULL,Others,"11204 DB templates not active on hypervisors in opc_qa_farm","Error: template: OVM_OL6U4_X86_64_11204DBRAC_PVM is not in active status on hypervisor slcag039",,OPC_QA_FARM,Closed
37,ARBATHIN,"2014-06-12 07:00:00",NULL,Others,"20091985  - failed waiting for HV and timedout","20091985  - failed waiting for HV and timedout

While the job was running, when i monitored , i could see 16 VM's status changed to registered 
[aime@slcaa699 farmmanager_logs]$ perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o listvm -farm warroom | grep REGISTERED | wc -l
16

Currently i ran another JOB just to see if issue still comes up","20091985 ",warroom,Closed
38,JKUTTAPP,"2014-06-14 07:00:00",NULL,Others,"Request to update DB for slc01dvr and slc03dox memory","Hi Sowmya,
         For IP more memory is required for node slc01dvr and slc03dox. I have increased the memory values. Can you please update the farm db?
slc01dvr old(87040)  new(107040)
slc03dox old(61440)  new(80440)",19392265,ovm_general,Closed
39,VNALLU,"2014-06-16 07:00:00",NULL,NULL,"Need to write a API in farm manager to increase memory","We need to find out if its feasible to write a API to increase physical memory of a VM and update the same in farm DB.

For upgrade testing, we might need to be at 'x' memory level and then later upgrade to 'y' memory level to perform upgrade. This cannot be handled in our automation with existing infrastructure.",,,Open
40,AMALON,"2014-06-16 07:00:00",NULL,Others,"No BIG IP Available Error","Hello,

When trying to add BIG IP to dte job 19326478, I receive no big ip available error. 

$/ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o getbigip -dteid 19326478
ERROR: No free bigips available!

The big ip here will be used for GSI Rel8 GA HA deployment  with slcal039 and slcal065 hypervisors.",19326478,QAFARM,Closed
41,SWSUDHAK,"2014-06-16 07:00:00",NULL,null,"want Hypervisor for 11.2.0.4 DB","Tried to take hyp slcai657
Getting error:

Enter RAC version 11202/11203/11204: default [11203]: 11204
Error: template: OVM_OL6U4_X86_64_11204DBRAC_PVM is not in active status on hypervisor slcai657",19670853S,OPC_QA_FARM,Closed
42,ARBATHIN,"2014-06-16 07:00:00",NULL,Others,"Request to configure shared repository with master slcaj503 and VM server slcaj504","Request to configure shared repository with master slcaj503 and VM server slcaj504",,warroom,Closed
43,SIDHJAIN,"2014-06-16 07:00:00",NULL,Others,"FA service url not working on slc03wcf","On SDI host : slc03wcf , FA_CRM service url for the request below is not working. We're not using ""-"" in system name.

JOB19525004CRM2REL8svc | FA_CRM    | BASIC     | 2014-06-11 09:18:59 Z | 2014-06-11 21:43:39 Z | https://sdiqav0028REL8-crm.vfarm.oraclecorp.com/customer/faces/CrmFusionHome

Can someone from the farm manager team please take a look.

-- 
Sidharth",19525004,OPC_QA_FARM,Closed
44,SOSHETTY,"2014-06-17 07:00:00",NULL,Others,"move 3 hosts for waroom from QAFARM","move 3 hosts for waroom from QAFARM",,QAFARM,Closed
45,SIDHJAIN,"2014-06-17 07:00:00",NULL,Others,"Unable to ssh to vm slc03yzf.us.oracle.com","The vm slc03yzf seems active from the OVM manager. Ping to this vm is successful but unable to ssh. 

This host is used has Java OHS for SaaS-PaaS Integration work.",,OPC_QA_FARM,Closed
46,NVPRABHU,"2014-06-17 07:00:00",NULL,Others,"Increase physical memory","Need to increase physical memory on Midtier hosts as below:
slc05jml.us.oracle.com: Available: 12.7G, required: 16G. 
slc05jnr.us.oracle.com: Available: 31.4G, required: 40G. 
slc05jmj.us.oracle.com: Available: 31.4G, required: 40G. 
slc05jmk.us.oracle.com: Available: 59.8G, required: 82G.
slc05jnn.us.oracle.com: Available: 31.4G, required: 40G.
slc05jnq.us.oracle.com: Available: 59.8G, required: 82G.",19888582,smart_clone,Closed
47,KRAMANJA,"2014-06-17 07:00:00",NULL,Others,"DTE job:20172848  Failed while Creating DEPLOYER_HOST","DTE job 20172848 has Failed while Creating DEPLOYER_HOST.

Seeing following errors in the",20172848,warroom,Closed
48,KRAMANJA,"2014-06-19 07:00:00",NULL,Others,"For Warroom Pool:  ERROR: No free bigips available!","CRM Rel9-ST7  job(20191838)
 failed with following error.

 ERROR: No free bigips available!",20191838,warroom,Closed
49,KRAMANJA,"2014-06-19 07:00:00",NULL,Others,"DEPLOY_TEMPLATE block has failed  with not enough memory for VM","DEPLOY_TEMPLATE block has failed with the following error.

An error occurred: There is not enough free memory predicted to be available to start VM 'slc05bth_us_oracle_com'.
Total memory needed in OVMM pool 'slcaj503' on host 'slc02oxi.us.oracle.com' is 126464MB, but there is only 124883MB available.",20191648,warroom,Closed
50,KRAMANJA,"2014-06-19 07:00:00",NULL,Others,"Enhancement request: Please provide support for incremental run of preparehyp.sh","Hi,

Currently we are not able to successfully run preparehyp.sh  when some the requirements are already met in the given hypervisor.

We are commenting the exit statement in preparehyp.sh  and continue.


Please enhance preparehyp.sh so that if a requirement is already met in the Hypervisor, the script can go ahead with a warning instead of failure.",Enhancemen,warroom,Closed
62,SIDHJAIN,"2014-07-04 09:28:03",NULL,Hypervisor,"VM's on hypervisor slcah811 not reachable after creation","Created two vms slc03wsn,slc03wom. After starting the vm with farm manager (-o startvm). Both vms are not pinging, but show running on OVM manager.",,OPC_QA_FARM,Closed
51,AMATHURA,"2014-06-20 07:00:00",NULL,Others,"Job stuck @ Creating Deployer host for past 9 hours","I added two new hyps - slcah814 and slcah784

I tried creating one host manually and it went through fine

Pls help",20215279,fastha,Closed
52,VIKUKUMA,"2014-06-20 07:00:00",NULL,Others,"Mounting issue with  hypervisor  slcai658","SDI 14.1.5 REL8 crm setup Register OVAm templ block failed due to below error -

mount: slcai658:/OVS/seed_pool failed, reason given by server: Permission denied",20211188,OPC_QA_FARM,Closed
53,MMAHAPAT,"2014-06-20 07:00:00",NULL,Others,"slc05vej.us.oracle.com unable to ssh","Hi ,
for the idm host slc05vej.us.oracle.com,we are not able to ssh ,as such this might be due to multiple connections ,however to kill the connections we would require root access for this we would need a ssh or vnc which is not avaible .",,QAFARM,Closed
54,NKATARLA,"2014-06-20 07:00:00",NULL,Others,"Not able to start VM on Hypervisor slcag794","Created a VM (slc03pgh) on Hypervisor slcag794 in the Server Pool OPC_QA_FARM. But, not able to start it. Seems that the Hypervisor is in bad state. Can you pls. look into it?",,,Closed
55,KRAMANJA,"2014-06-22 07:00:00",NULL,Others,"Need another big ip for warroom pool","Hi,

Currently we have only one big ip available in War room pool.

Already triggered a Rel7-HCM-HA dte job:20250959.

We will trigger one job for Rel8  env.  Pleast allot a another big ip in warroom pool.

Thanks.",,warroom,Closed
56,AMATHURA,"2014-06-22 07:00:00",NULL,Others,"Cleanup stuck","Cleanup stuck @ following :

<INFO> 21/06/2014 11:12:23 -> slc03wqv is never created

<INFO> 21/06/2014 11:12:23 -> Checking slc03wqv_us_oracle_com for cleanup

Pls help",20215279,fastha,Closed
57,MMAHAPAT,"2014-06-23 07:00:00",NULL,Others,"fa admin host not accesible slc06xys -reboot also fails","Hi ,
fa admin host not accesible slc06xys was failing with following error :
[root@slc06xys ~]# 
Message from syslogd@slc06xys at Jun 23 09:31:16 ...
 kernel:do_get_write_access: OOM for frozen_buffer tried to reboot ,server starts up but cannot ssh or access the host .",,QAFARM,Closed
58,VNALLU,"2014-06-23 07:00:00",NULL,NULL,"nfslock goes down or hangs in hypervisor","Orch job fails as we mount device from hypervisor and nfs is hanging there. Just restart nfs and it works. Need to identify the cause and fix it.",,,Open
59,KRAMANJA,"2014-06-24 07:00:00",NULL,Others,"job: 20257446 -> OSN host not allocated for Rel7 env setup","faovm deploy is failing with the following error.


 Verifying Hosts for Deploy ... An error occurred: [HOST_OSN=%OSN_HOST%.us.oracle.com should exist] {Unable to find host}","20257446 ",warroom,Closed
60,SIDHJAIN,"2014-06-24 07:00:00",NULL,Others,"ZFS mounts are read-only on JCS host slc06xxs","Following mounts with mount points on jcs host slc06xxs have become read-only.

 slcnas570:/export/jcs_commonshare_env2/js_mw1/javacloud_14_1_0_0__140321_0606 on /middleware  
 slcnas570:/export/jcs_commonshare_env2/js_tools on  /cloud9/tools  
 slcnas570:/export/jcs_commonshare_env2-0001/virt24009_domains  on  /domainsroot",,QAFARM,Closed
61,JKUTTAPP,"2014-06-25 07:00:00",NULL,Others,"Resource allocation fails in ovm_general","Resource allocation is not happening properly in ovm_general pool.
For the Job ID: 20290393, when checked using ""listdte"" command, it shows resources are allocated.
But farm ui and worker host logs shows ""WAITING_FOR_FREE_HV"". Can you please check?",20290393,ovm_general,Closed
63,USKUMAR,"2014-07-07 09:25:07",NULL,Others,"Can't create slc01yvn which already exist :: status is: (111, 'Connection refused')","While creating RAC DB, existing VMs are picked for DB creation and hence RAC DB creation fails with below error.

ERROR: Can't create slc01yvn which already exist :: status is: (111, 'Connection refused')
Error: node1 failed to create

Can you have a look on this.

Same error occurred on adcgdc16 hyp too",,ovm_general,Closed
64,MULIEGAD,"2014-07-09 15:19:31",NULL,Cleanup,"Job 20152586 failed to cleanup","Job 20152586 failed to cleanup and displays below log. Can you please help me to clean this job.
Have used the cleanup id 20152586_5rah6s7u4r5duij1422ro21l70 shared by venkatesh.


Can't bring down  on a timely manner. Something is Wrong
ERROR: VM:  slc06xwt_us_oracle_com cleanup failed
ERROR: Unable to release VM: slc06xwt
INFO: releasing VM slc06xwz
slc06xwz is never created",20152586,OPC_QA_SHARED,Closed
65,JKUTTAPP,"2014-07-10 05:16:20",NULL,Cleanup,"SDI cleanup job failed","For the FA job 20195078 on SDI Base, cleanup is failing to run DEPLOY_DELETE.
Log: Error Detected in Log File --> /net/adc6181303/scratch/aime/work/custom_cleanup.log

+ echo 'Deploy Delete block has failed. Hence skipping the zfs storage deletion.'
Deploy Delete block has failed. Hence skipping the zfs storage deletion.

SDI Deployer host: slc03eeo","20195078 ",OPC_QA_SHARED,Closed
66,AMATHURA,"2014-07-14 12:42:57",NULL,Cleanup,"Cleanup failed for job 19990372","Cleanup failed for job 19990372. Pls help.",19990372,fastha,Closed
67,AMATHURA,"2014-07-14 18:25:52",NULL,Cleanup,"HCM Rel9 HA 8.1 Setup failed","HCM Rel9 HA 8.1 Setup failed with below errors:

Add Rack error: javax.ejb.EJBException: EJB Exception: : java.lang.IllegalArgumentException: 12 address(es) already exist. example: 10.241.116.194",20560458,fastha,Closed
68,SWSUDHAK,"2014-07-15 03:49:52",NULL,VM,"slc04sxu_us_oracle_com could not be shutdown","During cancel request it failed due to socket exception as FA vm slc04sxu.us.oracle.com was not responding.But it showed Running in OVMM. We tried to restart and getting error message ""Can't bring down  on a timely manner. Something is Wrong""
can you help bringing up this vm so we can proceed with cancel request.",19670853,OPC_QA_SHARED,Closed
69,NKATARLA,"2014-07-15 08:16:05",NULL,Hypervisor,"Cleanup Failed","The cleanup of the SDI Job 20554499 failed.

Can you pls. look into it?",20554499,OPC_QA_FARM,Closed
70,AMATHURA,"2014-07-16 05:40:17",NULL,Others,"Job 20579392 failed while creating deployer host","Deployer host (slc03wkz) is already created in hyp slcah783 in ovmm. We are seeing issues everytime when we add a new hypervisor with VM Creation. Pls help.",20579392,,Closed
71,SOSHETTY,"2014-07-16 14:20:06",NULL,Others,"Big ip request porting team","2 big ip request for 2 weeks by senthil.arumugam@oracle.com",,,Closed
72,NKATARLA,"2014-07-17 04:18:00",NULL,Others,"Error while starting VM","The Nuviaq Host VM got registered successfully, but couldn't be started.

slc04swr	10.240.195.10	slcai656	4096	2	10.240.192.1	REGISTERED	20592424	OPC_QA_SHARED	

Powering on.
resetting slc04swr 
Virtual machine status has been reset.
Powering on.
Can't bring on  on a timely manner. Something is Wrong",20592424,OPC_QA_SHARED,Closed
73,AMATHURA,"2014-07-17 13:55:54",NULL,Others,"Pls allocate more Big IPs for farm fastha","Pls allocate more Big IPs for farm fastha",,fastha,Closed
74,NKATARLA,"2014-07-17 14:57:08",NULL,Cleanup,"SDI Job 20592457 Cleanup Failed","The SDI Job 20592457 cleanup failed, as the RACDB_DETAILS.properties file not found in /scratch/aime/work location on Deployer Host slc03wfj.

How do we cleanup in such scenarios? 

Pls. do the needful.",20592457,OPC_QA_FARM,Closed
75,SWSUDHAK,"2014-07-18 06:10:34",NULL,Cleanup,"19670853 -Job Cleanup Failed","19670853 - Job cleanup Failed.
Request to cleanup this",19670853,OPC_QA_FARM,Closed
76,NKATARLA,"2014-07-18 10:37:17",NULL,Cleanup,"Job Cleanup Failed","The FA Deploy Job 20621280 cleanup failed.

Pls. do the needful.",20621280,OPC_QA_FARM,Closed
77,AMATHURA,"2014-07-18 14:07:18",NULL,VM,"Setup is failing during RAC creation due to time out issue even though VM is created and in running","HA Setup is failing during RAC creation due to time out issue even though VM is created and in running state. Listener logs in hypervisor (slcah810) is updated successfully though.",20616630,fastha,Closed
78,AMATHURA,"2014-07-18 18:53:14",NULL,Cleanup,"Cleanup failed for job 20261861","SDI Req slc05jji shows as success for deletion - JOB217825Del       | DELETE       | [SYSTEM, FA_CRM] | COMPLETED     | sdiqa20261861-v0068 | 2014-07-18 07:30:03 -07:00 | 2014-07-18 08:02:21 -07:00 
Location of Log - /net/slc03wqv/scratch/aime/work/oracle/work/DEPLOY_DELETE/logs/request_delete.log

Old Rack and FADBs are not removed in SDI. Is this expected?",20261861,fastha,Closed
79,NKATARLA,"2014-07-22 08:15:32",NULL,Cleanup,"FA Deploy Job 20660523 Cleanup Failed","The FA Deploy Job 20660523 Cleanup Failed.

Pls. do the needful.",20660523,OPC_QA_FARM,Closed
80,SWSUDHAK,"2014-07-22 12:49:09",NULL,Others,"Master Setup failed due to SetupRAC task in job#20676847","Master job is failing second time at task SETUP_RAC_FOR_DEPLOYMENT.dif for job# 20676847
And 11.2.0.4 DB creation is failing when it tries to invoke farm createrac command

logs:
/net/slc04swt.us.oracle.com/scratch/aime/PFA_REMOTE/job_20676847/work/oracle/work/SETUP_RAC_FOR_DEPLOYMENT/logs/build_rac_22_07_2014_02_57_05_84.906264408195.log
SDI_PARENT_DTE_ID=20621298",20676847,OPC_QA_SHARED,Closed
81,AMATHURA,"2014-07-22 15:22:38",NULL,Others,"Following Roles are missing for FAAdmin user in Non SDI OVM Setups","Following Roles are missing for FAAdmin user in Non SDI OVM Setups

Application Diagonistics Administrator
Application Implementation Consultant
IT Security Manager

In our local setups, we could see the above roles. Could you pls confirm whether this restriction is specified anywhere in ovm-ha-deploy.properties?",20632842,fastha,Closed
82,AMATHURA,"2014-07-22 16:36:18",NULL,Others,"Pls allocate more Big IPs for farm fastha","Pls allocate more Big IPs for farm fastha",,fastha,Closed
83,KRAMANJA,"2014-07-23 10:37:54",NULL,Hypervisor,"job-20697309  was hung  at  CREATING_DEPLOYER_HOST","This is the listener log on hypervisor.  Nothing updated in the last 45 mins

/scripts/changevm.sh  command is successful.  But there after no update in the log.","20697309  ",warroom,Closed
84,NKATARLA,"2014-07-24 05:25:18",NULL,Cleanup,"FA Deploy Job 20678375 Cleanup Failed","The FA Deploy Job 20678375 cleanup failed.

Pls. do the needful.",20678375,OPC_QA_FARM,Closed
85,NKATARLA,"2014-07-25 00:56:49",NULL,Cleanup,"Job Cleanup Failed","The FA Deploy Job 20713810 cleanup failed.

As suggested, we've modified our Topo to copy the RACDB_DETAILS.properties file to /scratch/aime/work location on Deployer Host. Still, no luck.

Pls. do the needful.",20713810,OPC_QA_FARM,Closed
86,MULIEGAD,"2014-07-25 10:05:23",NULL,Cleanup,"Cleaning of job 20696947 is in CLEANING_INITIATED state from long","Had initiated cleanup for job 20696947 yesterday but it is in CLEANING_INITIATED",20696947,OPC_QA_FARM,Closed
87,MULIEGAD,"2014-07-25 17:20:27",NULL,Hypervisor,"Unable to shutdown VM;s of job 20680406","We are unable to bring down OVM's which is part of job 20680406.
slc04sxo.us.oracle.com
slc04sxi.us.oracle.com
slc04sxh.us.oracle.com
slc04sxg.us.oracle.com
slc04sxb.us.oracle.com
slc04svp.us.oracle.com
slc04svo.us.oracle.com
slc04svn.us.oracle.com
slc04qft.us.oracle.com 

The farm command to shutdown erros out as below:
WARNING:the vm is in invalid state. Can't bring it DOWN now 

Please help us to resolve this.",20680406,OPC_QA_FARM,Closed
88,RINSHARM,"2014-07-28 06:32:52",NULL,Hypervisor,"DB hosts not coming up for job id 20571994","hi All,

DB hosts RAC_NODE1_HOST=slc03zao.us.oracle.com 				
				 				 				 RAC_NODE2_HOST=slc03zan.us.oracle.com 				
				 				 				 RAC_NODE1_VIP_HOST=slc03zam.us.oracle.com 				
				 				 				 RAC_NODE2_VIP_HOST=slc03zal.us.oracle.com 

are not coming up for job id 20571994 

Hypervisors for these hosts are not pingable.

Please have a look.",,OPC_QA_FARM,Closed
89,SWSUDHAK,"2014-07-28 10:09:25",NULL,Hypervisor,"Facing issues while starting vm's for 20621298","Tried both start env as well as startvm command.
But facing issues
/net/slc03rwj/scratch/aime/QAFarm/ovm_farm_manager_v7.pl -o start_env  -envid 20621298
INFO: Starting Up VM : slc03wcm
Powering on.
Can't bring on  on a timely manner. Something is Wrong

/net/slc03rwj/scratch/aime/QAFarm/ovm_farm_manager_v7.pl -o startvm -host_name slc03wcm
resetting slc03wcm
Virtual machine status has been reset.",20621298,OPC_QA_FARM,Closed
90,NKATARLA,"2014-07-28 11:55:31",NULL,VM,"VMs Not Coming Up","The below VMs are not coming up after starting the SDI Base Environment 19967913.

SFTP_HOST=slc03edn.us.oracle.com 
HOSTED_ROUTING_OHS_HOST=slc03edm.us.oracle.com 
FR_1_EMGC_HOST=slc01dur.us.oracle.com 

Pls. do the needful.",19967913,ovm_general,Closed
91,VIKUKUMA,"2014-07-28 15:11:18",NULL,Hypervisor,"Hyp slcah764.us.oracle.com is not accessible","Hypervisor slcah764.us.oracle.com is not accessible

After quarterly maintenance .

Please help to bring up  slcah764",,OPC_QA_FARM,Closed
92,VNALLU,"2014-07-30 09:00:24",NULL,NULL,"cleanup sdi_base and sdi_fa table","Post QM, we need to clean these tables as few initiated cleanalldte manually and cleaned the shares. these tables would not ahve been updated.

Raising ticket for ourself to track this.",,,Open
93,JKUTTAPP,"2014-07-31 06:27:28",NULL,NFS,"""List/Edit OVM Topology"" link not responding in Farm UI","""List/Edit OVM Topology"" link in UI is not responding.
All other links are working fine.
Please take a look",,,Closed
94,MMAHAPAT,"2014-08-01 05:57:12",NULL,Cleanup,"Cleanup of job 19525004","The job 19525004 is stuck in cleaning status or couple of days now .Please help cleanup the job to clear some resources.",19525004,OPC_QA_FARM,Closed
95,PWARRIER,"2014-08-05 01:50:26",NULL,Others,"help to clean the pending jobs on smart_clone in the name of pwarrier","Hi ,

Looks the DTE command executed appended some invalid character for toposetmemberid and has triggered a job for all the members in the toposet and all are waiting for approval. I am unabel to abort the DTE job nor i have any key for cleaning up this. Please help to cleanup this. There are 17 of them and I am unable to paste those here.",,,Closed
96,PWARRIER,"2014-08-06 04:08:43",NULL,Others,"JOBs triggered on SDI base JOB ID 20621298 fails with proxy issue","FA only topo triggered on SDI base JOB ID 20621298 fails even if the proxy is up.",,OPC_QA_FARM,Closed
97,PWARRIER,"2014-08-06 09:20:54",NULL,Hypervisor,"slcai659 is shown as Inactive during rehydartion","Rehydration fails with error that slcai659 server pool is Inactive",,OPC_QA_SHARED,Closed
98,MULIEGAD,"2014-08-08 03:03:24",NULL,Cleanup,"Clean up of job 20698433 is going on infenitely.","Clean up of job 20698433 is going on infinitely.
Please help us to clean this job to start a new one.",20698433,smart_clone,Closed
99,SWSUDHAK,"2014-08-08 09:30:58",NULL,Hypervisor,"SocketTimeoutException seen in slcai656/slcai659","We are facing issues during ALLOCATE_RESOURCES_FOR_FA_OSN task with this hypervisor (slcai659/slcai656) ? OPC_QA_SHARED

We saw this consistently 3 times. Even delete/add rack did not help us.

Have locked the hypervisors so that these should not get picked up in new master job.",,OPC_QA_SHARED,Closed
100,KRAMANJA,"2014-08-08 10:33:47",NULL,Others,"ER: to enable Archive logging by default","This is enhancement request to enable archive logging in the RAC database by default.

For taking hot backups, we need to have archive logging enabled.",208623,warroom,Closed
101,PWARRIER,"2014-08-09 11:50:44",NULL,Cleanup,"Cleanup of Master environment failed","Applied the workarounds and managed to initiate the cleanup of deployments, but looks like unregistering is failing for job
20508806	OPC_QA_SHARED	muliegad	CLEANUP_FAILED	SDI_FA_REL9_GSI	N/A",,OPC_QA_SHARED,Work-in-Progress
102,VIKUKUMA,"2014-08-11 06:47:36",NULL,Others,"Need to define new rule for cloudUI based setup","In our SDI topo we were using identity domains like sdiqa-v0035 , but when using cloudUI it generates system names like ones i have shared below.
The Atkpage url generated for an FA using cloudUI will also be different ( with identity domains of format i.e usoraclecorptrialXXXX,usoraclecorpXXX ). 

Can this be achieved with the existing bigip config DTE block ? or would require additional configuration from your side.",,,Work-in-Progress
103,SWSUDHAK,"2014-08-12 07:48:53",NULL,Others,"Clicking on transfer button not working","i want to transfer a job to priya using qa farm
when i click on transfer icon , i see the page is not loading.","20959565 ",OPC_QA_SHARED,Closed
104,SRMEKA,"2014-08-12 14:57:05",NULL,Others,"Require 200gb diskspace on slc06xvm:/u01","Require 200gb diskspace on slc06xvm:/u01",,QAFARM,Closed
105,NKATARLA,"2014-08-13 04:03:31",NULL,Cleanup,"SDI Base Job Cleanup Failed","The SDI Base Job 20975474 cleanup got hung.

Pls. do the needful.",20975474,OPC_QA_FARM,Closed
106,PWARRIER,"2014-08-13 06:07:58",NULL,NFS,"Error status received while starting slc06xsf_us_oracle_com","Rehydration fails with Error status while starting slc06xsf_us_oracle_com on slcal040.",,OPC_QA_SHARED,Closed
107,AMATHURA,"2014-08-17 03:27:35",NULL,Others,"AddEmgcDiscoveryFaDBTask is failing","AddEmgcDiscoveryFaDBTask is failing in Rel8 SDI HCM HA GA env.

Pls help.",21049317,fastha,Closed
108,AMATHURA,"2014-08-18 11:06:50",NULL,Others,"Pls allocate one more Big IP...","Pls allocate one more Big IP...",,fastha,Closed
109,SRMEKA,"2014-08-20 16:39:34",NULL,Hypervisor,"slc06xvo.us.oracle.com,slc06xvq.us.oracle.com,slc06xxs.us.oracle.com,slc06xxr.us.oracle.com,slc06xxt","We did provisioned reference env on QA farm, but I am not able to access servers : slc06xvo.us.oracle.com,slc06xvq.us.oracle.com,slc06xxs.us.oracle.com,slc06xxr.us.oracle.com,slc06xxt.us.oracle.com,slc06xxu.us.oracle.com.",20576307,QAFARM,Closed
110,AMATHURA,"2014-08-21 14:55:57",NULL,Others,"Pls allocate one more Big IP for farm fastha","Pls allocate one more Big IP for farm fastha",,fastha,Closed
111,AMALON,"2014-08-24 06:10:04",NULL,Others,"No Available BIGIPs","Hello, 

Request BIGIP block failed in job 21156188 due to no free IPs. Can you please release sufficient ips for HCM HA scaleout run?",21156188,fastha,Closed
112,AMATHURA,"2014-08-25 08:51:59",NULL,Hypervisor,"Pls help in adding slcaj501 hyp to pool slcaj500","I have registered hyp slcaj500 in QA Farm OVMM. Pls help in combining the slcaj501 into server pool slcaj500.

User/Pass - ovmroot/welcome1",,fastha,Closed
113,MULIEGAD,"2014-08-25 10:24:18",NULL,Cleanup,"job 21152317 cleanup failed","Had initiated cleanup for SDI base job 21152317. But it has failed.  There is no manual intervention done for this job/jobid. We didn't use the job as it has failures on many blocks.",21152317,OPC_QA_FARM,Closed
114,SRMEKA,"2014-08-26 14:58:48",NULL,Others,"REL8 GSI DTE job not searchable, can you please check the status","/usr/local/packages/aime/dte/DTE3/bin/jobReqAgent -u srmeka -d fusionapps -r rel8 -p LINUX.X64 -v fa_mega_ovs -toposetid 12504 -toposetMemberID 55446 -s /tmp -l FAOVMINTEG_11.1.8.0.0_LINUX.X64_140814.2243 -e satyasai.meka@oracle.com -nocheckshiphome -parallel=true FAOVMINTEG_LABEL=FAOVMINTEG_11.1.8.0.0_LINUX.X64_140814.2243 OVM_TOPOLOGY=GSILARGE_REF_REL8 SHARE_NAME=srmeka_rel8_gsi_082614 ZFS_SHARE_NAME=srmeka_rel8_gsi_082614 ZFS_USER=farm_ovm ZFS_PASSWORD=farm_ovm321 SNAPSHOT_NAME=srmeka_rel8_gs",12624097,QAFARM,Closed
115,JKUTTAPP,"2014-09-01 10:04:50",NULL,VM,"non SDI deployment fails in OPC_QA_SHARED","I triggered a non SDI deployment in OPC_QA_SHARED. But after vm allocation it has failed even without creating deployer host.

Job ID 21219311",,OPC_QA_SHARED,Closed
116,SWSUDHAK,"2014-09-01 11:33:01",NULL,VM,"difs due to SHARED_IDM_SETUP failure","Shared_IDM_SETUP CreateVM is failing
Debug: create_vm slc03wcj,stit_oel5u6,slcah766,slcah766,admin,OVMadmin,welcome1,6,32768
Virtual machine ""slc03wcj"" is being created. Please check the status.
Maximum Memory Size has been changed to ""32768"".
Memory size changed to ""32768"".
Number of VCPUs changed to ""6"".
ERROR: subjobid 594750115548  failed!
Need to check why this is failing when disk space is actually present",21259695,OPC_QA_FARM,Work-in-Progress
117,JKUTTAPP,"2014-09-02 07:00:22",NULL,VM,"VM creation fails in slcal060","Start - /OVS/running_pool/243229_slc06xvk_us_oracle_com
VM deletion failed with the following error:
Delete failed. Result -failed:<CDSLockTimeout: Couldn't accquire CDS lock on 'srv.db'>

StackTrace:
File ""/opt/ovs-agent-2.3/OVSSiteUtility.py"", line 328, in rm_vm raise e",,QAFARM,Closed
118,JKUTTAPP,"2014-09-02 07:10:45",NULL,Hypervisor,"VM deletion failed in slcal060","During cleanup, VM deletion failed with the error message.

Start - /OVS/running_pool/243229_slc06xvk_us_oracle_com
Delete failed. Result -failed:<CDSLockTimeout: Couldn't accquire CDS lock on 'srv.db'>

StackTrace:
File ""/opt/ovs-agent-2.3/OVSSiteUtility.py"", line 328, in rm_vm raise e

Update VM Status - Error",,QAFARM,Closed
119,AMATHURA,"2014-09-02 09:53:40",NULL,Cleanup,"Cleanup failing for job 21171201","Pls check. SDI delete request completed successfully.",21171201,,Closed
120,KRAMANJA,"2014-09-09 07:32:04",NULL,null,"Not able to launch URL's  of a SDI based env setup","Not able to lanuch any of the urls  in  the SDI based  env that we setup in QAFarm.

Env wiki: http://aseng-wiki.us.oracle.com/asengwiki/pages/viewpage.action?pageId=856588493

Please let us know if you need more details.

Thanks.",20878249,warroom,Closed
121,BKASHYAP,"2014-09-10 11:01:20",NULL,Cleanup,"Cleanup of Job ID 20696471","Cleanup is taking a lot of time.",20696471,lifecycle_dev,Closed
122,KRAMANJA,"2014-09-17 06:56:45",NULL,Others,"job- -21502717  is completed bug env setup failed","Hi,

Our env setup job- -21502717 failed to create env.
Not finding any logs at the deployer host.

Can you please help to check if any thing missing on the SDI side?",21502717,QAFARM,Closed
123,PWARRIER,"2014-09-18 11:52:23",NULL,Hypervisor,"slc04svo is in Unknown State. Host not REachable","My SDI Delete request is failing as the Routing OHS host is not reachable slc04svo. Checked from the OVM Manager, its in Unknown state. Try to power off off the same, its hanging.",,OPC_QA_FARM,Closed
124,AMATHURA,"2014-09-19 06:52:34",NULL,Cleanup,"Cleanup is stuck","Cleanup is stuck",20589964,fastha,Closed
125,JKUTTAPP,"2014-09-22 15:55:14",NULL,Others,"Jobs in ovm_preflight not picking up","When I submitted a job in ovm_preflight, farm is not picking up the job.",,ovm_preflight,Closed
126,USKUMAR,"2014-09-23 05:09:51",NULL,Cleanup,"21594276 job fails to clean","Hi

After ventakesu cleaned this job successfully from backend, still seeing same state in UI.

Factory resources are not released.

Clean up the job immediately and release factory resources as we are running out of time to trigger SDI job.

Thanks
Usha","21594276 ",OPC_QA_SHARED,Closed
127,SIDHJAIN,"2014-09-23 08:16:48",NULL,VM,"slc04qfz  host not shutting down","I had initiated shutdown of EM host slc04qfz, 
But is still shutting down state for long . Can you please look into it.  

I wanted to restart the vm because EM processes were not responding correctly.",,OPC_QA_FARM,Closed
128,USKUMAR,"2014-09-23 10:54:47",NULL,Others,"Copy scripts on to slcnas551 storage","Hi

Copy below mentioned script to slcnas551 storage
Source: slc02pmh.us.oracle.com/ovab_home/PERF
Destination: slcnas551:/export/mcollective : /fsnadmin/scripts/PERF",,,Closed
129,JKUTTAPP,"2014-09-24 03:13:18",NULL,Others,"FA deployment fails at CONFIGURE_SDI","FA deployment at SDI 14.1.10 is failing at CONFIGURE_SDI block.
Error message at SDI deployer host:
[aime@slc02pat CONFIGURE_SDI_FOR_SDIFA_DEPLOY.2014_9_23.4_4_24.4356]$ cat localTaskProcess.log | grep -i error
 <ERROR> PILLAR --> GSI
 <ERROR> STANDARD_SDI_PROPS_TEMPLATE --> /net/slc02pam.us.oracle.com/scratch/aime/PFA_REMOTE/job_21531584/auto/scripts/SDI/13.0/standard_gsi_sdi.properties",,OPC_QA_SHARED,Closed
130,JKUTTAPP,"2014-09-24 08:37:48",NULL,Others,"CLEANUP of SDI job does not remove fadb entries","After the FA deployment job triggered against SDI_BASE is cleaned from farm UI, the fadbs used by the job still remain in the SDI base.",,OPC_QA_SHARED,Closed
131,JKUTTAPP,"2014-09-24 08:40:23",NULL,Others,"CLEANUP of SDI job does not remove rack entry","After the FA deployment job triggered against SDI_BASE is cleaned from farm UI, the rack used by the job still remain in the SDI base.",,OPC_QA_SHARED,Closed
132,JKUTTAPP,"2014-09-24 08:42:31",NULL,Others,"CLEANUP of SDI job does not OVAB_HOME symbolic link","After the FA deployment job triggered against SDI_BASE is cleaned from farm UI, the OVAB_HOME symbolic link still remains in SDI BASE.",,OPC_QA_SHARED,Closed
133,SWSUDHAK,"2014-09-24 10:25:25",NULL,Hypervisor,"ovmmjava.net.SocketTimeoutException: Read timed out on slcal040","During rehydration AllocateResources task is failing with below error:
[2014-09-24 03:01:14,698] [DEBUG] [REQ JOB20926242mulem6create] [Illegal values for ovmmjava.net.SocketTimeoutException: Read timed out] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:checkResourceLimit(145)] []
 [The OVMM webservice timeout is set to 30000 milliseconds] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:<init>(59)] []",20959565,OPC_QA_SHARED,Closed
134,JKUTTAPP,"2014-09-26 03:24:23",NULL,Others,"BIGIP urls not working on SDI reydrated env","I have rehydrated Rel9.1 ST10 in SDI 14.1.10 PS3 base. Deploymend & rehydration is completed successfully but no url are working. I am getting ""connection reset"" and ""ceritificate"" error.
VIP_NAME=sdiqa21647473-v0039
VIP_ENVIRONMENT_ID=v0039
DC_SHORT_NAME=us1
VIP_TO_OHS_MAPPING_FILE=/net/slc02pay.us.oracle.com/scratch/aime/PFA_REMOTE/job_21647473/work/oracle/work/GET_ALLOCATED_BIGIP/automation_vip_node_map.properties",,ovm_general,Work-in-Progress
135,AMALON,"2014-09-26 17:13:34",NULL,Others,"Add DB_Archive=true/false parameter for job triggering commands","P2T and Cloning requires ARCHIVELOG to be set to true, and currently QA farm sets to false by default. It'll be great to let the user set it during job triggering.",,,Closed
136,AMALON,"2014-09-26 18:21:53",NULL,Others,"Add SDI Deployer Host Name To Job Parameters","As things are now there is no straight forward method to correlate SDI FA deployments to SDI deployer host. This applies to topoligies such as:
SDI_FA_REL9_GSI, SDI_FA_REL9_CRM, SDI_FA_REL9_HCM, etc. 

Please add SDI deployer host to the job parameters for easy access.",,,Work-in-Progress
137,USKUMAR,"2014-09-29 06:46:27",NULL,Others,"Copy scripts on to slcnas551 storage","Hi

Copy below mentioned scripts in the storage with proper execution permission given to it.

Source: /fsnadmin/SDI_FA_OPSCALLOUTSCRIPTS/Rel8FAQA/DedicatedIdm/paid/HCM*

Destination: 1. /fsnadmin/SDI_FA_OPSCALLOUTSCRIPTS/Rel8FAQA/DedicatedIdm/paid/CRM/
2. /fsnadmin/SDI_FA_OPSCALLOUTSCRIPTS/Rel8FAQA/DedicatedIdm/paid/GSI/

Thanks
Usha",,,Closed
138,GSUNDARE,"2014-09-29 10:04:55",NULL,Cleanup,"Cleanup of job 21178135 failed due to VM invalid state","<INFO> 28/09/2014 07:46:42 -> WARNING:the vm is in invalid state. Can't bring it DOWN now

<INFO> 28/09/2014 07:46:42 -> ERROR: Unable to shutdown RAC node2: slc03yyn",21178135,,Closed
139,PWARRIER,"2014-09-29 11:52:37",NULL,Hardware,"Deployer host adc2140021 hangs","Connection to Deployer host adc2140021 hangs. 

21706485 OPC_QA_SHARED	pwarrier	RUNNING	SDI_FA_REL9_GSI",,OPC_QA_SHARED,Closed
140,KRAMANJA,"2014-09-30 07:48:36",NULL,Others,"DTE QA Farm job did not come to QAFarm","I triggered the following job

/usr/local/packages/aime/dte/DTE3/bin/jobReqAgent -u kramanja -d fusionapps -r rup5 -p LINUX.X64 -v fa_mega_ovs -toposetid 14278 -toposetMemberID 69364 -s /tmp -l JDK6_MAIN_LINUX_111003.1.6.0.29.B11 -report -e kavuri.ramanjaneyulu@oracle.com,arulvenkatesh.mathuranayagamn@oracle.com -nocheckshiphome -parallel=true -noalways=false SKIP_CLEANUP=""true"" OVM_TOPOLOGY=CRMLARGE_DEP_V1REL9_ST7_HA_SCALE  FAOVM_LABEL=FAOVMINTEG_11.1.9.1.0_LINUX.X64_140911.1325   FAOVM_TE",21721244,warroom,Closed
141,JKUTTAPP,"2015-08-04 14:09:36",NULL,NULL,"CLEANUP finished without invoking DEPLOY_DELETE","Triggered cleanup of the job 21647473 from UI. Without invoking the DEPLOY_DELETE, the job got cleaned and vms are released. I had verified before cleanup that entire cleanup script is in place at cleanup.sh.",21647473,ovm_general,Closed
142,GSUNDARE,"2014-09-30 10:44:06",NULL,Hypervisor,"Build RAC operation has failed","The Run (DTE job 21719311) has failed at  Build RAC.  The same error is observed in the job run by Swaroop also.",21719311,OPC_QA_FARM,Closed
143,AMATHURA,"2014-09-30 14:43:37",NULL,Hypervisor,"job:21723207   cleanup hanging","Clean up for dte job: 21723207 was hanging from past 4 hours.

Can any one please clean up this job ,and release the associated hardware.",21723207,fastha,Closed
144,PWARRIER,"2014-10-01 07:04:42",NULL,Others,"URL not working","I have Master setup but url is not working
https://stagerel92famaster-faop-v0025-fs.us1.vfarm.oraclecorp.com/homePage/faces/AtkHomePageWelcome. Can you help to fix",,OPC_QA_SHARED,Closed
145,MULIEGAD,"2014-10-01 07:11:47",NULL,Others,"Few blocks failing in SDI base topology","Below are few blocks failing in SDI base topo run:
PATCH_EMGC.dif,EMGC_CLOUD_INSTANCE,STOPALL/STARTALL/STOPALL3/STARTALL3/STARTALL4/STOPALL4,PATCH_SDI,EXPORT_SUN_STORAGE_DETAILS.dif,TAS_CERTS_CONFIG.dif,CONFIGURE_SDIBASE_14.dif,SDI_CONFIG_ASSISTANCE.dif",21724779,OPC_QA_FARM,Closed
146,MULIEGAD,"2014-10-01 11:07:58",NULL,Cleanup,"JOb 21739768 cleanup failed","Job 21739768 has failed to cleanup please help to clean it up manually.",21739768,,Closed
147,PWARRIER,"2014-10-01 17:30:25",NULL,Hypervisor,"Rehydration failing at slc03yza_us_oracle_com","Child pod rehydration is failing while at slc03yza_us_oracle_com. OVMM shows CopyFromTemplateAsync:Call agent
CopyFromTemplateCallBack
CallBack:end-success:end:failed:<Exception: no enough space under /var/ovs/mount/7307E93E4BD14B199E3D435B52D3FDF2, vm_size=24772 > free_space=83>

StackTrace:
  File ""/opt/ovs-agent-2.3/OVSSiteUtility.py"", line 210, in cp_vm
    raise Exception(""no enough space under %s, vm_size=%d > free_space=%d"" % \

Update virtual machine status:Error",,,Closed
148,JKUTTAPP,"2014-10-02 11:55:02",NULL,Others,"SDI FA job with RAC creation failed for CRM","I have tried triggering 9.2 CRM deployment with RAC creation.
But the job failed immediately after the deployer host is created. Even the PFA_REMOTE directory is not created.","21753726 ",OPC_QA_SHARED,Closed
149,MULIEGAD,"2014-10-07 06:55:18",NULL,Cleanup,"21058117 job failed to cleanup","We have triggered cleanup for job 21058117 and it has failed. Please help us to manually clean this up",21058117,OPC_QA_FARM,Closed
150,AMATHURA,"2014-10-07 09:58:13",NULL,Others,"fail to add a db instance is failing  to rac: slc04qri,slc04qrj nodes,: vnc: slc04qri:1(welcome)","adding a new instance to the rac is failing 

 perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o add_instance_to_rac -node1 slc04qri -node2 slc04qrj  --sid testdb --port 1555 --db_unique_name dbUnique


adding instance with different db_unique_name parameter is failing , please look at the issue once at 
vnc: slc04qri:1 (welcome)",,fastha,Closed
151,KRAMANJA,"2014-10-09 04:48:58",NULL,Hypervisor,"SDI based Rel8 env setup is failing","From the logs,  the job is failing to connect to  RAC nodes. PLease look into this.

Following is the error in the logs:

[aime@slc02pat logs]$ pwd
/scratch/aime/work/RAC_AGENT_DEPLOY_2.2014_10_8.12_51_8.7586/logs
[aime@slc02pat logs]$ cat check_ssh.log
spawn ssh -t aime@slc05btu echo ""Successfully Connected to Remote Machine""; exit
Yes

ssh: connect to host slc05btu port 22: Connection timed out
send: spawn id exp5 not open
    while executing
""send ""\r""""
[aime@slc02pat logs]$",21831081,warroom,Closed
152,MULIEGAD,"2014-10-09 09:28:42",NULL,Others,"21741356 and 21804165","Have triggered SDI base topology and FA only topology.But facing few issues with the run of few blocks which is explained as below.
JOB: 21741356

PATCH_EMGC.dif:This block has timed out.It seem to hung while applying OMS patch (Venki also faced this issue)and also it has not applied PS3 patches in em utils. emgcCloud9.sh is not run.

CONFIGURE_SDIBASE: block also failed due to deprecated sdi properties in both SDI base and FA only(CONFIGURE_SDI_FOR_SDIFA_DEPLOY block) topology run.",21741356,OPC_QA_FARM,Closed
153,PWARRIER,"2014-10-10 02:09:55",NULL,Hypervisor,"Jobs released on smart_clone pool","Job in Swaroops name has been cleanedup in smart_clone pool. Can you please merge the hypervisor to opc_qa_farm. There is no resource to trigger another job there. We are sharing SDI base, Master , Child pod with nagaraj team",,smart_clone,Closed
154,KRAMANJA,"2014-10-10 05:32:44",NULL,Others,"DPELOY_CREATE block failing in a  SDI based env setup","Our DPELOY_CREATE block fails with the following error:
java.lang.AssertionError: Request was not completed successfully. Please check the logs for the cause of failure.. Expression: (result == COMPLETED). Values: result = NOT_FOUNDerror at line: 62java.lang.AssertionError: Request was not completed successfully. Please check the logs for the cause of failure.. Expression: (result == COMPLETED). Values: result = NOT_FOUND
at org.codehaus.groovy.runtime.InvokerHelper.assertFa","21840377	",warroom,Closed
155,MULIEGAD,"2014-10-13 06:59:43",NULL,Others,"FA URL not working with new format","fa.service.host.template= {system_name}-{fa_domain_type}.{dc_short_name}.{oracle_services_domain}

With the ablve fa.service.host.template FA application URL not working.",21804165,,Closed
156,USKUMAR,"2014-10-14 09:25:13",NULL,Others,"copy db control files to storage slcnas551","Hi

Copy db control files and ops scripts to storage slcnas551

source: slcnas570.us.oracle.com:/export/ovmTemplates/crm_rel9.2_st11_c9share/db
Destination:
/fsnadmin/SDI_DB_STAGE/ST11Rel9.2/DedicatedIdm/paid/CRM/fusiondb/

Similarly oiddb and oimdb

Ops script:

source: SDI_FA_OPSCALLOUTSCRIPTS/Rel8FAQA/DedicatedIdm/paid/HCM/

Destination folder structure:
SDI_FA_OPSCALLOUTSCRIPTS/ST11Rel9.2/DedicatedIdm/paid/CRM/

thanks
Usha",,,Closed
157,MULIEGAD,"2014-10-14 12:03:28",NULL,Others,"SETUP_RAC_FOR_DEPLOYMENT  fails","There is a dif in  SETUP_RAC_FOR_DEPLOYMENT  of master setup. Build rac is failing as below:

Configuring slc04sww network
Configuring slc04swv network
Configuring slc04sww shm
Configuring slc04swv shm
building RAC ....
ERROR: subjobid 1027752142610  failed!

Please help us to resolve this issue.",21905032,OPC_QA_SHARED,Closed
158,JKUTTAPP,"2014-10-16 07:34:07",NULL,Hypervisor,"FA deployment fails with ""start deployer host error""","FA deployment task to validate Post repo patch failed with the error ""failed to start deployer host"".
DTE JobID: 21941180

Deployer host: adc2201884

Error message in OVM manager:
Start - /OVS/running_pool/259337_adc2201884
PowerOn Failed : Result - failed:errcode=20003, errmsg=No server selected.

StackTrace:
   File ""/opt/ovs-agent-2.3/OVSSiteVM.py"", line 155, in start_vm
     raise e",21941180,OPC_QA_SHARED,Closed
159,USKUMAR,"2014-10-16 09:07:56",NULL,VM,"Unable to release VMs from OVS","When Iam trying to release VMs, getting below error
 perl /net/adc2120135/scratch/aime/merge/ovm_farm_manager_v7.pl -o release -host_name adc2140006 -force -debug
Debug: need to stop and deregister from OVS

Debug:status is Access denied. Please check username or password.

Debug:status is Access denied. Please check username or password.
ERROR: unknown VM status: Access denied. Please check username or password.

Same error with adc2140026, adc2140007, adc2140009 and adc2140010,",,,Closed
160,KRAMANJA,"2014-10-17 09:28:55",NULL,Others,"In a SDI OVM env after enabling MC /systemSDI: ""YES""/ is missing","We enabled  MC in a SDI OVM env.

But in the /var/mcollective/facts.yaml  file,
systemSDI: ""YES""
property is missing.",21840377,warroom,Closed
161,KRAMANJA,"2014-10-17 12:18:30",NULL,VM,"job-21966263	 is waiting for WAITING_FOR_FREE_HV","Trying to setup  a SDI base env on 14.1.10

Total Memory Required for this Topology is 126496 MB

slcac613 - hypervisor is free with 140G. 
Not sure why this hypervisor is not being picked up.","21966263	",warroom,Closed
162,AMALON,"2014-10-18 00:40:45",NULL,VM,"OID VM in SIMv2 Missing Gateway","The IDM_HOST1 vm (slc00ygq) is missing a gateway address, and hence caused the SIMv2 deployment to fail. Please check for fixing it before triggering a new job.",21966263,warroom,Work-in-Progress
163,AMATHURA,"2014-10-20 10:17:21",NULL,VM,"slc02pat is running out of space","slc02pat is running out of space

[aime@slc02pat trace]$ df -h .
Filesystem            Size  Used Avail Use% Mounted on
/dev/xvda1             95G   90G     0 100% /",21531584,QAFARM,Closed
164,AMATHURA,"2014-10-20 12:25:00",NULL,Others,"22009455 is in waiting state for long time","22009455 is in waiting state for long time",22009455,fastha,Closed
165,PWARRIER,"2014-10-21 04:10:31",NULL,VM,"SDI base jobs triggered fails as deployerhost itself is not created.","SDI base triggered job 22021479 has failed to satrt the proxy on the deployer host. But it appears this host itself is not created.",,OPC_QA_FARM,Closed
166,KRAMANJA,"2014-10-21 06:05:10",NULL,Others,"Not able to add new hostnames in Farm","perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o add_hostname -gate_way 10.245.136.1 -net_mask 255.255.248.0 -ip_addr 10.245.137.173 -host_name slc03wcu -farm warroom
Adding VM slc03wcu
DBD::mysql::st execute failed: Column count doesn't match value count at row 1 at /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl line 8473.",NA,warroom,Closed
167,AMATHURA,"2014-10-21 06:22:17",NULL,Others,"Cleanup didn't initiate delete in SDI","I have completed the cleanup of my HA env in Venky's SDI. But it looks like without cleaning up the traces in SDI, I think all the machines have been wiped off through QA Farm.",21808763,fastha,Closed
168,AMATHURA,"2014-10-21 06:23:52",NULL,Others,"SDI Base topo stuck @ PATCH_EMGC","Currently the topo stuck @ PATCH_EMGC block for more than 7 hours. Could you pls help?","22009455 ",fastha,Closed
169,KRAMANJA,"2014-10-21 09:13:27",NULL,Others,"Combined hypervisors slcah782, slcah765","We have combined slcah782, slcah765 hypervisors under the warroom subpool.

Please update the QAFarm.",NA,warroom,Closed
170,KRAMANJA,"2014-10-21 09:15:01",NULL,Others,"please update the memory of hypervisors slcak524,","Please update the memory of the hypervisor slcak524 to 251811.",NA,warroom,Closed
171,KRAMANJA,"2014-10-21 09:36:41",NULL,Cleanup,"job-21646052 cleanup did not clean the shared disks","We have triggered cleanup of job-21646052.  Job cleanup is completed but the associated  shared disks did not get cleaned.

Hypervisor: slcah783
shared disks:
<INFO> 20/10/2014 06:33:20 -> INFO: Cleaning sharedstorage for slc03wix and slc03wiy

<INFO> 20/10/2014 06:33:22 -> Shared virtual disk ""21646052_1"" does not exist, or an error occurred while getting the shared disk resource.",21646052,fastha,Closed
172,KRAMANJA,"2014-10-21 15:48:36",NULL,Others,"job got struck at ""WAITING_FOR_FREE_HV""  HA job","submitted a job for SDI based HCM-rel9.2 HA+scaleout env setup. 
Combined hypervisors are slcah782,slcah765.
Job was struck at  ""WAITING_FOR_FREE_HV"" .",22029657,warroom,Closed
173,AMALON,"2014-10-21 22:53:29",NULL,Others,"Setup Rac Block Fails at Create Shared Disk","Shared disk creation fails to create during Setup Rac For Deployment block. 
From create_rac_21_10_2014_15_23_47_95.3083409302966.log 
Shared virtual disk ""22033892_5"" is being created. Please check the status.
ERROR: Unable to initizialize shared disk in given time!",22033892,QAFARM,Closed
174,AMALON,"2014-10-21 22:56:07",NULL,Others,"Setup Rac Block Fails at Create Shared Disk","Create shared disk fails during Create Rac For Deployment Block.

From create rac log:
Shared virtual disk ""22033901_5"" is being created. Please check the status.
ERROR: Unable to initizialize shared disk in given time!",22033901,QAFARM,Closed
175,AMALON,"2014-10-23 03:29:30",NULL,Others,"Setup Rac Block Fail Multiple times w/different jobs.","Hi,

This block constantly failed today with three different jobs. The latest job ID is attached to the bug. From log:
NIC has been added to the virtual machine.
ERROR: subjobid 168630022195220  failed!",22059205,warroom,Closed
176,AMALON,"2014-10-23 03:31:43",NULL,Hypervisor,"OPC_SHARED_FARM Jobs Fail To Install OVMCLI in Deployer Host","Hi,

Three jobs in OPC_SHARED_FARM failed to start due to:
Please note that your job 22058014 has Failed while Installing OVMCLI on DEPLOYER_HOST",22058014,OPC_QA_FARM,Closed
177,AMALON,"2014-10-23 21:39:00",NULL,Others,"CONFIGURE_SDI_FOR_FA Block Stuck, Running over 24hrs w/no files","Hi,

In the job attached and also in job 22061188, Configure SDI for FA block has been running over 24hrs without having any files. Please check root cause. Also, possible improve logic by setting time out.",22059498,QAFARM,Closed
178,AMALON,"2014-10-23 21:43:45",NULL,Others,"Shared IDM Setup Consistently Fails in SDI 14.1.10-12","Hi,

Shared IDM Setup has failed consistently for new SDI base with SDI 14.1.10 and 14.1.12. Seems to be conflicting patch issue. Also job 22032784",22048252,,Closed
179,AMALON,"2014-10-23 21:45:10",NULL,Cleanup,"Job 22026953 Stuck In Initiating Cleanup State","Hi,

The following job has been stuck in initiating cleanup state for a few days now.",22026953,warroom,Closed
180,KRAMANJA,"2014-10-27 13:40:34",NULL,Others,"CONFIGURE_BIGIP block failed  with login issues","CONFIGURE_BIGIP failed. From the error messages, it looks to  script login issue
$cat 
Permission denied at /net/slc05bsx.us.oracle.com/scratch/aime/PFA_REMOTE/job_22095527/auto/scripts/SDI/generic/bigip.pl line 194",22095527,warroom,Closed
181,PWARRIER,"2014-10-28 01:49:11",NULL,Cleanup,"Cleanup of jobs 22077604,22101591 stuck on opc_qa_farm","22077604	OPC_QA_FARM	pwarrier	CLEANUP_INITIATED	SDI_14.1_BASE_OHS	N/A	

22101591	OPC_QA_FARM	pwarrier	CLEANUP_INITIATED	SDI_FA_REL9_CRM	N/A",,OPC_QA_FARM,Closed
182,KRAMANJA,"2014-10-28 14:04:26",NULL,Others,"/scripts in hypervisor not getting updated with newer version","We have registered hypervisor to Farm.  At the time of registering we make sure /scripts/* has the latest by running preparehypervision script.


But later on /scripts did not get updated with latest. Due to this RAC setup block is failing.",NA,warroom,Closed
183,DJSUNDER,"2014-10-30 00:15:26",NULL,Filesystem,"Shared disk created in hypervisor should not have full permission","QAFarm job creates shared disks in hypervisors under /dev and clubs 2 or more Shared disks to create abstraction layer - ASM disk groups (eg DATA, RECO) which are essential for RAC services to function normally. 

After QM, we had an issue where CRS could not be started because ?shared disks? didn?t have enough permission. 

This ticket is to change the permission for SDs to 777 immediately after its creation. SETUP_RAC_FOR_DEPLOYMENT DTE block on all topologies",,QAFARM,Closed
184,KRAMANJA,"2014-10-30 08:58:29",NULL,Others,"Add  3 more BIG IP urls in warroom pool","We are planning to create a few more new environments in Warroom pool.  Please add  additional BIP IPs.",NA,warroom,Closed
185,SRMEKA,"2014-10-31 11:27:55",NULL,Others,"Need access to ""List/Edit OVM Topology""","Need access to ""List/Edit OVM Topology""",,,Closed
186,AMALON,"2014-10-31 23:35:13",NULL,Hypervisor,"A Farm Jobs Fail b/c of Deplyer Host Startup Time","Hi,
Recent jobs in QA farm pools: fastha and OPC_QA_SHARED fail because of deployer host startup issue. Most recent job 22177071 failed because of: ""Please note that your job 22177071 has Failed while Starting DEPLOYER_HOST"", but the host slc05jll has started and is accessible. Another job ID 22175578",22177071,fastha,Closed
187,AMATHURA,"2014-11-03 13:38:58",NULL,Others,"22214539 is failing","Initially its failing with below error:
Please note that your job 22214539 has Failed while registering FA SDI
Later with below error:
Please note that your job 22211277 has Failed while Starting DEPLOYER_HOST
I tried 3 times. I am hitting same error. I bounced listener on hypervisor where deployer host is created. Still hitting the same issue.",22214539,fastha,Closed
188,KRAMANJA,"2014-11-05 09:29:18",NULL,Hypervisor,"Hypervisor  slcak523 is showing in error state in OVMM","http://slc02oxi.us.oracle.com:8888/OVS
->""Server Pools"" -> search for slcak523  

It is showing in error state.

Due to this, if any Farm job allocates to this Hyp is failing to register templates.",NA,warroom,Closed
189,PWARRIER,"2014-11-06 03:59:07",NULL,Others,"URLs not working","SDI base 22129156
DEPLOYER_HOST=slc03yye.us.oracle.com 
HOSTED_ROUTING_OHS_HOST=slc03yyf.us.oracle.com 
BIGIP : v0029

I had not triggered a FA Deploy topo, just picked the rack from another job of mine. But I ahve run the configure BIGIP block after changing the bigip and routing ohs host in the map file.",,OPC_QA_FARM,Closed
190,MULIEGAD,"2014-11-11 06:41:10",NULL,Cleanup,"job 21804165 clean failed","JOb 21804165 cleanup failed. Please help us to clean the same.",21804165,OPC_QA_FARM,Closed
191,SWSUDHAK,"2014-11-11 07:28:42",NULL,Cleanup,"21938288 cleanup failing","Initiated job:21938288 cleanup and is failing",21938288,OPC_QA_FARM,Closed
192,MULIEGAD,"2014-11-14 08:26:37",NULL,Cleanup,"cleanup failed for 21941146","cleanup failed for 21941146.",21941146,,Closed
193,KRAMANJA,"2014-11-14 13:59:52",NULL,Others,"Cleanup failed for job-22330479","Following errors are reported in the log:

Shared virtual disk ""22330479_5"" does not exist, or an error occurred while getting the shared disk resource.

Done

INFO: cleaning VMs
INFO: releasing VM slc05bmm
Debug: need to stop and deregister from OVS
ERROR: subjobid 7072921451445  failed!
STDERR:",22330479,warroom,Closed
194,AMATHURA,"2014-11-17 10:49:08",NULL,Others,"Usage of Sun Storage in Primary and Standby envs","In all the QA Farm Jobs, source Templates and target project creations are happening on the same storage. All our templates which are announced by Nagendra are from slcnas570 sun storage. Is there a way to specify the QA Farm DTE run to use Templates from slcnas570 and create target (fa,ohs,idm3ohs,idm3oid) shares from different sun storage?",,fastha,Closed
195,KRAMANJA,"2014-11-18 09:44:15",NULL,Hypervisor,"Please adjust memory of Hypervisors  slcaj503 & slcaj504","Please adjust memory for hypervisors  slcaj503 & slcaj504

Change available max memory from 254672 to  251811.

We had registered the memory on higher side but at run time it is failing. So, please reduce the max available memory to  251811",NA,warroom,Closed
196,SRMEKA,"2014-12-02 07:00:38",NULL,Others,"Need Topo def for REL10 E2E RUN ""GSILARGE_E2E_REL10""","Since it has exceeded the number of characters, I will send an email with the exact specification.",,,Closed
197,SRMEKA,"2014-12-03 06:23:15",NULL,Others,"Need Topo def for REL10 E2E RUN ""HCMLARGE_E2E_REL10""","Since it has exceeded the number of characters, I will send an email with the exact specification.",,,Closed
198,AMPANAMA,"2014-12-04 05:30:27",NULL,Others,"Cleanup of JOB 22724396 is hanging in FARM UI","Hi,
Cleanup of one of our failed sdi job seems to be hanging and still shows as ?CLEANING?.

JOB ID: 22724396

Could anyone please take a look and help me to cleanup & submit another job?",22724396,OPC_QA_FARM,Closed
199,SRMEKA,"2014-12-04 12:50:23",NULL,Others,"Need Topo def for REL10 E2E RUN ""CRMLARGE_E2E_REL10""","Since it has exceeded the number of characters, I will send an email with the exact specification.",,,Closed
200,SRMEKA,"2014-12-05 10:45:39",NULL,Others,"Unable to create snapshot. Failed with authorization","CREATE_SNAP block failed with authorization access
/usr/bin/python ./manage_zfs_storage.py --user zfsuser --password zfsuser --project-name ovmTemplates --server slcnas570 --action create_snapshot --log-dir /net/slc08bwd.us.oracle.com/scratch/aime/PFA_REMOTE/job_22731601/work/oracle/work/CREATE_SNAP/logs --pool-name pool-570 --action create_snapshot --share-name srmeka_rel10_st2_gsi_02122014 --snapshot-name srmeka_rel10_st2_gsi_02122014_snap 
error: You are not authorized to perform this actio",,,Closed
201,KRAMANJA,"2014-12-08 05:53:58",NULL,Others,"Please provide the privilege for cloning/creating new OVM topology","Hi,

We need to create OVM topology with higher VM memory sizes for supporting upgrades from 
Rel7->Rel8->Rel9.2",NA,warroom,Closed
202,AMATHURA,"2014-12-10 06:29:18",NULL,Cleanup,"Cleanup failed for job 22819718","ERROR> 09/12/2014 12:08:06 -> There was a error while trying to establish Telnet connection to Host --> slc05bns.us.oracle.com.us.oracle.com

us.oracle.com is appended twice. Pls help.",22819718,DRQA,Closed
203,AMATHURA,"2014-12-10 06:30:57",NULL,Cleanup,"Cleanup is stuck","Cleanup is stuck for job 22826008. Pls help.",22826008,DRQA,Closed
204,KRAMANJA,"2014-12-10 07:12:28",NULL,Cleanup,"Cleanup is failing for job-22137387","Clenaup job failed for the job22137387",22137387,warroom,Closed
205,AMPANAMA,"2014-12-10 09:05:00",NULL,Others,"Unable to access Domain Admin Console using external ports","Hi,

SD_BASE14.1.16: HCM Order: 

During sanity check we found that none of the domain admin console are accessible using external ports. We could see there is some issue with LBR hosts. so, please take a look

[Observations]

- Unable to launch ""https://sdiqa22743488-v0046-hcm.us1.vfarm.oraclecorp.com/hcmCore/faces/HcmFusionHome"" shown as part of ""./sdictl.sh listsv"" output

- Unable to launch http://slc03zan.us.oracle.com:10613/console ( FAAdmin / fusionfa1 )",22743488,OPC_QA_FARM,Closed
206,DJSUNDER,"2014-12-12 21:34:00",NULL,Others,"job 22883688 had failed at DEPLOY TEMPLATE block - RMAN duplicate","Error in the faovmdeploy.log under /scratch/aime/PFA_REMOTE/job_22883688/work/oracle/work/DEPLOY_TEMPLATE/logs:
[12:46:32]   Executing Task: RMAN Duplicate Database ...              ... FAILED. [10m6s]
An error occurred: An error occurred during command execution: Remote RMAN Duplicate Database command '/tmp/FAOVMDB718782103415927176_fusiondb/rman_dbutil.sh' has exited with a non-zero exit code (1)",22883688,QAFARM,Closed
207,KSRIRAMA,"2014-12-16 06:02:38",NULL,Hypervisor,"Job 22916965 : DEPLOY_TEMPLATE failed to create VM","Job 22916965 failed to create VM :

""/net/slc08bxd.us.oracle.com/scratch/aime/PFA_REMOTE/job_22916965/work/oracle/work/DEPLOY_TEMPLATE/logs/faovmdeploy.log""

[05:33:58/fa] Executing Task: Create VM - slc08bxm_us_oracle_com ... FAILED. [1h0m5s]
An error occurred: An error occurred during command execution: TimeoutException caught when creating VM by slc08bxm_us_oracle_com Error: VM:slc08bxm_us_oracle_com/275733 >> Timeout expired while waiting for target state PoweredOff
Exiting with error",22916965,QAFARM,Closed
208,KSRIRAMA,"2014-12-19 14:24:18",NULL,NULL,"Enhancement : Add expiry date/time to the job details","Hi,

Currently job details show the start date/time and end date/time of the job. Please add ""Expiry date/time"" for the live jobs - this will be useful info for users to know and take necessary action for extending the env, as required.",N/A,QAFARM,Open
209,SRMEKA,"2014-12-24 04:53:28",NULL,Others,"Our CRM OVM E2E run has failed with memory issue","Hi,

Our CRM OVM E2E run has failed with the below memory issue, can you please check if there any issue with hypervisor/farm.

Error :
[15:02:51] Verifying Hosts for Deploy ... Done. [0m24s]
An error occurred: There is not enough free memory predicted to be available to start VM 'slc08bur_us_oracle_com'.
Total memory needed in OVMM pool 'slcan523' on host 'slc02oxi.us.oracle.com' is 188928MB, but there is only 188835MB available.
Exiting with error
~",23036961,QAFARM,Closed
210,SRMEKA,"2014-12-29 06:19:34",NULL,Cleanup,"Clean up this env 23065044 as it is no longer required","Clean up this env 23065044 as it is no longer required","23065044 ",QAFARM,Closed
211,SRMEKA,"2014-12-29 06:22:20",NULL,Cleanup,"Cleanup for this job 23036961 has failed.","Cleanup for this job 23036961 has failed.",23036961,QAFARM,Closed
212,AMATHURA,"2014-12-31 05:26:12",NULL,Others,"Pls allocate 2 more Big IPs for farm fastha","Pls allocate 2 more Big IPs for farm fastha",,fastha,Closed
213,AMATHURA,"2014-12-31 17:16:13",NULL,Others,"Pls add one big ip to pool fastha","Pls add one big ip to pool fastha",,fastha,Closed
214,SWSUDHAK,"2015-01-02 05:44:44",NULL,VM,"slc03peo not responding","slc03peo.us.oracle.com is SDI deployer host.
aime/2cool for sdi deployer host is not connectiong.. getting closed as we connect. Tried shutting down from OVMM is still showing shutting down for long time. Pls help us re-start this vm.",22270935,OPC_QA_FARM,Closed
215,AMPANAMA,"2015-01-06 11:29:35",NULL,Cleanup,"Cleanup of Job 22375401 failed to clean rack and db instances","Cleanup of REL10 JOB 22375401 failed to cleanup ip's from RACK and all the db instances.
B/C of this issue these ip's got allocated to one of our new HCM job.

To proceed further we had manually delete rack id - 22375401 and all the db instances.

So, this issue should be addressed for proper cleanup of the job to avoid the above mentioned issue.",22375401,OPC_QA_FARM,Closed
216,PRTUMMAL,"2015-01-06 13:12:01",NULL,Hypervisor,"Please cleanup Job","timeout while cleaning up. Please cleanup from backend",22945795,fastha,Closed
217,AMATHURA,"2015-01-06 13:15:17",NULL,Hypervisor,"Cleanup is stuck for job 22945795","Cleanup is stuck for job 22945795. Pls help.",22945795,fastha,Closed
218,JKUTTAPP,"2015-01-07 05:48:51",NULL,Others,"VM creation fails with ""NetworkCardException: OVM-4105 MAC address exists""","During the vm creation of SHARED IDM setup for TAS based SDI setup, it throws the exception ""Caught exception while handling request: oracle.ovs.biz.exception.NetworkCardException: OVM-4105 MAC address exists""",23233410,ovm_general,Closed
219,SRMEKA,"2015-01-09 06:01:55",NULL,Hypervisor,"GSI E2E PREFLIGHT FAILED TO START VM slc06xxy","Log : /net/slc06yeq.us.oracle.com/scratch/aime/PFA_REMOTE/job_23285867/work/oracle/work/START_STOP_VM/logs/request_vm_start_slc06xxy.log

Powering on.
resetting slc06xxy
Virtual machine status has been reset.
Powering on.
Can't bring on  on a timely manner. Something is Wrong",23285867,QAFARM,Closed
220,MAMEHRA,"2015-01-09 06:57:50",NULL,null,"GSI E2E PREFLIGHT FAILED TO CREATE VM slc06xuy","on checking the log :

Caught exception while handling request: oracle.ovs.biz.exception.NetworkCardException: OVM-4105 MAC address exists.
ERROR: Creation Failed : slc06xuy is stil in Virtual machine ""slc06xuy"" does not exist. status after 40 Minutes",23269901,QAFARM,Closed
221,SRMEKA,"2015-01-09 10:03:08",NULL,Others,"Update E2E topo defs HCMLARGE_E2E_REL10,CRMLARGE_E2E_REL10","Please update topo def of HCMLARGE_E2E_REL10 by appending one more host to the existing def:
21:4:2000:GRC_HOST:stit_oel5u6:false
Please update topo def of CRMLARGE_E2E_REL10 by appending to the existing def:
21:4:2000:GRC_HOST:stit_oel5u6:false",,,Closed
222,AMALON,"2015-01-09 17:44:28",NULL,VM,"Shared IDM VM Fail To start","In SDI base deployment, shared IDM setup failed due to failed VM start.

Error:
can't connect to port 9000 on slc05jjs: No route to host at /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl line 5115.",23293788,warroom,Closed
223,KRAMANJA,"2015-01-09 18:53:28",NULL,Others,"memory mismatch with slcak524 hypervisor","After allocating 254192M
 memory , in the Farm it is still showing 1808M free.

Max memory registered for slcak524 is 251811M.",23293788,warroom,Closed
224,PRTUMMAL,"2015-01-12 06:14:58",NULL,Cleanup,"Please cleanup Job 22945795","cleanup key is:  22945795_epdv8j7otcabst7veec2c2l78a",,,Closed
225,JKUTTAPP,"2015-01-13 07:22:42",NULL,Others,"Patch_TAS_DC1 give incorrect status","Patch_TAS_DC1 is giving incorrect status after running tas_patch script.

tas_patch log have the message ""^[[1mOverall Upgrade Status : FAILED"" but localTaskProcess.log shows ""Upgrade Status : SUCCESS was found in log""",,,Closed
226,AMALON,"2015-01-13 21:27:18",NULL,Others,"Allocate BIG IP Error Not Informative","Hello, 

When running the block above in job we got the following error:
ERROR: DTEID 23332166 is not registered with bigip:!
This error does not elaborate enough for debugging.",23332166,warroom,Closed
227,AMALON,"2015-01-13 23:27:05",NULL,Others,/SDI/generic/ADD_FA_TEMPLATE_NAME.pl,"Hi,

The following script sets the old definitions for invoking sdictl and hence causing the flow to fail.

Please duplicate and/or update. 

Thank you,

Amit",,,Closed
228,AMPANAMA,"2015-01-14 09:45:52",NULL,Others,"15.1.2: Unable to access LBR URLs in SDI environment","Hi, SD_BASE15.1.2: GSI Master Order: 
During sanity check we found that none of the domain admin console are accessible using external ports. We could see there is some issue with LBR hosts. so, please take a look 
[Observations] - Unable to launch https://stagerel100famaster-faop.fs.changeme.vfarm.oraclecorp.com/homePage/faces/AtkHomePageWelcome shown as part of ""./sdictl.sh listsv"" output - Unable to launch http://slc04sxs.us.oracle.com:10613/console 
JOBID: CREATE_MASTER_1420810197315",22481222,OPC_QA_SHARED,Closed
229,SRMEKA,"2015-01-14 09:47:32",NULL,Others,"Create a seperate pool name ""CDRMPREFLIGHT"" with the given hypervisors","Create a seperate pool name ""CDRMPREFLIGHT"" with the given hypervisors:
slcao392,slcao573,slcao574",,,Closed
230,DJSUNDER,"2015-01-14 23:30:43",NULL,Others,"23355088 had failed - need to debug and fix the issues ASAP","Following blocks had failed.

RAC_AGENT_DEPLOY_1.dif
RAC_AGENT_DEPLOY_2.dif
CONFIGURE_SDI_FOR_SDIFA_DEPLOY.dif
DEPLOY_CREATE.dif
DEPLOY_UPDATE.dif
GENERATE_PLAN.dif
ExportInfo4OSNTopo.dif
Run_OSN_Tests.dif
DEPLOY_DELETE.dif","23355088 ",warroom,Closed
231,SRMEKA,"2015-01-16 04:52:40",NULL,Cleanup,"Please cleanup this stale job : 23400211","Please cleanup this stale job : 23400211",23400211,QAFARM,Closed
232,PWARRIER,"2015-01-16 13:08:30",NULL,Hypervisor,"shutting down VMs from slcak528 and slcak530 is hanging","faovm.ha.HOST_FA=slc05jmg.us.oracle.com
faovm.ha.HOST_PRIMARY=slc05jmf.us.oracle.com
faovm.ha.HOST_SECONDARY=slc05jmq.us.oracle.com
faovm.ha.HOST_OSN=slc05jmw.us.oracle.com
faovm.ha.HOST_BI=slc05jmh.us.oracle.com
faovm.ha.HOST_OHS=slc05jmx.us.oracle.com
faovm.ha.HOST_LDAP=slc05jmc.us.oracle.com
faovm.ha.HOST_OIM=slc05jmd.us.oracle.com
faovm.ha.HOST_WEBGATE=slc05jme.us.oracle.com

We are cleaning the environemtn, but cleanup of every VM hangs in the shutdown state.",,OPC_QA_SHARED,Closed
233,MULIEGAD,"2015-01-19 05:08:04",NULL,Hypervisor,"slc05jmi.us.oracle.com VM  has connection problems","We are unable to connect to slc05jmi.us.oracle.com. Please help us to get the access back.",,OPC_QA_FARM,Closed
234,SWSUDHAK,"2015-01-19 09:19:35",NULL,Others,"Change in LBR mapping","For BIG IP v0029 the rule is set as -us1.
We want to change this to .us1 across all environments.",22129156,OPC_QA_FARM,Closed
235,SWSUDHAK,"2015-01-19 10:15:16",NULL,Hypervisor,"Failed to shutdown VM 'slc03yzz_us_oracle_com'","During SDI delete Flow we noticed below error related to VM shutdown.

An error occurred: An error occurred during command execution: Failed to shutdown VM 'slc03yzz_us_oracle_com' in OVMM pool 'slcai056'",22367754,OPC_QA_FARM,Open
236,MULIEGAD,"2015-01-20 06:45:58",NULL,Others,"DB instances down for DB Factory resource FUSG01_UNIQUE","DB instance is down with below error
ORA-03113: end-of-file on communication channel
Session ID: 0 Serial number:For details refer to ""(:CLSN00107:)"" in ""/u01/app/11.2.0/grid/log/slc03edz/agent/crsd/oraagent_oracle/oraagent_oracle.log"".
ORA-01589: must use RESETLOGS or NORESETLOGS option for database open For details refer to ""(:CLSN00107:)"" in ""/u01/app/11.2.0/grid/log/slc03edy/agent/crsd/oraagent_oracle/oraagent_oracle.log"".",22366946,OPC_QA_SHARED,Closed
237,MAMEHRA,"2015-01-20 10:00:56",NULL,Others,"CRM E2E FAILED DUE TO INSUFFICIENT MEMORY","[01:54:15] Verifying Hosts for Deploy ... Done. [0m24s]
An error occurred: There is not enough free memory predicted to be available to start VM 'slc08ucd_us_oracle_com'.
Total memory needed in OVMM pool 'slcao574' on host 'slc02oxi.us.oracle.com' is 188928MB, but there is only 188835MB available.
Exiting with error",23436994,warroom,Closed
238,AMPANAMA,"2015-01-20 11:20:05",NULL,Hypervisor,"Smart Clone Master Setup failed while starting deployer host","We had submitted Smart Clone Master Setup topology pointing to REL10 ST6 templates and binaries and it is observed that topology execution has failed while starting deployer host",23454645,OPC_QA_SHARED,Closed
239,AMATHURA,"2015-01-20 11:35:10",NULL,VM,"slc04qfr.us.oracle.com is not accesible","slc04qfr.us.oracle.com is not accessible , can you reboot it.",,fastha,Closed
240,AMALON,"2015-01-21 19:09:28",NULL,Hypervisor,"slcag155 Hypervisor Not Reachable","Hi,

In job 23406319 hypervisor slcag155 stopped being responsive and accessible. Please take a look.",23406319,ovm_general,Closed
241,PWARRIER,"2015-01-22 07:27:11",NULL,Others,"URL not working","https://sdiqa23173118-v0029-hcm.us1.vfarm.oraclecorp.com/hcmCore/faces/HcmFusionHome

Can someone oplease check. We need to release this environemnt to Smartclone team for testing",,OPC_QA_FARM,Closed
242,EKANTER,"2015-01-26 18:55:23",NULL,Cleanup,"job cleanup failed","Job Cleanup repeatedly failed.",23511583,QAFARM,Closed
243,RINSHARM,"2015-01-27 05:59:52",NULL,Hypervisor,"QAFARM  Pod 23173118 are not coming up.","Pod details given below
[aime@slc03gmk ~]$ /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o start_env -envid 23173118


irtual machine status has been reset.
Virtual machine ""slc03zao"" does not exist.
Can't bring on  on a timely manner. Something is Wrong
ERROR: Unable to start VM:",23173118,QAFARM,Closed
244,RINSHARM,"2015-01-27 06:00:51",NULL,VM,"DB nodes of QAFARM master 22129156 not coming up","Master details given below 
/ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o start_env -envid 22129156

DB Host Login Creds: oracle / oracle

faovm.ha.HOST_DB=slc04qev.us.oracle.com

faovm.ha.HOST_DB2=slc04qeu.us.oracle.com",22129156,QAFARM,Closed
245,BARUNACH,"2015-01-27 07:15:56",NULL,null,"Not able to connect DB on SDI environment","Environment Info:
faovm.ha.HOST_FA=slc07zuy.us.oracle.com
faovm.ha.HOST_PRIMARY=slc07zuz.us.oracle.com
faovm.ha.HOST_SECONDARY=slc07zut.us.oracle.com
faovm.ha.HOST_OSN=slc07zuv.us.oracle.com	faovm.ha.HOST_BI=slc07zuu.us.oracle.com
faovm.ha.HOST_OHS=slc07zur.us.oracle.com
faovm.ha.HOST_DB=slc02qxu.us.oracle.com
faovm.ha.HOST_DB2=slc02qxt.us.oracle.com
faovm.ha.HOST_LDAP=slc07zus.us.oracle.com
faovm.ha.HOST_OIM=slc07zux.us.oracle.com
faovm.ha.HOST_WEBGATE=slc07zuw.us.oracle.com",,,Closed
246,MULIEGAD,"2015-01-27 07:42:04",NULL,null,"Factory resourec DB slc03eea/ec down","Below Factory DB is down . Please help us to bring this host up.

faovm.ha.HOST_DB=slc03eea.us.oracle.com faovm.ha.HOST_DB2=slc03eec.us.oracle.com",22366946,OPC_QA_SHARED,Closed
247,KRAMANJA,"2015-01-27 09:13:23",NULL,null,"MC script did not put /fsnadmin  mount entry in /etc/fstab","We have enabled MC in our Farm env using given steps in the automation wiki.

After  QM down time, we found /fsnadmin was not mounted in any of the VMs.

Please enhance the automation script to write /fsnadmin mount in /etc/fstab",NA,warroom,Closed
248,AMPANAMA,"2015-01-27 09:20:45",NULL,Hypervisor,"startvm command for host slc03yyn failed","Post maintainence we tried bringing below vm where SDI_BASE is installed and startvm command failed with error shown below.

[aime@slc04rky ~]$ /usr/bin/perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o startvm -host_name slc03yyn
Powering on.
resetting slc03yyn 
Virtual machine status has been reset.
Powering on.
Can't bring on  on a timely manner. Something is Wrong",22129156,OPC_QA_FARM,Closed
249,RINSHARM,"2015-01-27 11:10:47",NULL,Others,"DB's of QAFARM master 22129156 not coming up","srvctl start instance -d FUSG02_UNIQUE -i FUSG022
PRCD-1027 : Failed to retrieve database FUSG02_UNIQUE
PRCR-1070 : Failed to check if resource ora.fusg02_unique.db is registered
Cannot communicate with crsd",22129156,OPC_QA_SHARED,Closed
250,BKASHYAP,"2015-01-27 11:14:38",NULL,null,"Cleanup of jobs 20572689 and 20717546","Lost the cleanup keys, can you please help cleaning up.",20572689,lifecycle_dev,Closed
251,PWARRIER,"2015-01-27 16:48:18",NULL,VM,"VIP host names not pinging","FR_1_RAC_NODE1_HOST=slc03eue.us.oracle.com 
FR_1_RAC_NODE2_HOST=slc03eui.us.oracle.com 
FR_1_RAC_NODE1_VIP_HOST=slc03eub.us.oracle.com 
FR_1_RAC_NODE2_VIP_HOST=slc03ebj.us.oracle.com 
FR_1_RAC_CLUSTER_VIP_HOST=slc03eua.us.oracle.com 

Was able to start actual host but not VIP and Cluster. SDI ovm-ha-deploy.properties refers to VIP.Can you help to being it up",,OPC_QA_SHARED,Closed
252,EKANTER,"2015-01-28 03:19:08",NULL,Hypervisor,"DEPLOY_TEMPLATE failed",/net/slc06yga.us.oracle.com/scratch/aime/PFA_REMOTE/job_23556237/work/oracle/work/DEPLOY_TEMPLATE/localTaskProcess.err,23556237,QAFARM,Closed
257,AMATHURA,"2015-02-05 06:49:46",NULL,null,"Job 23578619 is already cleaned. Pls change the status","Job 23578619 is already cleaned. Pls change the status",23578619,fastha,Closed
253,JKUTTAPP,"2015-02-02 06:44:07",NULL,NULL,"SDI FA deployments pick two RAC from factory","For the latest Rel9.2 ST13 FA deployment job triggered agains TAS based SDI base, it picks two RAC db from factory resource.
Looks like some farm or topo issue.",23611370,FASAASQA,Open
254,EKANTER,"2015-02-02 22:36:36",NULL,Others,"job consistently fails - out of virtual memory in a pool","Please see bug https://bug.oraclecorp.com/pls/bug/webbug_edit.edit_info_top?rptno=20458000",23662902,QAFARM,Closed
255,MLWU,"2015-02-03 11:36:20",NULL,Others,"Please create CDRM REF topo defs for HCM, CRM, FSCM","Please create CDRM REF topo defs for HCM, CRM, FSCM

Will send the defs in email as it won't fit in this.",,,Closed
256,AMALON,"2015-02-03 16:52:35",NULL,null,"Job Stuck At ""Cleanup Initiated"" Phase for 24hours","Hi,

Job 23665496 doesn't have any assigned h/w for it still stuck at ""cleanup initiated"" phase for the last 24hours. Can you please take a look?

Thank you,

Amit",23665496,ovm_preflight,Closed
258,AMATHURA,"2015-02-05 06:50:22",NULL,null,"Job 23677638 is already cleaned. Pls change the status","Job 23677638 is already cleaned. Pls change the status",23677638,fastha,Closed
259,SWSUDHAK,"2015-02-06 14:03:45",NULL,NFS,"Error during start of VM  'slc06xsg_us_oracle_com'","During Master rehydration startup of vm is erroring out.
2015-02-06T03:26:44.049-08:00] [apps] [ERROR] [] [oracle.apps.fnd.provisioning.ovm.deployfw.cli.DeployControl] [tid: 10] [ecid: 0000KhURtBZBT8Q5IbK6yf1Kp9Tl000000,0] An error occurred during command execution: Error during start of VM  'slc06xsg_us_oracle_com'[[
oracle.apps.fnd.provisioning.ovm.sdk.cli.FAOVMCLIException: An error occurred during command execution: Error during start of VM  'slc06xsg_us_oracle_com'",23473185,OPC_QA_SHARED,Closed
260,AMATHURA,"2015-02-10 07:05:28",NULL,null,"The jobs submitted to fastha pool are not getting allocated with resources","The jobs submitted to fastha pool are not getting allocated with resources eventhough resources are available. Pls help",,fastha,Closed
261,EKANTER,"2015-02-12 22:28:35",NULL,Hypervisor,"request rac failed","dbname: fusiondb service_name: fusiondb db_unique_name: fusiondb
running:  replace into racinfo (farm,dteid,node1,node2,node1vip,node2vip,racclustername,sid1,status) values ('ovm_general','23822542','slc06yek','slc06yel','slc06yem','slc06yen','slc06yeo','fusiondb:1616','REGISTERED');
Unable to determine the free disk space on server pool slcal073.
ERROR: Unable to initizialize shared disk in given time!",23822542,QAFARM,Closed
262,EKANTER,"2015-02-13 00:27:06",NULL,Cleanup,"job cleanup failed","first attempt failed, second attempt got stuck.

http://slc02kas.us.oracle.com:8080/QAFarmUI/jsp/CleanUpLogPopup.jsp?jobid=23822542&status=CLEANING",23822542,QAFARM,Closed
263,PWARRIER,"2015-02-16 06:54:42",NULL,Cleanup,"Cleanup of Job 23690255 failed. DB not released","23690255	FASAASQA	jkuttapp	CLEANUP_FAILED	SDI_FA_REL9.2_GSI_PFA_RAC_HA	N/A	

We need DB resource for GSI on OPC_QA_SHARED. Please help to cleanup this.","23690255 ",FASAASQA,Closed
264,PWARRIER,"2015-02-16 13:06:20",NULL,Cleanup,"Cleanup Key for Job 22481222 is invalid","22481222_fbamooh2uv05ibac2upe7gju9c is shown as invalid. Can you please provide the correct key?",22481222,OPC_QA_SHARED,Closed
265,AMATHURA,"2015-02-16 13:07:38",NULL,Others,"OSN HA logic should be implemented in QA Farm non SDI REL10 step","OSN is supporting HA from REL10 onwards. The same has to be supported by QA Farm non SDI steps. Currently logic is not in place. Pls help",,fastha,Closed
266,AMATHURA,"2015-02-16 13:09:36",NULL,Others,"QAFarm Non SDI Steps should use latest LBR urls","QAFarm Non SDI Steps should use latest LBR urls as the old urls are going to be decommissioned soon according to Sowmya",,fastha,Closed
267,AMATHURA,"2015-02-16 14:23:57",NULL,null,"Pls change the staus of job 23888650 as CLEANED","Pls change the staus of job 23888650 as CLEANED",23888650,fastha,Closed
268,AMATHURA,"2015-02-17 06:58:41",NULL,Others,"[urgent][23897571] CREATE_VM is failing","command ran:
/usr/bin/perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o create_vm -host_name slc05bqd >& /net/slc05bms.us.oracle.com/scratch/aime/PFA_REMOTE/job_23897571/work/oracle/work/CREATE_VM/logs/request_vm_create_16_02_2015_22_40_46_2.93810575708164.log

------------
Debug: create_vm slc05bqd,OVM_OL5U6_X86_64_11203RAC_PVM,slcaj497,slcaj497,admin,OVMadmin,welcome1,8,32000
Error: slc05bqd is alive.. please release it first",23897571,fastha,Closed
269,AMATHURA,"2015-02-17 07:38:42",NULL,Others,"Pls update the status of VMs belonging to gateway ""10.241.112.1"" to AVAILABLE & dte_jobid to INVALID","Pls update the status of VMs belonging to gateway ""10.241.112.1"" to AVAILABLE & dte_jobid to INVALID.
Because we used some vms outside qafarm without notifying qafarm and those are in running state.But actually got picked up for new jobs.Now if we run cleanup, it will delete the vms which is actually being used for other setups",,fastha,Closed
270,AMPANAMA,"2015-02-17 10:30:44",NULL,Others,"15.1.4: Unable to access LBR URLs in SDI environment","Hi, SD_BASE15.1.4: HCM ST7 Order: While testing taleo we found that none of the domain admin console are accessible using external ports. We could see there is some issue with LBR hosts. so, please take a look [Observations] - Unable to launch https://sdiqa23473020-v0046-v0046-hcm.us1.vfarm.oraclecorp.com/hcmCore/faces/HcmFusionHome shown as part of ""./sdictl.sh listsv | grep 23473020""",23473020,OPC_QA_FARM,Closed
271,KRAMANJA,"2015-02-23 06:59:00",NULL,NULL,"P4FA oneoffs block is failing in  GSI env setup","As part of a Rel8 env setup and applying P4FA one-offs label we  triggered job:23934711.   Env setup is completed but P4FAOneOffsPatching1  block got failed. 

BUILD FAILED
 Failed to copy /clonepod/clonelite/output/patching/work/arupatches/opatch/linux64/11.1.0.10.3/OPatch to /clonepod/clonelite/faspot/FASPOT/patch_log_dir/arupatches/opatch/linux64/11.1.0.10.3/OPatch due to java.io.FileNotFoundException /clonepod/clonelite/output/patching/work/arupatches/opatch/linux64/11.1.0.10.3/OPatch",23934711,warroom,Open
272,JKUTTAPP,"2015-02-23 09:14:37",NULL,Hypervisor,"Vm created with wrong /etc/resolv.conf in slcai058","Vms created in the hypervisor slcai058 are not having correct /etc/resolv.conf value",,ovm_preflight,Closed
273,PRTUMMAL,"2015-02-23 10:50:31",NULL,Others,"Need SDI DR in QA Farm","We need SDI infrastructure with DR support (End - To - End)",,,Closed
274,AMATHURA,"2015-02-24 11:55:07",NULL,Others,"request to add new bigip to FADRDev pool","Requesting new big-ip is is failing ERROR: No free bigips available!, due to this all our new env setups are failing 

can you please add 2 more bigips to this pool",,FADRDev,Closed
275,AMATHURA,"2015-02-24 11:57:07",NULL,Cleanup,"cleanup the entries for jobs: 24001811 , 24003145 ,24003146 ,24015870","cleanup for following jobs failed please perform the cleanup in backed. and mark them ask cleaned

 24001811 , 24003145 ,24003146 ,24015870",,FADRDev,Closed
276,AMATHURA,"2015-02-24 12:35:41",NULL,null,"Request to add new pool: fadrem to qa-farm database","we have created new pool : fadrem   and created worker host slc03ouv  (aime/2cool)

can you please add this this to qa-farm database so that it can be list in qa-farm ui.",,,Closed
277,SWSUDHAK,"2015-02-25 15:48:34",NULL,VM,"'Create VM - slc04sxw_us_oracle_com': Execution Failed","2015-02-25T04:12:40.074-08:00] [apps] [ERROR] [] [oracle.apps.fnd.provisioning.ovm.deployfw.cli.DeployControl] [tid: 10] [ecid: 0000Kj0JM0UBT8Q5IbK6yf1KvQKr000000,0] An error occurred during an earlier execution of command 'Create VM - slc04sxw_us_oracle_com': Execution Failed[[",23896013,OPC_QA_SHARED,Closed
278,SRMEKA,"2015-02-26 09:39:07",NULL,null,"Please update DB vm specs in the CDRM topo defs","CRMLARGE_CDRM_REF_REL10 :
6:16:36864:FUSION_DB_HOST:stit_oel5u6:false

GSILARGE_CDRM_REF_REL10 :
6:16:36864:FUSION_DB_HOST:stit_oel5u6:false

HCMLARGE_CDRM_REF_REL10 :
6:16:36864:FUSION_DB_HOST:stit_oel5u6:false",,,Closed
279,MULIEGAD,"2015-02-27 07:31:12",NULL,Others,"Below VM needs to be released","slc06xxi.us.oracle.com
slc06xxh.us.oracle.com
slc06xxa.us.oracle.com
slc06xxb.us.oracle.com

We tried releasing these VMs but getting below error

[aime@adc1140368 13.0]$ /usr/bin/perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o release -host_name slc06xxi -force
Debug: need to stop and deregister from OVS 
ERROR: unknown VM status: (111, 'Connection refused')
Please help us to release these VM's back to pool",23900801,OPC_QA_SHARED,Closed
280,AMATHURA,"2015-02-27 08:51:41",NULL,Others,"Job 24075474  went to failed state","All recently started jobs in fastha pool is going to FAILED state during deployer host creation. When creating deployer host, its creating two vms with same name in hyp slcah815. I rebooted slcah815 , still same issue. Pls help.
Select	
Select to show informationShow
	slc03wrz	1,024	
	Powered Off
	admin	My Workspace	N/A	slcah815
Select	
Select to show informationShow
	slc03wrz	4,000	
	Powered Off
	admin	My Workspace	N/A	slcah815",24075474,fastha,Closed
281,AMATHURA,"2015-03-02 06:08:27",NULL,Others,"Server Pool slcaj497 is in ERROR state","Hi Sowmya,

One of the server pool (slcaj497) in QA Farm OVMM is showing the status as ERROR. But if check the status of Server (slcaj497), OVMM is showing as ACTIVE. Have you ever faced this issue before?

Could you pls help? It has made the above hypervisor to be unusable in QA Farm.

Thanks,
Arul",,fastha,Closed
282,KRAMANJA,"2015-03-03 06:51:58",NULL,Hypervisor,"Please fix memory issues on hypervisor - slcak524","We had seen memory issues with slcak524 hypervisor.

As per the mail chain discussions with Sowmya. after freeing the hypervisor,  raising the ticket to fix memroy issues",NA,warroom,Closed
283,SRMEKA,"2015-03-03 07:55:32",NULL,Hypervisor,"Cleanup has failed for this DTE job : 24013218","Cleanup has failed for this DTE job : 24013218",24013218,QAFARM,Closed
284,AMATHURA,"2015-03-03 08:18:14",NULL,null,"Pls update below jobs status as cleaned","24057190
24074615
24056526
23857931				
24075474",,fastha,Closed
285,AMALON,"2015-03-04 19:32:39",NULL,Others,"Transfer Job Breaks Farm UI Script","When clicking on the transfer job button breaks Farm UI script.

After which the UI script needs to be restarted.",,,Closed
286,YOSAINI,"2015-03-06 05:55:05",NULL,null,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): OVS Space Low Create VM Failed<br />.",HealthCheck,prov_qa_automation,Closed
287,AMATHURA,"2015-03-06 08:08:01",NULL,null,"Add Worker Host to DB","Pool : P2TDEV
Worker Host : slc07bfu
Hypervisor : slcam181",,,Closed
288,AMATHURA,"2015-03-06 08:14:11",NULL,Others,"BIGIPs needed for this Pool P2TDEV","Need atleast 6 BIGIPS for this New pool.
Please allocate BIGIPS for this Pool",,,Closed
289,YOSAINI,"2015-03-06 09:00:31",NULL,Hypervisor,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): OVS Space Low, Create VM Failed<br />",HealthCheck,prov_qa_automation,Closed
290,AMPANAMA,"2015-03-10 05:04:41",NULL,Hypervisor,"Unable to fetch server pool [slcan526] data from OVMM","We are in the process of seeding REL10 ST10 GSI master and SDI task ""AllocateResourcesForFaAndOsnTask"" failed with below error message.

[DEBUG] [REQ CREATE_MASTER_1425908349291] [Not able to fetch server pool [slcan526] data from OVMM.]
[2015-03-10 00:00:25,116] [DEBUG] [REQ CREATE_MASTER_1425908349291] [No server pool metrics data for pool [slcan526].] 

Please take a look and help us resolve this as we are blocked with this order seeding.",24166332,OPC_QA_SHARED,Closed
291,AMATHURA,"2015-03-10 09:07:26",NULL,Others,"request to add 2 new bigip to fadrem pool","we have recently create new pool fadrem, can you please add 2 big-ips to this pool, so that we can create env in this pool",,fadrem,Closed
292,AMPANAMA,"2015-03-10 14:50:38",NULL,VM,"Failed to shutdown VM 'slc05jmd_us_oracle_com' in OVMM pool 'slcak528'","We had submitted delete_master request for job 22271888 and cleanupfaservicetask has failed with below error ""Failed to shutdown VM 'slc05jmd_us_oracle_com' in OVMM pool 'slcak528'"". Also, ""poweroff"" & ""poweron"" buttons for this vm are disabled in OVMM. Please take a look",22271888,OPC_QA_SHARED,Closed
293,SAMETHUK,"2015-03-11 06:22:26",NULL,null,"please cleanup jobs :24167457 , 24167458","we have issued cleanup for jobs: 24167457 ,24167458  but even after 12 hours also status is still showing as CLEANING ,

please perform cleanup these jobs.",24167457,fadrem,Closed
294,NJAYARAM,"2015-03-12 02:58:36",NULL,Hypervisor,"JOBS ARE failing at CREATE_DEPLOYER HOST","New Pool P2TDEV Has been setup,
All the Jobs are failing at CREATE_DEPLOYER HOST

Please note that your job 24287286 has Failed while Creating DEPLOYER_HOST",24287286,P2TDEV,Closed
295,NJAYARAM,"2015-03-12 03:02:22",NULL,Hypervisor,"CLEANUP IS FAILING on the JOBS","24278122, 24280876, - RAN CLEANUP, Not Getting Cleaned
24285238, 24287286 - FAILED, 

Please clean-up all of these, as CREATE_DEPLOYER HOST HAs failed on all of the above",24278122,P2TDEV,Closed
296,YOSAINI,"2015-03-13 09:47:37",NULL,Hypervisor,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_SHARED,Closed
297,YOSAINI,"2015-03-13 06:04:28",NULL,null,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_SHARED,Closed
298,YOSAINI,"2015-03-13 07:07:49",NULL,null,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_SHARED,Closed
299,MULIEGAD,"2015-03-13 10:41:10",NULL,Cleanup,"24313991 is in WAITING_FOR_INTERNAL_FIX","We had triggered one GSI job which used a hypervisor which had issues and later the job chnaged to WAITING_FOR_INTERNAL_FIX status. We are unable to clean this.It errors out as below:
 Cleanup for Jobs in this status --> WAITING_FOR_INTERNAL_FIX is not handled. Please analyze and include handler...",24313991,OPC_QA_SHARED,Closed
300,MULIEGAD,"2015-03-13 10:45:36",NULL,Others,"24314500 Waiting","We triggered GSI and as there was no enough resource , it went into waiting state. But now that we have cleaned one GSI job it shoudl have enough resources to proceed. But th job has not started still.
Hence raising ticket for this issue","24314500 ",OPC_QA_SHARED,Closed
301,SAMETHUK,"2015-03-13 11:25:53",NULL,Hypervisor,"preparehyp.sh  script for slcag789 failing with ERROR  OVS/seed_pool does not exist","vnc:  slc02idh:36 (welcome)
while preparing hyper-visor for slcag788,slcag789,slcag790,slcag791  

prepare hyper visor script is failing with following error

 Server pool ""slcag788"" already exists.
/bin/chmod: cannot access `/OVS/seed_pool': No such file or directory
ERROR: unable to chmod /OVS/seed_pool

steps followed in wiki:
http://aseng-wiki.us.oracle.com/asengwiki/display/ASQA/Move+hypervisor+to+QAFarm+OVM+from+local+OVM",,,Closed
302,YOSAINI,"2015-03-16 06:35:55",NULL,NULL,"Hypervisor Memory Issue","Hypervisor slcag051 needs memory updation (Rebooted)",,prov_qa_automation,Open
303,SWSUDHAK,"2015-03-16 07:04:25",NULL,null,"Could not restart slc03peo","As we had issues with slc03peo.us.oracle.com connectivity 
/bin/tcsh: Too many open files in system
Connection to slc03peo.us.oracle.com closed.
We want to restart this vm. Tried poweroff from OVMM, its still in shutting down state. Pl help. This needs to be up for triaging p1 bug",,OPC_QA_FARM,Closed
304,MULIEGAD,"2015-03-16 10:38:55",NULL,null,"24314500 cleanup failed","The Cleanup request has failed for job 24314500.",24314500,OPC_QA_SHARED,Closed
305,MULIEGAD,"2015-03-16 11:03:05",NULL,null,"24318064 failed to clean","job 24318064 in OPC_QA_FARM is failed.Please help us to clean this job","24318064 ",OPC_QA_FARM,Closed
306,EKANTER,"2015-03-17 01:50:03",NULL,null,"run 24363826 failed","<ERROR> Either WORKDIR is not passed or it is not writable. Setting WORKDIR to . 
send: spawn id exp5 not open
    while executing
""send ""Welcome1\r""""
<ERROR> SCP failed with slc06xpb.us.oracle.com, aime, Welcome1, /u01/ldap_automation_info.txt
send: spawn id exp5 not open
    while executing
""send ""Welcome1\r""""
send: spawn id exp5 not open
    while executing
""send ""Welcome1\r""""
<ERROR> /etc/hosts of FA node not accessable
DEPLOY_TEMPLATE/localTaskProcess.err",24363826,QAFARM,Closed
307,SAMETHUK,"2015-03-17 07:29:30",NULL,Others,"request to add 2 new bigip to fastha pool","can you please add 2 new bigips into the fastha pool",,fastha,Closed
308,BARUNACH,"2015-03-17 07:41:43",NULL,null,"Lost the cleanup key","Lost the clean up key for the job ID. By mistake mail got deleted. Please provide me the cleanup key for the same.",24359979,OPC_QA_SHARED,Closed
309,BARUNACH,"2015-03-17 09:45:11",NULL,null,"Clean Up Failed","Clean up has failed for the job 24359979. Please look into the issue.",24359979,OPC_QA_SHARED,Closed
310,EKANTER,"2015-03-18 03:08:28",NULL,Hypervisor,SETUP_RAC_FOR_DEPLOYMENT.dif,"submitted another job and it is failed again.
Mar 17 12:56 SETUP_RAC_FOR_DEPLOYMENT.dif
right now
Mar 17 23:07
Why doesn't DTE immediately notify user that job failed and instead hangs indefinitely?",24378133,,Work-in-Progress
311,BARUNACH,"2015-03-18 07:07:26",NULL,null,"Clean Up Failed","Clean up has been failed for the job 24373207.",24373207,OPC_QA_FARM,Closed
312,AMPANAMA,"2015-03-18 07:24:46",NULL,Hypervisor,"Not able to fetch server pool [slcal070] data from OVMM","We are in the process of seeding REL10 ST10 GSI master and SDI task ""AllocateResourcesForFaAndOsnTask"" failed [REQ JOB23900801Z] [Not able to fetch server pool [slcal070] data from OVMM.] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:getServerPoolMetrics(161)] []
[2015-03-18 07:12:00,846] [DEBUG] [REQ JOB23900801Z] [No server pool metrics data for pool [slcal070]. Please take a look and help us resolve this as we are blocked with this order seeding.",23900801,OPC_QA_SHARED,Closed
313,TCHUKKA,"2015-03-18 19:58:32",NULL,null,"Request to clean up the job 24377500","We didn't receive cleanup key for 24377500.
Could you plesae help to the job 24377500",24377500,FADRDev,Closed
314,TCHUKKA,"2015-03-18 20:00:48",NULL,NULL,"Request to create New Big IP","We are planning to add few more hypervisors to this pool. could you please help add 4 more new big IPs",,FADRDev,Open
315,AMPANAMA,"2015-03-19 10:15:12",NULL,NULL,"Custom cleanup script failed for job 23567920","Cleanup for JOB 23567920 is failed and below are the error from cleanup.log

++ grep REMOVE_ENTRY_RACK_LIST1
+ return_text1=
+ '[' '' '!=' '' ']'
+ echo 'Deploy Delete block has failed. Hence skipping the zfs storage deletion.'
Deploy Delete block has failed. Hence skipping the zfs storage deletion.",23567920,OPC_QA_FARM,Open
316,YOSAINI,"2015-03-20 08:01:17",NULL,Others,"Hypervisor slcak527 Locked","Please unlock slcak527 after fixing the issue(s): Memory Mismatch (Farm:251860,Hyper:251859), ",HealthCheck,OPC_QA_SHARED,Closed
317,YOSAINI,"2015-03-20 08:12:04",NULL,NULL,"Hypervisor slcal070 Locked","Please unlock slcal070 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:74707), ",HealthCheck,OPC_QA_SHARED,Closed
318,YOSAINI,"2015-03-20 08:13:43",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:98211,Hyper:74658), ",HealthCheck,OPC_QA_SHARED,Closed
319,YOSAINI,"2015-03-22 08:01:13",NULL,NULL,"Hypervisor slcal063 Locked","Please unlock slcal063 after fixing the issue(s): Memory Mismatch (Farm:68051,Hyper:68050), ",HealthCheck,QAFARM,Closed
320,YOSAINI,"2015-03-22 08:05:38",NULL,NULL,"Hypervisor slcal060 Locked","Please unlock slcal060 after fixing the issue(s): Memory Mismatch (Farm:21251,Hyper:21249), ",HealthCheck,QAFARM,Closed
321,YOSAINI,"2015-03-22 08:06:42",NULL,NULL,"Hypervisor slcal061 Locked","Please unlock slcal061 after fixing the issue(s): Memory Mismatch (Farm:164643,Hyper:164642), ",HealthCheck,QAFARM,Closed
322,YOSAINI,"2015-03-22 08:07:16",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Memory Mismatch (Farm:94496,Hyper:92962), ",HealthCheck,QAFARM,Closed
323,YOSAINI,"2015-03-22 08:10:37",NULL,NULL,"Hypervisor slcan522 Locked","Please unlock slcan522 after fixing the issue(s): Memory Mismatch (Farm:52131,Hyper:52129), ",HealthCheck,QAFARM,Closed
324,YOSAINI,"2015-03-23 08:01:17",NULL,NULL,"Hypervisor slcag413 Locked","Please unlock slcag413 after fixing the issue(s): Memory Mismatch (Farm:24597,Hyper:24310), ",HealthCheck,ovm_preflight,Closed
325,YOSAINI,"2015-03-23 08:01:52",NULL,NULL,"Hypervisor slcae232 Locked","Please unlock slcae232 after fixing the issue(s): Memory Mismatch (Farm:5925,Hyper:5639), ",HealthCheck,ovm_preflight,Closed
326,YOSAINI,"2015-03-23 08:02:26",NULL,NULL,"Hypervisor slcae231 Locked","Please unlock slcae231 after fixing the issue(s): Memory Mismatch (Farm:22216,Hyper:21927), ",HealthCheck,ovm_preflight,Closed
327,YOSAINI,"2015-03-23 08:03:00",NULL,NULL,"Hypervisor slcae147 Locked","Please unlock slcae147 after fixing the issue(s): Memory Mismatch (Farm:141000,Hyper:140711), ",HealthCheck,ovm_preflight,Closed
328,YOSAINI,"2015-03-23 08:03:34",NULL,NULL,"Hypervisor slcae148 Locked","Please unlock slcae148 after fixing the issue(s): Memory Mismatch (Farm:22216,Hyper:21927), ",HealthCheck,ovm_preflight,Closed
329,APINNAMR,"2015-03-23 06:30:35",NULL,null,"Clean up failed for job#24411139","We have tried to cleanup the QAFARM job but it is failing to release the env.
Key# 24411139_s03lp5tn0d5n9cpej2slff3cdn",24411139,QAFARM,Closed
330,YOSAINI,"2015-03-25 08:00:43",NULL,NULL,"Hypervisor slcag259 Locked","Please unlock slcag259 after fixing the issue(s): Memory Mismatch (Farm:8901,Hyper:8615), ",HealthCheck,prov_qa_automation,Closed
331,YOSAINI,"2015-03-25 08:01:17",NULL,NULL,"Hypervisor slcaf897 Locked","Please unlock slcaf897 after fixing the issue(s): Memory Mismatch (Farm:1736,Hyper:1447), ",HealthCheck,prov_qa_automation,Closed
332,YOSAINI,"2015-03-25 08:01:51",NULL,NULL,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Memory Mismatch (Farm:1971,Hyper:1969), ",HealthCheck,prov_qa_automation,Closed
333,YOSAINI,"2015-03-25 08:02:25",NULL,NULL,"Hypervisor slcal035 Locked","Please unlock slcal035 after fixing the issue(s): Memory Mismatch (Farm:25891,Hyper:25889), ",HealthCheck,prov_qa_automation,Closed
334,YOSAINI,"2015-03-26 08:00:41",NULL,NULL,"Hypervisor slcae117 Locked","Please unlock slcae117 after fixing the issue(s): Memory Mismatch (Farm:8997,Hyper:8711), ",HealthCheck,OPC_QA_FARM,Closed
335,YOSAINI,"2015-03-26 08:01:16",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Memory Mismatch (Farm:71635,Hyper:68050), ",HealthCheck,OPC_QA_FARM,Closed
336,PWARRIER,"2015-03-26 06:01:13",NULL,null,"Create OVM_TOPOLOGY for REL10 GSI","GSI 
===========
faovm.ha.fa.memory=27648
faovm.ha.fa.post.rehydrate.memory=1536
faovm.ha.primary.memory=114688
faovm.ha.secondary.memory=146432
faovm.ha.grc.memory=17408
faovm.ha.osn.memory=5120
faovm.ha.bi.memory=18432
faovm.ha.ohs.memory=3072
faovm.ha.idm3oid.memory=4096
faovm.ha.idm3mw.memory=12288
faovm.ha.idm3ohs.memory=1536
faovm.ha.auxvm.memory=10240
#faovm.ha.auxvm.scale1.memory=10240",,,Closed
337,PWARRIER,"2015-03-26 06:01:42",NULL,null,"Create OVM_TOPOLOGY for REL10 HCM","faovm.ha.fa.memory=14336
faovm.ha.fa.post.rehydrate.memory=1536
faovm.ha.primary.memory=44032
faovm.ha.secondary.memory=80896
faovm.ha.osn.memory=5120
faovm.ha.bi.memory=18432
faovm.ha.ohs.memory=3072
faovm.ha.idm3oid.memory=4096
faovm.ha.idm3mw.memory=11264
faovm.ha.idm3ohs.memory=1536
faovm.ha.auxvm.memory=10240
#faovm.ha.auxvm.scale1.memory=10240",,,Closed
338,PWARRIER,"2015-03-26 06:02:26",NULL,null,"Create OVM_TOPOLOGY for REL10 CRM","faovm.ha.fa.memory=19456
faovm.ha.fa.post.rehydrate.memory=1536
faovm.ha.primary.memory=30720
faovm.ha.secondary.memory=90624
faovm.ha.osn.memory=5120
faovm.ha.bi.memory=18432
faovm.ha.ohs.memory=3072
faovm.ha.idm3oid.memory=4096
faovm.ha.idm3mw.memory=11264
faovm.ha.idm3ohs.memory=1536
faovm.ha.auxvm.memory=10240
#faovm.ha.auxvm.scale1.memory=10240",,,Closed
339,APINNAMR,"2015-03-26 07:36:44",NULL,null,"Please cleanup the jobs 24472900, 24473702.","Please cleanup the jobs 24472900, 24473702.

Somehow i did not get any mail that has the key of above mentioned jobs. So i do not have a provision to cleanup the jobs. Please do needful.

username: sandmeno","24472900 ",QAFARM,Closed
340,YOSAINI,"2015-03-27 08:00:42",NULL,NULL,"Hypervisor slcak528 Locked","Please unlock slcak528 after fixing the issue(s): Memory Mismatch (Farm:98258,Hyper:49619), ",HealthCheck,OPC_QA_SHARED,Closed
341,YOSAINI,"2015-03-27 08:01:51",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:74707), ",HealthCheck,OPC_QA_SHARED,Closed
342,YOSAINI,"2015-03-27 08:04:08",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:98211,Hyper:74658), ",HealthCheck,OPC_QA_SHARED,Closed
343,YOSAINI,"2015-03-28 08:01:18",NULL,NULL,"Hypervisor slcap026 Locked","Please unlock slcap026 after fixing the issue(s): Memory Mismatch (Farm:7891,Hyper:7889), ",HealthCheck,ovm_general,Closed
344,YOSAINI,"2015-03-29 08:00:43",NULL,NULL,"Hypervisor slcan516 Locked","Please unlock slcan516 after fixing the issue(s): Memory Mismatch (Farm:58787,Hyper:58785), ",HealthCheck,QAFARM,Closed
345,YOSAINI,"2015-03-29 08:04:04",NULL,NULL,"Hypervisor slcal073 Locked","Please unlock slcal073 after fixing the issue(s): Memory Mismatch (Farm:8963,Hyper:8961), ",HealthCheck,QAFARM,Closed
346,YOSAINI,"2015-03-29 08:04:38",NULL,NULL,"Hypervisor slcal069 Locked","Please unlock slcal069 after fixing the issue(s): Memory Mismatch (Farm:88595,Hyper:88594), ",HealthCheck,QAFARM,Closed
347,YOSAINI,"2015-03-29 08:05:47",NULL,NULL,"Hypervisor slcan517 Locked","Please unlock slcan517 after fixing the issue(s): Memory Mismatch (Farm:64419,Hyper:64417), ",HealthCheck,QAFARM,Closed
348,YOSAINI,"2015-03-29 08:08:04",NULL,NULL,"Hypervisor slcan515 Locked","Please unlock slcan515 after fixing the issue(s): Memory Mismatch (Farm:58787,Hyper:58785), ",HealthCheck,QAFARM,Closed
349,YOSAINI,"2015-03-29 08:09:12",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): Memory Mismatch (Farm:235427,Hyper:235426), ",HealthCheck,QAFARM,Closed
350,YOSAINI,"2015-03-30 08:42:11",NULL,NULL,"Hypervisor slcag412 Locked","Please unlock slcag412 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
351,AMPANAMA,"2015-03-30 09:33:28",NULL,null,"Cleanup failed for JOB 23473185","Cleanup failed for this job because of multiple error's seen while cleaning the vm and releasing the resources as shown below
INFO> 29/03/2015 11:47:13 -> ERROR: VM: slc06xse_us_oracle_com cleanup failed
Failed to Release Resources for Job --> 23473185
ERROR: Cleanup is not complete, please contact admin for a manual cleanup",23473185,OPC_QA_SHARED,Closed
352,PWARRIER,"2015-03-30 10:34:26",NULL,Hypervisor,"slcai056 no VMs seen, by hypervisor is not fully free","[aime@slc03wtf ~]$ /net/adc2120135/scratch/aime/qafarm_fwk/ovm_farm_manager_v7.pl -o listvm -farm OPC_QA_FARM | grep slcai056

No VMs are shown here, but hyerpvisor is not free also.

slcai056        245715  251859  996     1000    999     1000    10.244.160.1    OPC_QA_FARM     slc     FORCE",,OPC_QA_FARM,Closed
353,ANHANUMA,"2015-03-30 11:35:17",NULL,null,"ERROR: Creation Failed : slc03hbl is stil in Creating status after 40 Minutes","No logs created in the OVM Manager, slc03hbl still in "" Creating"" status form long time.

from localTaskProcess.log file, below is the message
Debug: create_vm slc03hbl,stit_oel5u6,slcag410,slcag410,admin,OVMadmin,welcome1,12,36864
Virtual machine ""slc03hbl"" is being created. Please check the status.
ERROR: Creation Failed : slc03hbl is stil in Creating status after 40 Minutes",24589675,ovm_preflight,Closed
354,SJANDHYA,"2015-03-31 06:22:47",NULL,null,"Job is in running status but unable to login to the deployer host","Said job is in Running status, but unable to login to the deployer host - slc06yan.us.oracle.com.",24589232,QAFARM,Closed
355,VNALLU,"2015-03-31 10:57:57",NULL,Cleanup,test,"test ticket",,DRQA,Closed
356,VNALLU,"2015-03-31 11:00:52",NULL,Cleanup,test,"perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o add_hostname -gate_way 10.241.112.1 -net_mask 255.255.248.0 -ip_addr 10.241.116.150 -host_name slc05bqe -farm drqa
Adding VM slc05bqe",,,Closed
357,SAMETHUK,"2015-03-31 11:02:02",NULL,null,"adding a vm to -farm is failing with eror","DBD::mysql::st execute failed: Column count doesn't match value count at row 1 at /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl line 8505.",,,Closed
358,YOSAINI,"2015-04-01 08:00:40",NULL,Others,"Hypervisor slcal035 Locked","Please unlock slcal035 after fixing the issue(s): Memory Mismatch (Farm:25891,Hyper:25889), ",HealthCheck,prov_qa_automation,Closed
359,EKANTER,"2015-04-01 23:01:59",NULL,Others,"FA hosts are not accesible after running a job","/net/slc06ygb.us.oracle.com/scratch/aime/PFA_REMOTE/job_24621553/work/oracle/work/CLEANUP_OVM_TOPO/localTaskProcess.log
says that no clean up is performed.
DB hosts are up but FA are not reachable.

[aime@slc06ygb ~]$ ping slc06yfp.us.oracle.com
PING slc06yfp.us.oracle.com (10.244.54.90) 56(84) bytes of data.

--- slc06yfp.us.oracle.com ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2010ms",24621553,,Closed
360,YOSAINI,"2015-04-03 08:01:15",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:74707), ",HealthCheck,OPC_QA_SHARED,Closed
361,YOSAINI,"2015-04-03 08:04:02",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:98211,Hyper:74658), ",HealthCheck,OPC_QA_SHARED,Closed
362,YOSAINI,"2015-04-05 09:08:08",NULL,NULL,"Hypervisor slcan520 Locked","Please unlock slcan520 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
363,MULIEGAD,"2015-04-05 10:09:21",NULL,Others,"slcak529 is full in job 24683362","Hypervisor slcak529 is full. We are unable to create template folders on this. Please help us to create space on this",24683362,OPC_QA_SHARED,Closed
364,YOSAINI,"2015-04-06 08:03:25",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:5075), ",HealthCheck,ovm_preflight,Closed
365,MULIEGAD,"2015-04-06 07:50:08",NULL,Cleanup,"24372400  cleanup failed","Cleanup failed for job 24372400. We wanted to edit cleanup.sh script to proceed with the cleanup. But the cleanup script at deployer host of the job is not editable as it errors out as  chmod: changing permissions of `cleanup.sh': Read-only file system",24372400,OPC_QA_SHARED,Closed
366,SJANDHYA,"2015-04-06 08:42:49",NULL,null,"Unable to connect to deployer host, job is running","Unable to connect to deployer host(slc06xzy.us.oracle.com), job is running",24667775,QAFARM,Closed
367,MABOGEGO,"2015-04-06 11:07:41",NULL,null,"JOB ID:24704063 - cleanup is failing","Hi,

Cleanup is failing for ""JOB ID: 24704063"".. so could you please help us to cleanup??

Key: 24704063_21tkf9gcr47fqq9oviu345pie",24704063,ovm_preflight,Closed
368,YOSAINI,"2015-04-07 09:47:50",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:247763), ",HealthCheck,QAFARM,Closed
369,AMPANAMA,"2015-04-08 11:18:21",NULL,Cleanup,"Cleanup failed for job 24510459","Cleanup failed as cleanup script errored out while cleaning some of the vm as well as while releasing the vm.

Please take a look.",24510459,OPC_QA_SHARED,Closed
370,YOSAINI,"2015-04-10 08:01:15",NULL,NULL,"Hypervisor slcak527 Locked","Please unlock slcak527 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74707), ",HealthCheck,OPC_QA_SHARED,Closed
371,YOSAINI,"2015-04-10 08:03:32",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:93139), ",HealthCheck,OPC_QA_SHARED,Closed
372,YOSAINI,"2015-04-10 08:04:06",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:98211,Hyper:74658), ",HealthCheck,OPC_QA_SHARED,Closed
373,MULIEGAD,"2015-04-10 13:23:10",NULL,Hypervisor,24630035,"During delete of CRM FA request there was issue with slcai051 hypervisor .
 All the VMs on slcai051 was entering into Unknown state on power off
Only when we reset the status from OVMM the status was entering as powered off and the clean up proceeded

Can you take a look into this hypervisor if it needs any restart before i seed the order as this is critical for saas-paas",24630035,OPC_QA_FARM,Closed
374,SJANDHYA,"2015-04-13 07:17:09",NULL,Others,"Dep host is shut down while the job is in running state","Deployer host is shut down while the job is in running state. We verified the code, the code is not shutting down the dep host and even the earlier job_24723428 was successful. Dep host did not get shut down. If the code was shutting down the dep host , the job should not be successful even once.",24746291,QAFARM,Work-in-Progress
375,APINNAMR,"2015-04-14 13:18:25",NULL,Cleanup,"cleanup failed for job# 24704091","Clean up failed for job: 24704091",24704091,QAFARM,Closed
376,YOSAINI,"2015-04-17 08:00:40",NULL,NULL,"Hypervisor slcak528 Locked","Please unlock slcak528 after fixing the issue(s): Memory Mismatch (Farm:105426,Hyper:67539), ",HealthCheck,OPC_QA_SHARED,Closed
377,YOSAINI,"2015-04-17 08:02:53",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:75730), ",HealthCheck,OPC_QA_SHARED,Closed
378,YOSAINI,"2015-04-17 08:03:27",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:98211,Hyper:74658), ",HealthCheck,OPC_QA_SHARED,Closed
379,YOSAINI,"2015-04-19 08:10:21",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): Memory Mismatch (Farm:251811,Hyper:239522), ",HealthCheck,QAFARM,Closed
380,AMPANAMA,"2015-04-20 10:24:23",NULL,Cleanup,"Cleanup job timedout","Cleanup submitted of JOB in OPC_QA_FARM is hung due to below error

<INFO> The Timer --> Cleanup Job Invocation via Telnet has exceeded the pre-defined timeout period of 20 minutes

Please take a look.",24413610,OPC_QA_FARM,Closed
381,GSUNDARE,"2015-04-21 05:33:32",NULL,Others,"GSI FA deploy 24963115 failed due to Resource availability","DTE Job --> 24963115 has failed due to Resource unavailable.

It looks there are no RAC DBs available.

There is one RAC which is in Creation Failed state.   G_RAC_DB_2276  - Creation Failed

Probably, once this is available we can resubmit the GSI job",24963115,OPC_QA_SHARED,Closed
382,TCHUKKA,"2015-04-21 20:45:18",NULL,NULL,"Request to clean up the job 24916714 & 24916715",24916714,24916714,FADRDev,Open
383,TCHUKKA,"2015-04-21 20:47:47",NULL,NULL,"Request for Cleanup Job 24916715","Clean up job hanging after timed out. Please cleanup 24916715",24916715,FADRDev,Open
384,TCHUKKA,"2015-04-22 04:59:51",NULL,NULL,"Request for Cleanup Job 24997380","Please cleanup job 24997380",,FADRDev,Open
385,YOSAINI,"2015-04-23 08:04:06",NULL,NULL,"Hypervisor slcai051 Locked","Please unlock slcai051 after fixing the issue(s): Memory Mismatch (Farm:89185,Hyper:52298), ",HealthCheck,OPC_QA_FARM,Closed
386,YOSAINI,"2015-04-26 08:08:40",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): Memory Mismatch (Farm:251811,Hyper:239522), ",HealthCheck,QAFARM,Closed
387,YOSAINI,"2015-04-27 08:09:20",NULL,NULL,"Hypervisor slcag414 Locked","Please unlock slcag414 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
388,YOSAINI,"2015-04-27 08:14:34",NULL,NULL,"Hypervisor slcag412 Locked","Please unlock slcag412 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
389,YOSAINI,"2015-04-27 08:20:18",NULL,NULL,"Hypervisor slcah767 Locked","Please unlock slcah767 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
390,YOSAINI,"2015-04-27 08:21:01",NULL,NULL,"Hypervisor slcai058 Locked","Please unlock slcai058 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
391,YOSAINI,"2015-04-27 08:21:44",NULL,NULL,"Hypervisor slcaj455 Locked","Please unlock slcaj455 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
392,YOSAINI,"2015-04-27 08:26:28",NULL,NULL,"Hypervisor slcao573 Locked","Please unlock slcao573 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
393,YOSAINI,"2015-04-27 08:31:12",NULL,NULL,"Hypervisor slcao574 Locked","Please unlock slcao574 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_preflight,Closed
394,YOSAINI,"2015-04-28 08:07:03",NULL,Hypervisor,"Hypervisor slcap172 Locked","Please unlock slcap172 after fixing the issue(s): Create VM Failed, ",HealthCheck,saas_paas,Work-in-Progress
395,YOSAINI,"2015-08-04 13:14:58",NULL,NULL,"Hypervisor slcap174 Locked","Please unlock slcap174 after fixing the issue(s): Create VM Failed, ",HealthCheck,saas_paas,Closed
396,YOSAINI,"2015-04-28 08:18:00",NULL,NULL,"Hypervisor slcap176 Locked","Please unlock slcap176 after fixing the issue(s): Create VM Failed, ",HealthCheck,saas_paas,Closed
397,YOSAINI,"2015-04-29 09:08:36",NULL,NULL,"Hypervisor slcag248 Locked","Please unlock slcag248 after fixing the issue(s): Ping VM Failed, ",HealthCheck,prov_qa_automation,Closed
398,SJANDHYA,"2015-04-29 11:56:30",NULL,NULL,"Job is not getting cleaned","The hypervisor which the job is using is having disk failure. Until it comes back the job cant be cleaned. Please raise ticket we will handle after hypervisor comes back.",24827854,,Closed
399,SJANDHYA,"2015-04-29 11:56:30",NULL,NULL,"Job is not getting cleaned","The hypervisor which the job is using is having disk failure. Until it comes back the job cant be cleaned. Please raise ticket we will handle after hypervisor comes back.",24827854,QAFARM,Open
400,YOSAINI,"2015-04-30 08:02:23",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Memory Mismatch (Farm:21669,Hyper:9600), ",HealthCheck,OPC_QA_FARM,Closed
401,BKASHYAP,"2015-04-30 11:21:56",NULL,Cleanup,"Cleanup failed for job 25110883","Please help cleaning up of job 25110883",25110883,lifecycle_dev,Closed
402,YOSAINI,"2015-05-03 08:02:24",NULL,Hypervisor,"Hypervisor slcal067 Locked","Please unlock slcal067 after fixing the issue(s): Memory Mismatch (Farm:251853,Hyper:247763), ",HealthCheck,QAFARM,Closed
403,YOSAINI,"2015-05-04 08:01:50",NULL,VM,"Hypervisor slcaj455 Locked","Please unlock slcaj455 after fixing the issue(s): Memory Mismatch (Farm:1316,Hyper:-734), ",HealthCheck,ovm_preflight,Work-in-Progress
404,MULIEGAD,"2015-05-04 03:07:51",NULL,VM,"slc04zir is not created and started in SDI base topo","We are facing issue with creating and starting VM  slc04zir. When SDI base topology is run, all hosts are created successfully. But slc04zir gives issue.
In this current run this host is assigned to TASC and all blocks related to TASC fails due to this issue.

Please help us to resolve this.

Sent mail with details.",25161673,OPC_QA_FARM,Closed
405,AMATHURA,"2015-05-04 10:40:01",NULL,null,"Pls create new pool in QAFARM with name outageqa","Could you pls help in creating a new pool with name ""outageqa"" in QAFARM with worker host as ""slc02hnh""?

[aime@slc06chu ~]$ perl
/ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl
-o addfarm -farm outageqa
Done",,,Closed
406,AMATHURA,"2015-05-04 17:14:02",NULL,Others,"Pls increase server pool column to more than 20 characters","Pls increase server pool column to more than 20 characters in QAFARM DB",,,Closed
407,AMATHURA,"2015-05-05 15:17:48",NULL,Others,"Pls create new pool in QAFARM with name faseqqa","Pls create new pool in QAFARM with name faseqqa",,,Closed
408,AMATHURA,"2015-05-05 15:19:57",NULL,NULL,"Pls remove slcae82bfvm01 from QAFARM DB","Pls remove slcae82bfvm01 from QAFARM DB",,outageqa,Closed
409,AMATHURA,"2015-05-05 15:23:18",NULL,Others,"Register hyp slcao149 to QAFARM OVMM is throwing error","We registered hyp slcao149 to QAFARM OVMM with server pool name as ae82_outage_infra_pool. But with pool name different than hyp is causing issues while creating VM. So we unregistered slcao149 from QAFARM OVMM. When we try to re-register its erroring out saying server slcao149 already exists. Pls cleanup traces from QAFARM OVMM DB.",,outageqa,Closed
410,MULIEGAD,"2015-05-05 16:00:51",NULL,Others,"slc08but VM has issues with starting","Debug: create_vm slc08but,stit_oel5u6,slcan522,slcan522,admin,OVMadmin,welcome1,6,32768
Caught exception while handling request: oracle.ovs.biz.exception.NetworkCardException: OVM-4105 MA",25208867,QAFARM,Closed
411,AMATHURA,"2015-05-05 16:57:55",NULL,Others,"Not receiving job received/accepted/completed mails in fastha pool","Not receiving job received/accepted/completed mails in fastha pool for any jobs submitted. Pls check.",,fastha,Closed
412,EKANTER,"2015-05-05 18:46:32",NULL,null,"P4FAOneOffsPatching1 fails","/net/slc06xpf.us.oracle.com/scratch/aime/PFA_REMOTE/job_25220511/work/oracle/work/P4FAOneOffsPatching1/localTaskProcess.err
unzip:  cannot find or open /net/slcnas570/export/P4FALite/p4falite_REL9.zip, /net/slcnas570/export/P4FALite/p4falite_REL9.zip.zip or /net/slcnas570/export/P4FALite/p4falite_REL9.zip.ZIP.

file /net/slcnas570/export/P4FALite/p4falite_REL9.zip exist but autofs is not started, at least on FA hosts.",25220511,QAFARM,Closed
413,SAMETHUK,"2015-05-06 04:11:23",NULL,NULL,"please cleanup job: 25235699","I have not received  cleanup key for job:25235699,

please cleanup this job.",25235699,fastha,Closed
414,AMATHURA,"2015-05-06 04:44:11",NULL,null,"Jobs are going to waiting state even though resources are available","Jobs are going to waiting state even though resources are available",25236038,fastha,Closed
488,MCAINE,"2015-06-17 23:32:56",NULL,NULL,"Need job 26138803 cleaned up","Hi, due to a mail quota issue - and a brain issue on my part - I no longer have the cleanup key for this job.
Can it please be cleaned?

Thanks",26138803,BI,Closed
415,BKASHYAP,"2015-05-06 08:23:10",NULL,NULL,"Cleanup of 25187367","I deleted the mail having the cleanup id, please cleanup job 25187367","25187367 	",lifecycle_dev,Closed
416,SWSUDHAK,"2015-05-06 11:46:11",NULL,NULL,"22270935 cleanup failed","Its showing error message and failed to release resource and unknown VM status.",22270935,OPC_QA_FARM,Work-in-Progress
417,AMATHURA,"2015-05-06 12:27:56",NULL,NULL,"Pls update the machine column as dummy host for the below vms","[aime@slc06chu ~]$ perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o listvm -farm fastha | grep AVAILABLE | grep -v dummyhost | awk '{ print $1}'
slc04qnp
slc04qno
slc04qnn
slc04qnl
slc04qnk
slc04qnj
slc04qng
slc04qnf
slc04qne",,fastha,Closed
418,AMATHURA,"2015-05-06 13:43:56",NULL,Cleanup,"QAFARM VM deletion issue","For the below two jobs, even though the cleanup was successful - few of the vms associated with those jobs were still in running state in QAFARM OVMM:

24265332
24265781

I have manually deleted the VMs from QAFARM OVMM for time being. Pls check the root cause.",24265781,fastha,Closed
419,EKANTER,"2015-05-06 20:28:29",NULL,NULL,"unable to cleanup the job","key hasn't been sent to me, as well as no messages from farm whatsoever.",25220869,QAFARM,Closed
420,MULIEGAD,"2015-05-07 09:00:54",NULL,NULL,"25280157 is not getting resource when HV is free","We submitted HCM job SDI_FA_REL10_HCM. And the job is still in status WAITING FOR FREE HV. We have cleaned up two jobs(CRM/HCM) to make space for it but still it fails to allocate resource.","25280157 ",OPC_QA_FARM,Closed
421,YOSAINI,"2015-05-08 08:00:42",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
422,YOSAINI,"2015-05-08 08:01:50",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:161235,Hyper:86482), ",HealthCheck,OPC_QA_SHARED,Closed
423,KRAMANJA,"2015-05-08 10:23:42",NULL,Others,"25135609 : job got struck at ""STARTING_DEPLOYER_HOST"" state","Our SDI base 15.2.4 setup job 25135609  got struck at ""STARTING_DEPLOYER_HOST"" state",25135609,warroom,Closed
424,MULIEGAD,"2015-05-08 16:09:34",NULL,Cleanup,"23900801 is cleaned from UI but resources not released","This job in my name is cleaned, but the resources are not released. Please help us to cleanup these vms",23900801,OPC_QA_SHARED,Closed
425,YOSAINI,"2015-05-11 08:04:40",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:1826), ",HealthCheck,ovm_preflight,Closed
426,YOSAINI,"2015-05-11 09:13:10",NULL,NULL,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): Ping VM Failed, ",HealthCheck,ovm_preflight,Closed
427,MULIEGAD,"2015-05-11 09:37:33",NULL,NULL,"25334613 fails to cleanup","25334613 job fails with below error.
xecuting Command ----> /usr/bin/perl /net/slc05muq/scratch/aime/FASAASQA_Custom/ovm_farm_manager_v7.pl -o cleanalldte -dteid 25334613 -key <HIDDEN KEY>
DBI connect(':','',...) failed: Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) at /net/slc05muq/scratch/aime/FASAASQA_Custom/ovm_farm_manager_v7.pl line 8519",25334613,OPC_QA_FARM,Closed
428,PWARRIER,"2015-05-12 06:00:21",NULL,NULL,"error while runnign farm manager commands","while running the ovm farm manager commands, I am getting error,

aime@slc03wsz OPC_PATCHING]$  /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager.pl -o listhv -farm OPC_QA_SHARED 
DBI connect('ovm_farm_db:adc2120708.us.oracle.com','ovm_farm_user',...) failed: Can't connect to MySQL server on 'adc2120708.us.oracle.com' (111) at /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager.pl line 5308",,,Closed
429,YOSAINI,"2015-05-14 08:01:17",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Memory Mismatch (Farm:70611,Hyper:69027), ",HealthCheck,OPC_QA_FARM,Closed
430,YOSAINI,"2015-05-14 08:04:08",NULL,NULL,"Hypervisor slcai051 Locked","Please unlock slcai051 after fixing the issue(s): Memory Mismatch (Farm:63539,Hyper:48810), ",HealthCheck,OPC_QA_FARM,Closed
431,AMATHURA,"2015-05-14 11:22:45",NULL,NULL,"Cleanup failed for job 25386363","Cleanup failed for job 25386363",25386363,warroom,Closed
432,YOSAINI,"2015-05-15 08:00:42",NULL,NULL,"Hypervisor slcak528 Locked","Please unlock slcak528 after fixing the issue(s): Memory Mismatch (Farm:161234,Hyper:77266), ",HealthCheck,OPC_QA_SHARED,Closed
433,YOSAINI,"2015-05-15 08:01:16",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
434,YOSAINI,"2015-05-15 08:02:24",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:95699), ",HealthCheck,OPC_QA_SHARED,Closed
435,AMATHURA,"2015-05-19 11:30:09",NULL,NULL,"Adding VM is failing QAFARM","perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o add_hostname -gate_way 10.249.108.1 -net_mask 255.255.254.0 -ip_addr 10.249.109.176 -host_name slcae82bevm01 -farm outageqa
Adding VM slcae82bevm01
DBD::mysql::st execute failed: Column count doesn't match value count at row 1",,outageqa,Closed
436,SWSUDHAK,"2015-05-19 16:50:37",NULL,NULL,"SDI_FA_REL10_HCM_PFA_RAC_HA requires one additional vm","Additional OSN HA support came in the middle of Rel10 release.
This OVM_TOPOLOGY - SDI_FA_REL10_HCM_PFA_RAC_HA was created before that. 
Can you please add the extra vm ?

13:2:5120:FAGRP_OSN_HOST2:stit_oel5u6:false",,OPC_QA_SHARED,Closed
437,SWSUDHAK,"2015-05-20 06:19:18",NULL,Cleanup,"Seeing Farm_Error for timed out jobs","Timed out jobs after waiting for resources is failing with FARM_ERROR and is not getting cleaned",25559447,OPC_QA_SHARED,Closed
438,AMATHURA,"2015-05-20 06:44:04",NULL,Others,"Job status for 25554774 is FARM_ERROR","Please help resolve this issue.","25554774 ",outageqa,Closed
439,KRAMANJA,"2015-05-20 15:14:46",NULL,Cleanup,"job-25467539 cleanup job failed","Cleanup request for job-25467539  is in failed state. Please look inot this .",25467539,warroom,Closed
440,SWSUDHAK,"2015-05-21 12:52:42",NULL,NULL,"25448417 clean not happening","25448417 job cleanup is not moving ahead for more than an hour. seeing below message
<INFO> The Timer --> Cleanup Job Invocation via Telnet has exceeded the pre-defined timeout period of 20 minutes",25448417,OPC_QA_FARM,Closed
441,JKUTTAPP,"2015-05-21 13:05:21",NULL,Hypervisor,"VM creation hangs with ""Creating"" status","In the hypervisor slcal074 hitting vm creation issues frequently. In OVMM vm status hangs at ""Creating"". In ovs_operations.log below error is there:
Seeing issues with hypervisor slcal074 while creating vms.
During deployment, vm creation hangs at ""Creating"" status in OVMM. No logs are added in OVMM but I can see the following exception in ovs_operation.log

""2015-05-21 11:37:07"" ERROR=> new_async_task_ext: failed! ->cp_vm () => <Exception: Failed to save the atask information into atask.db",,QAFARM,Closed
442,YOSAINI,"2015-05-22 08:01:15",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
443,EKANTER,"2015-05-22 20:57:29",NULL,NULL,"cleaup failed.","job failed, and subsequent clean up is stuck.",25606154,QAFARM,Closed
444,EKANTER,"2015-05-27 03:25:26",NULL,NULL,"job consistently fails","I am running CRM job from this page
http://aseng-wiki.us.oracle.com/asengwiki/pages/viewpage.action?pageId=5181999621
and its consistently fails",25702710,QAFARM,Open
445,YOSAINI,"2015-05-28 08:02:27",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Memory Mismatch (Farm:5169,Hyper:-623), ",HealthCheck,OPC_QA_FARM,Closed
446,AMALON,"2015-05-29 00:26:57",NULL,NULL,"Jobs In QA Farm Fail To Start","Jobs are pending in ""waiting"" state and error out with ""Farm Error""",25757101,QAFARM,Open
447,YOSAINI,"2015-05-29 08:00:41",NULL,NULL,"Hypervisor slcak528 Locked","Please unlock slcak528 after fixing the issue(s): Memory Mismatch (Farm:161234,Hyper:77266), ",HealthCheck,OPC_QA_SHARED,Closed
448,YOSAINI,"2015-05-29 08:01:15",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
449,YOSAINI,"2015-05-29 08:02:57",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:95699), ",HealthCheck,OPC_QA_SHARED,Closed
450,SAMETHUK,"2015-05-29 06:39:26",NULL,NULL,"please cleanup jobs: 25442527,25468051,25629797","please clean the following jobs:

25442527,25468051,25629797",,fastha,Closed
451,AMPANAMA,"2015-05-29 07:03:53",NULL,Hardware,"Issue with allocating resources for job submitted in OPC_QA_SHARED","we had submitted DTE job to provision REL9 order and JOB remained in this ?WAITING_FOR_FREEHV? state for long period of time and after sometime it was automatically cleaned due to timeout.
Looks like there is some issue with allocating resources. Please take a look
OLD JOB ID which got cleanedup automatically: 25745123",25761805,OPC_QA_SHARED,Closed
452,SBALASUN,"2015-05-29 11:44:33",NULL,Hypervisor,"VMs not responding for job 25357306","VMs of job 25357306 are not responding. Tried rebooting the VMs from ovmm url. But the VM goes to initializing state and does not progress after that.

Please help in resolving the issue.",25357306,OPC_QA_FARM,Closed
453,YOSAINI,"2015-05-31 08:02:59",NULL,Others,"Hypervisor slcal067 Locked","Please unlock slcal067 after fixing the issue(s): Memory Mismatch (Farm:70394,Hyper:19714), ",HealthCheck,QAFARM,Closed
454,YOSAINI,"2015-05-31 08:07:33",NULL,NULL,"Hypervisor slcan515 Locked","Please unlock slcan515 after fixing the issue(s): Memory Mismatch (Farm:70563,Hyper:69025), ",HealthCheck,QAFARM,Closed
455,YOSAINI,"2015-06-01 08:02:58",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Memory Mismatch (Farm:207139,Hyper:61859), ",HealthCheck,ovm_preflight,Closed
456,MULIEGAD,"2015-06-02 07:38:33",NULL,NULL,"25838855 job fails to start deployer host","25838855 job failed to start deployer host
slc04sxy is the deployer host allocated for this job and we are able to connect to this host as aime/2cool.
Please let us know how we can overcome this issue.","25838855 ",OPC_QA_FARM,Closed
457,GSUNDARE,"2015-06-02 10:19:55",NULL,NULL,"Correct the Memory Values for slcah766","Regarding job 25661515, 
There is a mismatch in memory consumption of two vms on the hypervisor slcah766 and OVM_TOPO Spec. which is causing an issue and the hypervisor is FORCE locked.

The VMs are:
FAGRP_PRIMARY_HOST:stit_oel5u6:false,7:12:90624
FAGRP_SECONDARY_HOST:stit_oel5u6:false,8:6:18432

But the memory consumed on hyp. is (16000 + 100352)
-----------
Please correct the Memory values as 3 GB was used to create a new APEX VM and there was NO space",25661515,,Closed
458,PWARRIER,"2015-06-03 10:41:56",NULL,NULL,"configure adc1140271 to be Worker host for farm - farm_ovs328","I have created a new farm farm_ovs328 with worker host as my devops machine adc1140271.us.oracle.com.

Per my discussion with Sowmya, we need to configure this as the worker host. Please do the needful.",,farm_ovs328,Closed
459,MULIEGAD,"2015-06-03 11:11:10",NULL,Hypervisor,"25860838 job fails to start vm slc03wtf","CreateVM command fails on slc03wtf VM.

Maximum Memory Size has been changed to ""20000"".
Caught exception while handling request: oracle.ovs.biz.exception.InvalidMemorySizeException: OVM-4005 The largest amount of virtual memory that can be allocated is 1,024 MB.

Hypervisor is listed as below for this VM:
slc03wtf        10.245.145.110  slcah816        20000   4       10.245.144.1    REGISTERED      25860838        OPC_QA_FARM     adcgdb05","25860838 ",OPC_QA_FARM,Closed
460,PWARRIER,"2015-06-03 12:18:18",NULL,NULL,"need help to move IPs for hypervisors slcai057, slcai059,slcai056 and slcah811 to new pool","need help to move IPs for hypervisors slcai057, slcai059,slcai056 and slcah811 to new pool farm_ovs328. Mail sent with details",,farm_ovs328,Closed
461,KRAMANJA,"2015-06-03 12:56:17",NULL,NULL,"MC  enabling is failing on a SDI based env","In a SDI based env setup,  already  /fsnadmin mount is created.  And MC enablement script is failing.",25600489,warroom,Closed
462,YOSAINI,"2015-06-05 08:00:44",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
463,SAMETHUK,"2015-06-05 07:07:19",NULL,NULL,"please cleanup jobs: 25747414,25762509,25768961,25769095,25787988,25795814,25804340,25807632","can you please perform cleanup for following jobs

25747414,25762509,25768961,25769095,25787988,25795814,25804340,25807632",,fastha,Closed
464,YOSAINI,"2015-06-07 09:58:15",NULL,NULL,"Hypervisor slcal039 Locked","Please unlock slcal039 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
465,MCAINE,"2015-06-07 16:41:39",NULL,Others,"Jobs on BI farm failing","So far no jobs have completed on BI Farm, they never even get picked up by the pool.
Is this really a case of insufficient resources?",,BI,Closed
466,VIKUKUMA,"2015-06-09 05:00:54",NULL,Others,"Failed to create/register the new template with ovmn","Hi,

   We are trying to register JCS assembly templ with OVMN .

Issue :

    Status shows importing since long time (> 7 hr)


Templ Name - 2fUX2YLPHZYYW_49AdiOFvR9bYW

Server pool- slcai053

Current Status - Importing",,OPC_QA_FARM,Closed
467,MCAINE,"2015-06-09 17:44:42",NULL,Hypervisor,"Adding additional HV","Please add slcan619 to BI Farm.
Details can be found here:
http://devops.oraclecorp.com/host/slcan619/detail/

Please let me know if you need me to do anything",,BI,Closed
468,AMATHURA,"2015-06-10 07:25:35",NULL,Others,"RAC Setups are failing in our recent jobs","(farm,dteid,node1,node2,node1vip,node2vip,racclustername,sid1,status) values ('ovm_general','26006946','slc05jlf','slc05jlg','slc05jlh','slc05jli','slc05jlj','fusiondb:1616','REGISTERED');
Shared virtual disk ""26006946_1"" is being created. Please check the status.
Shared virtual disk ""26006946_2"" is being created. Please check the status.
Shared virtual disk ""26006946_3"" is being created. Please check
ERROR: Unable to initizialize shared disk in given time!",26006947,fastha,Closed
469,YOSAINI,"2015-06-11 08:02:24",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Memory Mismatch (Farm:129877,Hyper:117810), ",HealthCheck,OPC_QA_FARM,Closed
470,MULIEGAD,"2015-06-11 07:34:06",NULL,Others,"24683362 job cleanup issue","I was facing issue with cleanup of GSI run as we have reused the resources many times. 
Hence wanted to run only cleanup topology which takes care of releasing the resources.

But I am facing issue changing cleanup.sh @/net/slc05jmm/scratch/aime/work/cleanup.sh , It errors out as its readonly file system","24683362 ",,Closed
471,MULIEGAD,"2015-06-11 11:29:43",NULL,Others,"Jobs on farm_ovs328 fails to accept job with factory resource","Submitted a HCM job on farm_ovs328 and it failed with below error:
Please note that the your Job --> 26058447 is rejected because the FARM farm_ovs328 is not authorized to use Factory Resources",,farm_ovs328,Closed
472,YOSAINI,"2015-06-12 08:00:43",NULL,NULL,"Hypervisor slcak527 Locked","Please unlock slcak527 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:93139), ",HealthCheck,OPC_QA_SHARED,Closed
473,YOSAINI,"2015-06-12 08:02:17",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Memory Mismatch (Farm:141000,Hyper:136711), ",HealthCheck,OPC_QA_SHARED,Closed
474,YOSAINI,"2015-06-12 08:02:51",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
475,YOSAINI,"2015-06-12 08:04:00",NULL,NULL,"Hypervisor slcal070 Locked","Please unlock slcal070 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74707), ",HealthCheck,OPC_QA_SHARED,Closed
476,YOSAINI,"2015-06-12 08:04:34",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:95699), ",HealthCheck,OPC_QA_SHARED,Closed
477,GSUNDARE,"2015-06-12 06:11:03",NULL,Cleanup,"cleanup of the job 26057431  has failed","cleanup of the job 26057431  has failed

Error detected in Output Line --> ERROR: Cleanup is not complete, please contact admin for a manual cleanup
<INFO> 11/06/2015 11:03:33 ->
Failed to Release Resources for Job --> 26057431

Please cleanup the job and release the resource",26057431,,Closed
478,GSUNDARE,"2015-06-14 06:04:06",NULL,Cleanup,"Farm job 26081084 cleanup not identifying the Key","job 26081084 has Failed while Creating DEPLOYER_HOST

Trying to cleanup the job through QAFarm UI
Cleanup Key - 26081084_mi3g9d6kfuia182pj9hv7n7cj1

It throws error - ""NOT a Valid Key""

Please cleanup the job and free resources",26081084,,Closed
479,YOSAINI,"2015-06-15 08:53:55",NULL,NULL,"Hypervisor slcag412 Locked","Please unlock slcag412 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,ovm_preflight,Closed
480,GSUNDARE,"2015-06-15 05:27:53",NULL,Cleanup,"Unlock hypervisor adcgbf23 and cleanup job 26081084","Cleanup of the job 26081084 has failed 
ERROR: Unable to shutdown RAC node1: adc6181302
ERROR: Unable to shutdown RAC node2: adc6181303

Both the RAC VMs are not pingable.

The Hypervisor adcgbf23 is in force, can't be unlocked due to which NOT able to submit new requests.

Please cleanup the job and unlock the hypervisor",26081084,OPC_QA_SHARED,Closed
481,GSUNDARE,"2015-06-15 06:51:20",NULL,Others,"cleanup job 26127149 from UI","Job has not generated the cleanup key and has not started.

This job 26127149 is aborted but still shows as Pending Approval on the UI

Please cleanup the job from UI.",26127149,OPC_QA_SHARED,Closed
482,VIKUKUMA,"2015-06-15 09:01:26",NULL,Others,"15.2.6 :JCS BIG IP Url is not working","JCS BIG IP urls are not working -

https://console-v0028-java.us1.vfarm.oraclecorp.com/em/faces/javaservice?serviceName=myJAVA21&identityGroup=sdiqa25621946-test 



DEPLOYER_HOST=slc04zjb.us.oracle.com 

SDI_DEPLOYER_HOST=slc03wcf.us.oracle.com",,,Closed
483,APINNAMR,"2015-06-15 11:42:56",NULL,NULL,"Clean up failed for job#26079128","Clean up failed for job#26079128",26079128,QAFARM,Closed
484,MCAINE,"2015-06-15 21:21:27",NULL,Others,"Job 26138803 failed","Job fails with error:
""Failed while setting Proxy port for FA SDI""

Am I missing something?",26138803,BI,Closed
485,GSUNDARE,"2015-06-16 07:38:20",NULL,NULL,"Cleanup of the job 25282759 has failed","Cleanup of the job 25282759 has failed

Deploy Delete block has failed. Hence skipping the zfs storage deletion.

Please cleanup",25282759,OPC_QA_SHARED,Closed
486,MULIEGAD,"2015-06-17 05:28:24",NULL,Hypervisor,"Not able to fetch server pool [slcak527] data from OVMM","[2015-06-16 17:49:43,113] [DEBUG] [REQ 500044156_500045208] [Not able to fetch server pool [slcak527] data from OVMM.] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:getServerPoolMetrics(168)] []
[2015-06-16 17:49:43,113] [DEBUG] [REQ 500044156_500045208] [No server pool metrics data for pool [slcak527].] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:checkResourceLimit(133)] []",26053523,OPC_QA_SHARED,Closed
487,AMATHURA,"2015-06-17 08:49:23",NULL,NULL,"Pls change the below jobs' status as CLEANED","25998261
25998262
26006946
26006947
26037401",,fastha,Closed
489,MCAINE,"2015-06-17 23:33:41",NULL,Others,"Admin for BI Pool","HI,
can I become an admin for this pool please, so that if ever a job needs approval I can do do.

Thanks",n/a,BI,Closed
490,SBALASUN,"2015-06-18 05:57:19",NULL,NFS,"Not able to login to VMs slc06xsm and slc06xse","Login to hosts slc06xsm.us.oracle.com & slc06xse.us.oracle.com hangs.

Tried shutting down the VMs from OVMM as well as using ovm_farm_manager_v7.pl but the hosts doesn't shut down.

Please help in rebooting these VMs.",25440862,OPC_QA_SHARED,Closed
491,YOSAINI,"2015-06-19 08:00:43",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Memory Mismatch (Farm:141000,Hyper:136711), ",HealthCheck,OPC_QA_SHARED,Closed
492,YOSAINI,"2015-06-19 08:01:17",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
493,YOSAINI,"2015-06-19 08:02:26",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:83410), ",HealthCheck,OPC_QA_SHARED,Closed
494,ANHANUMA,"2015-06-19 10:59:29",NULL,NULL,"Triggered jobs are not picking up in ovm_preflight pool","we have enough resources and worker host is active. but still we jobs are not picking up. pls check on this",,ovm_preflight,Closed
495,AMATHURA,"2015-06-19 14:55:45",NULL,NULL,"Pls release the following two vms","Pls release the following two vms

slc00ygq        110.240.33.62   dummyhost       20000   6       10.240.32.1     AUTOLOCK-PING   INVALID warroom adcgdb05
slc00ygp        10.240.33.61    dummyhost       32768   2       10.240.32.1     AUTOLOCK-PING   INVALID warroom adcgdb05",,warroom,Closed
496,YOSAINI,"2015-06-21 08:09:17",NULL,NULL,"Hypervisor slcan520 Locked","Please unlock slcan520 after fixing the issue(s): Memory Mismatch (Farm:17824,Hyper:1441), ",HealthCheck,QAFARM,Closed
497,YOSAINI,"2015-06-22 08:00:41",NULL,NULL,"Hypervisor slcag414 Locked","Please unlock slcag414 after fixing the issue(s): Memory Mismatch (Farm:2581,Hyper:246), ",HealthCheck,ovm_preflight,Closed
498,SWSUDHAK,"2015-06-23 05:55:00",NULL,Others,"Cleaned up job is still holding factory resource","Cleaned job:25305265 on 08/05/2015. 
But the RACDB factory resource is still not released.
Due to this new run is not getting picked up as there is no resources for RACDB.",25305265,OPC_QA_SHARED,Closed
499,YOSAINI,"2015-06-23 07:07:50",NULL,NULL,"Hypervisor slcai656 Locked","Please unlock slcai656 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:152998,Hyper:95651)<br />",HealthCheck,OPC_QA_SHARED,Closed
500,YOSAINI,"2015-06-23 07:07:54",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): In FORCE StateMemory Mismatch (Farm:152998,Hyper:95651), ",HealthCheck,OPC_QA_SHARED,Closed
501,YOSAINI,"2015-06-23 07:07:58",NULL,NULL,"Hypervisor slcal041 Locked","Please unlock slcal041 after fixing the issue(s): Memory Mismatch (Farm:135590,Hyper:77779), ",HealthCheck,OPC_QA_SHARED,Closed
502,YOSAINI,"2015-06-23 07:08:03",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): In FORCE StateMemory Mismatch (Farm:135590,Hyper:77779), ",HealthCheck,OPC_QA_SHARED,Closed
503,MCAINE,"2015-06-24 22:23:37",NULL,NULL,"Jobs on BI farm failing","Jobs fail with Creating DEPLOYER_HOST",26343485,BI,Closed
504,YOSAINI,"2015-06-25 08:03:35",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Memory Mismatch (Farm:129877,Hyper:117810), ",HealthCheck,OPC_QA_FARM,Closed
505,KRAMANJA,"2015-06-25 05:07:01",NULL,NULL,"SDI env setup is having a different /fsnadmin mounted","HI,

We had setup a Rel10 SDI env setup using QAFarm topologies.

After the env creation,  we noticed that different /fsnadmin  got mounted at the end of env creation. 

slcnas570:/export/FSNADMIN

Our standard fsnadmin is  /fsnadmin (slcnas551.us.oracle.com:/export/mcollective)",26257044,warroom,Closed
506,GSUNDARE,"2015-06-25 08:24:10",NULL,Hypervisor,"hypervisor - adcgbf23   issues","Rel10 ST17 HCM setup on 15.2.6  SDI  is failing due to QA Farm issues ( probably with the hypervisor - adcgbf23 ).  
Run 1 (Current Run)
Error - Skipping Rack [26328884] because of exception Cannot select rack 26328884 because it does not have enough VM memory
Analysis ? Checking the localTaskProcess.log of PREPARE_SDI_FOR_FA1.2015_6_24.0_0_6.3183  shows the command addfadb has hung due to missing hostname.
		When traced back, Block SETUP_RAC_FOR_Dwith  ?Error: node1: adc6181302 is alive...""",26328884,,Closed
507,MCAINE,"2015-06-25 20:49:55",NULL,Cleanup,"26364211 Timed out","Error received:
Please note that your job 26364211 has timedout after trying to wait for resources to be fulfilled for 240 minutes on Farm --> BI","26364211 ",BI,Closed
508,YOSAINI,"2015-08-04 10:42:19",NULL,NULL,"Hypervisor slcak527 Locked","Please unlock slcak527 after fixing the issue(s): Memory Mismatch (Farm:98259,Hyper:73682), ",HealthCheck,OPC_QA_SHARED,Closed
509,YOSAINI,"2015-06-26 08:01:16",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Memory Mismatch (Farm:141000,Hyper:136711), ",HealthCheck,OPC_QA_SHARED,Closed
510,YOSAINI,"2015-06-26 08:01:50",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74660), ",HealthCheck,OPC_QA_SHARED,Closed
511,YOSAINI,"2015-06-26 08:02:59",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:105379,Hyper:82850), ",HealthCheck,OPC_QA_SHARED,Closed
512,GSUNDARE,"2015-06-26 10:24:39",NULL,NULL,"Cleanup of job 26328884  failed","Cleanup of job 26328884 has failed due to error of Deploy Delete block.

Please cleanup the job",26328884,OPC_QA_SHARED,Closed
513,YOSAINI,"2015-06-27 08:41:24",NULL,NULL,"Hypervisor slcap026 Locked","Please unlock slcap026 after fixing the issue(s): Create VM Failed, ",HealthCheck,ovm_general,Closed
514,YOSAINI,"2015-06-29 08:04:42",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Memory Mismatch (Farm:9168,Hyper:6946), ",HealthCheck,ovm_preflight,Closed
515,MULIEGAD,"2015-06-29 10:32:55",NULL,NULL,"Chnages required in HA REL10/REL11 Topology","We need to add OSN HOST 2 to below OVM topologies

SDI_FA_REL10_CRM_HA
SDI_FA_REL10_GSI_PFA_RAC_HA
SDI_FA_REL10_GSI_HA
SDI_FA_REL10_HCM_HA",,,Closed
516,AMATHURA,"2015-06-29 12:41:33",NULL,NULL,"Pls change the below job's status as CLEANED","Pls change the below job's status as CLEANED",26026912,fastha,Closed
517,VIKUKUMA,"2015-06-29 19:50:47",NULL,Hypervisor,"InvalidMemorySizeException while creating VMs","DTE JOB ID- 26436642  

CreateVM task failed with below exception-

Debug: create_vm slc09brb,stit_oel5u6,slcap174,slcap174,admin,OVMadmin,welcome1,4,20000
Virtual machine ""slc09brb"" is being created. Please check the status.
Maximum Memory Size has been changed to ""20000"".
Caught exception while handling request: oracle.ovs.biz.exception.InvalidMemorySizeException: OVM-4005 The largest amount of virtual memory that can be allocated is 1,024 MB.",26436642,,Closed
518,SBALASUN,"2015-06-30 10:37:48",NULL,NULL,"Cleanup of job 24989428 failed","Cleanup of job 24989428 [Rel10 ST14 SC master env] failed.",24989428,OPC_QA_SHARED,Closed
519,YOSAINI,"2015-06-30 10:59:22",NULL,NULL,"Hypervisor slcai051 Locked","Please unlock slcai051 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:246859), ",HealthCheck,OPC_QA_FARM,Closed
520,VIKUKUMA,"2015-06-30 11:26:35",NULL,Hypervisor,"JOB- 26450691 Error while creating VMs","All the Tasks associated with Job- 26450691 failed with some common error -

Error: slcxxxx is alive.. please release it first

e.g-

Debug: create_vm slc09btc,stit_oel5u8,slcap176,slcap176,admin,OVMadmin,welcome1,2,4096
Error: slc09btc is alive.. please release it first"," 26450691",,Closed
521,SBALASUN,"2015-07-01 07:59:02",NULL,NULL,"Cleanup of job 25440862 failed","This seems to be the issue:

Failed to Release Resources for Job --> 25440862

<INFO> 01/07/2015 12:35:44 -> ERROR: Unable to release VM: slc06xsl

<INFO> 01/07/2015 12:35:44 ->

Error detected in Output Line --> ERROR: Unable to release VM: slc06xsl",25440862,OPC_QA_SHARED,Closed
522,YOSAINI,"2015-07-02 08:01:17",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Memory Mismatch (Farm:67539,Hyper:57763), ",HealthCheck,OPC_QA_FARM,Closed
523,BKASHYAP,"2015-07-02 05:33:22",NULL,NULL,"Cleanup job ID 26338223","Please clean up as we would like to submit new job",26338223,QAFARM,Closed
524,YOSAINI,"2015-07-03 08:00:51",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Memory Mismatch (Farm:141000,Hyper:71687), ",HealthCheck,OPC_QA_SHARED,Closed
525,YOSAINI,"2015-07-03 08:02:00",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:105379,Hyper:82850), ",HealthCheck,OPC_QA_SHARED,Closed
526,BKASHYAP,"2015-07-03 02:59:30",NULL,NULL,"Cleanup of 26452601","Cleanup failed with the following error, please help

<INFO> The Timer --> Cleanup Job Invocation via Telnet has exceeded the pre-defined timeout period of 20 minutes",26452601,lifecycle_dev,Closed
527,YOSAINI,"2015-07-03 05:32:36",NULL,NULL,"Hypervisor adcgdc17 Locked","Please unlock adcgdc17 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
528,YOSAINI,"2015-07-03 05:32:40",NULL,NULL,"Hypervisor adcgdb03 Locked","Please unlock adcgdb03 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
529,YOSAINI,"2015-07-03 05:32:45",NULL,NULL,"Hypervisor adcgdb06 Locked","Please unlock adcgdb06 after fixing the issue(s): Memory Mismatch (Farm:250749,Hyper:248060), ",HealthCheck,OPC_QA_SHARED,Closed
530,YOSAINI,"2015-07-03 05:32:49",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
531,YOSAINI,"2015-07-03 05:32:53",NULL,NULL,"Hypervisor slcan525 Locked","Please unlock slcan525 after fixing the issue(s): Memory Mismatch (Farm:144710,Hyper:122180), ",HealthCheck,OPC_QA_SHARED,Closed
532,YOSAINI,"2015-07-03 05:32:57",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): OVS Processes = 10 , Memory Mismatch (Farm:144710,Hyper:122180)<br />",HealthCheck,OPC_QA_SHARED,Closed
533,MABOGEGO,"2015-07-03 09:27:04",NULL,NULL,"Unable to create VMs with slcal074  and slcal069 hypervisors","unable to create the below vms:
slc06xxr        10.244.54.5     slcal069        4096    4       10.244.48.1     REGISTERED      26506073        QAFARM  adcgdb05

slc06xxt        10.244.54.7     slcal069        12288   4       10.244.48.1     REGISTERED      26506073        QAFARM  adcgdb05

slc06xxs        10.244.54.6     slcal074        1536    2       10.244.48.1     REGISTERED      26506073        QAFARM  adcgdb05",26506073,,Work-in-Progress
534,YOSAINI,"2015-07-05 08:10:41",NULL,NULL,"Hypervisor slcan521 Locked","Please unlock slcan521 after fixing the issue(s): Memory Mismatch (Farm:1232,Hyper:-303), ",HealthCheck,QAFARM,Closed
535,YOSAINI,"2015-07-05 09:46:24",NULL,NULL,"Hypervisor slcal065 Locked","Please unlock slcal065 after fixing the issue(s): OVS Processes = 9 , Create VM Failed<br />",HealthCheck,QAFARM,Closed
536,YOSAINI,"2015-07-06 09:48:37",NULL,NULL,"Hypervisor adcgdc17 Locked","Please unlock adcgdc17 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
537,YOSAINI,"2015-07-06 09:48:42",NULL,NULL,"Hypervisor adcgdb03 Locked","Please unlock adcgdb03 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
538,YOSAINI,"2015-07-06 09:48:46",NULL,NULL,"Hypervisor adcgdb06 Locked","Please unlock adcgdb06 after fixing the issue(s): Memory Mismatch (Farm:250749,Hyper:248060), ",HealthCheck,OPC_QA_SHARED,Closed
539,YOSAINI,"2015-08-04 10:31:56",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:250749,Hyper:248060)<br />",HealthCheck,OPC_QA_SHARED,Closed
540,YOSAINI,"2015-08-04 11:07:32",NULL,NULL,"Hypervisor slcap173 Locked","Please unlock slcap173 after fixing the issue(s): Memory Mismatch (Farm:222115,Hyper:219042), ",HealthCheck,saas_paas,Closed
541,YOSAINI,"2015-07-07 14:01:21",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:74851), ",HealthCheck,FASAASQA,Closed
542,YOSAINI,"2015-07-28 12:33:14",NULL,NULL,"Hypervisor slcap303 Locked","Please unlock slcap303 after fixing the issue(s): Memory Mismatch (Farm:251811,Hyper:219554), ",HealthCheck,FASAASQA,Closed
543,GSUNDARE,"2015-07-07 07:33:40",NULL,NULL,"26587162 has Failed while Starting DEPLOYER_HOST","The job failed  ?26587162 has Failed while Starting DEPLOYER_HOST?

DEPLOYER_HOST=slc06xsr.us.oracle.com 

Please fix it.
Have triggered the cleanup and the new job (26598414) is in queue",26587162,,Closed
544,MMAHAPAT,"2015-07-08 16:11:55",NULL,NULL,"adc6181303 ,adc6181302 in shutdown status","Hi ,
Since morning the machines  adc6181303 ,adc6181302 are not in consitent shape intially ssh did not work and now the vm remains in shutting down status .Could you please bring down the Vms and re boot it",,,Closed
545,VIKUKUMA,"2015-07-08 20:28:51",NULL,Others,"JCS & DBCS  BIG IP urls are not working","JCS BIG & DBCS BIG IP urls are not working - 


myJAVA35 (Oracle Java Cloud Service - SaaS Extension - Trial)
Service URL:   https://console-v0059-java.us1.vfarm.oraclecorp.com/em/faces/javaservice?serviceName=myJAVA35&identityGroup=sdiqa26516694-test 

myDB35 (Oracle Database Cloud Service - Trial)
Service URL:   https://mydb35-sdiqa26516694-test-v0059-db.us1.vfarm.oraclecorp.com/apex/
SDI_DEPLOYER_HOST=slc09brv.us.oracle.com
DEPLOYER_HOST=slc09btp.us.oracle.com",,,Closed
546,YOSAINI,"2015-07-28 09:27:56",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Memory Mismatch (Farm:70611,Hyper:69027), ",HealthCheck,OPC_QA_FARM,Closed
547,YOSAINI,"2015-07-09 08:07:33",NULL,Hypervisor,"Hypervisor slcap022 Locked","Please unlock slcap022 after fixing the issue(s): Memory Mismatch (Farm:59395,Hyper:49665), ",HealthCheck,OPC_QA_FARM,Closed
548,MCAINE,"2015-07-09 21:18:16",NULL,NULL,"26491533 cleaup failed","Cleanup for this job failed.
Can you tell me how to diagnose/proceed?",26491533,BI,Closed
549,MMAHAPAT,"2015-07-10 02:56:54",NULL,Others,"Jobs failing in farm because of OVS lock issues","Hi we have deploymnents failing with following error :
 Releasing lock on pool 'slcal040' on OVMM host 'slc02oxi.us.oracle.com'
0000KtoX5US1Fgk5Gzk3yf1Lbhcn000000,0] An error occurred during command execution: Error during start of VM  'slc06xtp_us_oracle_com""
with java.sql.SQLException: Protocol violationError Code: 17401",26665375,OPC_QA_FARM,Closed
550,YOSAINI,"2015-07-12 08:04:41",NULL,Others,"Hypervisor slcal074 Locked","Please unlock slcal074 after fixing the issue(s): Memory Mismatch (Farm:48931,Hyper:47394), ",HealthCheck,QAFARM,Closed
551,GSUNDARE,"2015-07-13 09:20:16",NULL,NULL,"Cleanup of job 26665375 has failed","The job 26665375 needs to be cleaned and the cleanup has failed. 

Please do the needful",26665375,,Closed
572,BARUNACH,"2015-07-15 09:45:44",NULL,NULL,"Memory Mismatch For hypervisor","Following Hypervisor having a memory mismatch under OPC_QA_FARM pool. Could you please look into it and let me know to use for our job.

slcah817, slcah766, slcap022",,OPC_QA_FARM,Closed
552,YOSAINI,"2015-08-04 13:14:58",NULL,NULL,"Hypervisor slcap174 Locked","Please unlock slcap174 after fixing the issue(s): Memory Mismatch (Farm:70563,Hyper:69025), ",HealthCheck,saas_paas,Closed
553,YOSAINI,"2015-07-14 14:00:42",NULL,NULL,"Hypervisor slcaf895 Locked","Please unlock slcaf895 after fixing the issue(s): Memory Mismatch (Farm:133000,Hyper:), ",HealthCheck,FASAASQA,Closed
554,YOSAINI,"2015-07-14 14:01:16",NULL,NULL,"Hypervisor slcai656 Locked","Please unlock slcai656 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:), ",HealthCheck,FASAASQA,Closed
555,YOSAINI,"2015-07-14 14:01:51",NULL,NULL,"Hypervisor slcaj502 Locked","Please unlock slcaj502 after fixing the issue(s): Memory Mismatch (Farm:14755,Hyper:), ",HealthCheck,FASAASQA,Closed
556,YOSAINI,"2015-07-14 14:02:25",NULL,NULL,"Hypervisor slcai051 Locked","Please unlock slcai051 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:), ",HealthCheck,FASAASQA,Closed
557,YOSAINI,"2015-07-14 14:32:31",NULL,NULL,"Hypervisor slcaj514 Locked","Please unlock slcaj514 after fixing the issue(s): Memory Mismatch (Farm:71075,Hyper:), ",HealthCheck,FASAASQA,Closed
558,YOSAINI,"2015-07-14 14:33:05",NULL,NULL,"Hypervisor slcap299 Locked","Please unlock slcap299 after fixing the issue(s): Memory Mismatch (Farm:87971,Hyper:), ",HealthCheck,FASAASQA,Closed
559,YOSAINI,"2015-07-14 14:33:39",NULL,NULL,"Hypervisor slcap304 Locked","Please unlock slcap304 after fixing the issue(s): Memory Mismatch (Farm:82851,Hyper:), ",HealthCheck,FASAASQA,Closed
560,YOSAINI,"2015-07-14 14:34:14",NULL,NULL,"Hypervisor slcap305 Locked","Please unlock slcap305 after fixing the issue(s): Memory Mismatch (Farm:233891,Hyper:), ",HealthCheck,FASAASQA,Closed
561,YOSAINI,"2015-07-14 14:34:49",NULL,NULL,"Hypervisor slcap023 Locked","Please unlock slcap023 after fixing the issue(s): Memory Mismatch (Farm:4915,Hyper:), ",HealthCheck,FASAASQA,Closed
562,YOSAINI,"2015-07-14 14:35:23",NULL,NULL,"Hypervisor slcap024 Locked","Please unlock slcap024 after fixing the issue(s): Memory Mismatch (Farm:251811,Hyper:), ",HealthCheck,FASAASQA,Closed
563,YOSAINI,"2015-07-14 14:35:58",NULL,NULL,"Hypervisor slcac613 Locked","Please unlock slcac613 after fixing the issue(s): Memory Mismatch (Farm:62272,Hyper:), ",HealthCheck,FASAASQA,Closed
564,YOSAINI,"2015-07-14 14:36:02",NULL,NULL,"Hypervisor slcai656 Locked","Please unlock slcai656 after fixing the issue(s): Memory Mismatch (Farm:105427,Hyper:), Memory Mismatch (Farm:105427,Hyper:66979)<br />",HealthCheck,FASAASQA,Closed
565,YOSAINI,"2015-07-14 14:36:07",NULL,NULL,"Hypervisor slcaj502 Locked","Please unlock slcaj502 after fixing the issue(s): Memory Mismatch (Farm:14755,Hyper:), Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
566,YOSAINI,"2015-07-14 14:36:11",NULL,NULL,"Hypervisor slcaj514 Locked","Please unlock slcaj514 after fixing the issue(s): Memory Mismatch (Farm:71075,Hyper:), Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
567,YOSAINI,"2015-07-14 14:36:15",NULL,NULL,"Hypervisor slcaj499 Locked","Please unlock slcaj499 after fixing the issue(s): Insufficient Avl. MemoryMemory Mismatch (Farm:85830,Hyper:), ",HealthCheck,FASAASQA,Closed
568,YOSAINI,"2015-07-14 10:11:55",NULL,NULL,"Hypervisor slcai656 Locked","Please unlock slcai656 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:105427,Hyper:66979)<br />",HealthCheck,FASAASQA,Closed
569,YOSAINI,"2015-07-14 10:11:59",NULL,NULL,"Hypervisor slcaj502 Locked","Please unlock slcaj502 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
570,YOSAINI,"2015-07-14 10:12:03",NULL,NULL,"Hypervisor slcaj514 Locked","Please unlock slcaj514 after fixing the issue(s): Request_adv Failed, Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
571,YOSAINI,"2015-07-14 10:12:08",NULL,NULL,"Hypervisor slcaj499 Locked","Please unlock slcaj499 after fixing the issue(s): Insufficient Avl. MemoryMemory Mismatch (Farm:85830,Hyper:), ",HealthCheck,FASAASQA,Closed
573,YOSAINI,"2015-07-15 10:33:18",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Memory Mismatch (Farm:248565,Hyper:236499), ",HealthCheck,OPC_QA_FARM,Closed
574,YOSAINI,"2015-07-16 09:17:14",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
575,BARUNACH,"2015-07-16 06:42:22",NULL,NULL,"Clean Up Failed","Cleanup failed with following message.
Cleaning Hypervisor Locks

INFO: Cleanup Completed

ERROR: Cleanup is not complete, please contact admin for a manual cleanup

INFO: Cleanup Completed

STDERR:",26665375,OPC_QA_SHARED,Closed
576,GSUNDARE,"2015-07-16 11:04:19",NULL,NULL,"job 26813973 cleanup has failed","Cleanup of job 26813973 has failed.

Please cleanup manually",26813973,OPC_QA_SHARED,Closed
577,YOSAINI,"2015-07-17 09:13:42",NULL,NULL,"Hypervisor slcal041 Locked","Please unlock slcal041 after fixing the issue(s): Memory Mismatch (Farm:187395,Hyper:177670), ",HealthCheck,OPC_QA_SHARED,Closed
578,YOSAINI,"2015-08-03 18:46:16",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:187395,Hyper:177670), ",HealthCheck,OPC_QA_SHARED,Closed
579,MULIEGAD,"2015-07-17 03:22:08",NULL,VM,"Facing issue while creating VM","Create VM is failing as below.

Debug: create_vm slc09foe,stit_oel5u6,slcap022,slcap022,admin,OVMadmin,welcome1,6,32768
Caught exception while handling request: java.lang.NullPointerException
ERROR: Creation Failed : slc09foe is stil in Virtual machine ""slc09foe"" does not exist. status after 40 Minutes

Please help us to resolve this issue",26824886,OPC_QA_FARM,Closed
580,AMATHURA,"2015-07-17 08:55:25",NULL,Others,"Pls update the memory details of hypervisor slcai551 as below","Pls update the memory details of hypervisor slcai551 as below

[aime@slc06chu amathura]$ perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o listhv -farm fastha | grep slcai551
slcai551        188934  251859  958     1000    92      100     10.240.184.1    fastha  slc     slcai551        UNLOCKED


Change to 
slcai551        188934  251859  958     1000    92      100     10.240.184.1    fastha  slc     slcai551        UNLOCKED",,fastha,Closed
581,SAMETHUK,"2015-07-17 14:28:25",NULL,NULL,"Please cleanup job:26423846","Cloud you please job:26423846",26423846,fastha,Closed
582,YOSAINI,"2015-07-19 09:32:17",NULL,NULL,"Hypervisor slcal067 Locked","Please unlock slcal067 after fixing the issue(s): Ping VM Failed, ",HealthCheck,QAFARM,Closed
583,YOSAINI,"2015-07-19 09:39:23",NULL,NULL,"Hypervisor slcal067 Locked","Please unlock slcal067 after fixing the issue(s): Ping VM Failed, Memory Mismatch (Farm:1072,Hyper:-462)<br />",HealthCheck,QAFARM,Closed
584,YOSAINI,"2015-07-20 09:43:36",NULL,NULL,"Hypervisor slcag412 Locked","Please unlock slcag412 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
585,YOSAINI,"2015-07-20 09:53:13",NULL,NULL,"Hypervisor slcag412 Locked","Please unlock slcag412 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />Memory Mismatch (Farm:3608,Hyper:-779)<br />",HealthCheck,ovm_preflight,Closed
586,YOSAINI,"2015-07-20 09:53:17",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:12776,Hyper:)<br />",HealthCheck,ovm_preflight,Closed
587,YOSAINI,"2015-07-21 15:54:28",NULL,NULL,"Hypervisor slcaj502 Locked","Please unlock slcaj502 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
588,YOSAINI,"2015-07-21 15:54:33",NULL,NULL,"Hypervisor slcaj514 Locked","Please unlock slcaj514 after fixing the issue(s): Request_adv Failed, Memory Mismatch (Farm:85830,Hyper:)<br />",HealthCheck,FASAASQA,Closed
589,YOSAINI,"2015-07-21 15:54:37",NULL,NULL,"Hypervisor slcaj499 Locked","Please unlock slcaj499 after fixing the issue(s): Insufficient Avl. MemoryMemory Mismatch (Farm:85830,Hyper:), ",HealthCheck,FASAASQA,Closed
590,MULIEGAD,"2015-07-21 09:55:43",NULL,NULL,"OVM_TOPOLOGY for REL11 with VCP","As of now we have below OVM_TOPLOGY with GRC included.
?	SDI_FA_REL10_GSI 
?	SDI_FA_REL10_GSI_HA
?	SDI_FA_REL10_GSI_PFA_RAC
?	SDI_FA_REL10_GSI_PFA_RAC_HA

Please clone above OVM topologies to include VCP VM for REL11 . That is one extra ip with below memory requirements.
vcp.cpus=4
vcp.memory=8192

Name convention should be as below:

SDI_FA_REL11_GSI 
SDI_FA_REL11_GSI_HA
SDI_FA_REL11_GSI_PFA_RAC
SDI_FA_REL11_GSI_PFA_RAC_HA",,,Closed
591,GSUNDARE,"2015-07-22 08:34:59",NULL,NULL,"Hypervisor - slcaf473  login issues","Rel10 ST17 HCM run has failed at REGISTER_TEMPLATES

Error - mount: slcaf473:/OVS/seed_pool failed

""mount slcaf473:/OVS/seed_pool /OVS/seed_pool_26916811_slcaf473""  fails  from SDI deployer host (slc09brv) to Hypervisor

Deployer Host
[aime@slc05jmw REGISTER_TEMPLATES]$ pwd
/scratch/aime/PFA_REMOTE/job_26916811/work/oracle/work/REGISTER_TEMPLATES

SDI Deployer Host - slc09brv",26916811,,Closed
592,YOSAINI,"2015-07-22 12:21:01",NULL,NULL,test,test,,prov_qa_automation,Closed
593,YOSAINI,"2015-07-23 10:05:14",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:38963,Hyper:29233)<br />",HealthCheck,OPC_QA_FARM,Closed
594,YOSAINI,"2015-07-23 10:05:19",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:3145,Hyper:2113)<br />",HealthCheck,OPC_QA_FARM,Closed
595,APINNAMR,"2015-07-23 10:42:26",NULL,NULL,"cleanup failed for job# 26917775","cleanup failed for job# 26917775"," 26917775",ovm_preflight,Closed
596,APINNAMR,"2015-07-23 11:14:34",NULL,NULL,"cleanup failed for job: 26942677","cleanup failed for job: 26942677",26942677,ovm_preflight,Closed
597,YOSAINI,"2015-08-01 17:13:56",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:243667), ",HealthCheck,QAFARM,Closed
598,YOSAINI,"2015-07-27 09:39:31",NULL,Hypervisor,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
599,AMPANAMA,"2015-07-27 07:30:18",NULL,Others,"Create vm failed during SHARED_IDM_SETUP","We had submitted topology to setup SDI_BASE using 15.2.6 labels and it is observed that one of the block i.e. SHARED_IDM_SETUP has failed while creating vm

[Error]
Debug: create_vm slc03wtf,stit_oel5u6,slcah816,slcah816,admin,OVMadmin,welcome1,6,32768
Caught exception while handling request: java.lang.NullPointerException
ERROR: Creation Failed : slc03wtf is stil in Virtual machine ""slc03wtf"" does not exist. status after 40 Minutes",26973926,OPC_QA_FARM,Closed
600,MULIEGAD,"2015-07-27 09:49:47",NULL,Hardware,"DTE Job fails on QAFARM","The DTE job on QAFARM fails as below:
Please note that your job SDI_SaaS_PaaS_Infra_with_EMGC_rac_outside_parallel_jcs_fa_SIM31 has Failed while Running on QA Farm","27011879 ",OPC_QA_FARM,Closed
601,GSUNDARE,"2015-07-27 10:19:42",NULL,Cleanup,"Cleanup of job 26916811 has failed","Cleanup of job 26916811 has failed due to error in Deploy delete block.

Please cleanup",26916811,OPC_QA_SHARED,Closed
602,MAMEHRA,"2015-07-27 12:28:50",NULL,NULL,"Deployer Host Creation Failed and Cleanup also seems to have failed","The Deployer host for job 27011051 has failed to be created. I have cleaned up the job and re-triggered. This job is not getting cleaned up",27011051,ovm_preflight,Closed
603,VIKUKUMA,"2015-07-27 14:50:51",NULL,VM,"Unable to login into VMs","Unable to login into below vms using credentials aime/2cool

Host1 - slc09bto.us.oracle.com
Hosts2 -slc09btb.us.oracle.com

Note- before QM aime/2cool was working for both the hosts.",,,Closed
604,YOSAINI,"2015-07-28 10:53:02",NULL,NULL,"Hypervisor slcaf895 Locked","Please unlock slcaf895 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Create VM Failed<br />",HealthCheck,FASAASQA,Closed
605,EKANTER,"2015-07-28 22:36:41",NULL,NULL,"job got stuck, cleanup failed","job got stuck, cleanup failed",27025803,QAFARM,Closed
606,YOSAINI,"2015-07-29 08:06:33",NULL,NULL,"Hypervisor slcag051 Locked","Please unlock slcag051 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,prov_qa_automation,Closed
607,YOSAINI,"2015-07-29 08:11:47",NULL,NULL,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Space Low<br />Create VM Failed<br />",HealthCheck,prov_qa_automation,Closed
608,GSUNDARE,"2015-07-29 08:31:24",NULL,Hypervisor,"Hypervisor slcal040 is Not accessible","Hypervisor slcal040 is Not accessible and in turn the ENV is down.
Please bring up

PING slcal040.us.oracle.com (10.245.53.172) 56(84) bytes of data.
From slcal041.us.oracle.com. (10.245.53.173) icmp_seq=2 Destination Host Unreachable",26608943,,Closed
609,BARUNACH,"2015-07-29 09:49:02",NULL,Others,"Job is in WAITING status.","Since morning Job is in waiting status. Farm is not picking up due to might be agent is not running.
27067865 	OPC_QA_SHARED 	barunach 	WAITING 

Please check it out.",27067865,OPC_QA_SHARED,Closed
610,MCAINE,"2015-07-29 15:08:38",NULL,NULL,"Cleanup of 26951213 failed","Job cleanup failed, please help",26951213,,Closed
611,YOSAINI,"2015-07-31 11:03:18",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
612,YOSAINI,"2015-07-30 08:19:07",NULL,NULL,"Hypervisor slcai053 Locked","Please unlock slcai053 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
613,YOSAINI,"2015-07-30 08:24:22",NULL,NULL,"Hypervisor slcai052 Locked","Please unlock slcai052 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.10.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
614,YOSAINI,"2015-07-30 08:29:37",NULL,NULL,"Hypervisor slcah764 Locked","Please unlock slcah764 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
615,YOSAINI,"2015-07-30 08:34:52",NULL,NULL,"Hypervisor slcah816 Locked","Please unlock slcah816 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
616,YOSAINI,"2015-07-30 08:40:37",NULL,NULL,"Hypervisor slcag794 Locked","Please unlock slcag794 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
617,YOSAINI,"2015-07-31 11:01:08",NULL,NULL,"Hypervisor slcaj457 Locked","Please unlock slcaj457 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
618,YOSAINI,"2015-07-30 08:51:07",NULL,NULL,"Hypervisor slcai657 Locked","Please unlock slcai657 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
619,YOSAINI,"2015-07-31 11:00:35",NULL,NULL,"Hypervisor slcao381 Locked","Please unlock slcao381 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
620,YOSAINI,"2015-07-31 10:05:39",NULL,NULL,"Hypervisor slcap172 Locked","Please unlock slcap172 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
621,YOSAINI,"2015-07-30 09:06:53",NULL,NULL,"Hypervisor slcag792 Locked","Please unlock slcag792 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
622,YOSAINI,"2015-08-01 08:10:57",NULL,NULL,"Hypervisor slcap025 Locked","Please unlock slcap025 after fixing the issue(s): Memory Mismatch (Farm:183779,Hyper:182242), ",HealthCheck,ovm_general,Closed
623,ANHANUMA,"2015-08-01 10:59:40",NULL,NULL,"cleanup failing for job 26842761","Can you pls help us to cleanup job  : 26842761. 


Below error message getting while cleanup. Also OVMM seems to be down, getting 500 error.

Can't bring down  on a timely manner. Something is Wrong
ERROR: Unable to shutdown RAC node1: slc02qxx
debug: forcing...
Can't bring down  on a timely manner. Something is Wrong
ERROR: Unable to shutdown RAC node2: slc02qxi
Skipping VM cleanups because of RAC failure",26842761,ovm_preflight,Closed
624,AMALON,"2015-08-01 16:17:07",NULL,NULL,"Invalid Clean Key For Job 26058779","Hi,

Please cleanup job 26058779, since the key I have is invalid. 

Current cleanup key:
26058779_hkdooe2o2nliafo70vsv1h69jc",26058779,QAFARM,Closed
625,EKANTER,"2015-08-01 22:13:25",NULL,Others,"jobs fail - same command works on another farm","jobs fail - same command works on QAFARM",27146230,LCM_DEV_PROV,Closed
626,YOSAINI,"2015-08-02 09:53:00",NULL,NULL,"Hypervisor slcan517 Locked","Please unlock slcan517 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
627,YOSAINI,"2015-08-02 12:02:38",NULL,NULL,"Hypervisor slcan521 Locked","Please unlock slcan521 after fixing the issue(s): OVS Processes = 9 , Ping VM Failed<br />",HealthCheck,QAFARM,Closed
628,YOSAINI,"2015-08-02 12:05:02",NULL,NULL,"Hypervisor slcal073 Locked","Please unlock slcal073 after fixing the issue(s): Memory Mismatch (Farm:70611,Hyper:69074), ",HealthCheck,QAFARM,Closed
629,PWARRIER,"2015-08-02 07:57:28",NULL,VM,"create racvips is creating only 2 IPs. No enough IPs for 10.240.104.1","[aime@slc03zao ~]$ /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o request_adv -mem 1 -cpu 1 -temp_type OVM_OL6U6_X86_64_12102DBRAC_PVM -hypervisor slcae117 -farm OPC_QA_FARM -dteid pwarrier_12c_noncdb -debug

Debug:No available VM found!
Could not complete command, please run with -debug switch for more details 
[aime@slc03zao ~]$",,OPC_QA_FARM,Closed
630,YOSAINI,"2015-08-03 09:31:47",NULL,NULL,"Hypervisor slcag414 Locked","Please unlock slcag414 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
631,YOSAINI,"2015-08-03 09:52:59",NULL,Hypervisor,"Hypervisor adcgdb05 Locked","Please unlock adcgdb05 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
632,BARUNACH,"2015-08-03 06:00:27",NULL,NULL,"Job Hung on EXPORT_BASE_SDI_DETAILS block and killed automatically.","Job Hung on EXPORT_BASE_SDI_DETAILS block and killed automatically. We did not find log information about the failure or hung.",27129209,OPC_QA_SHARED,Closed
633,PWARRIER,"2015-08-03 16:43:30",NULL,Others,"Please provide permissions to Babu Arunachalam to trigger job","Please provide permissions to Babu Arunachalam to trigger job in OPC_QA_SHARED.",,OPC_QA_SHARED,Closed
634,AMATHURA,"2015-08-04 06:48:11",NULL,NULL,"QA farm is allocating more memory than the actual size of the hypervisor","QA farm is allocating more memory than the actual size of the hypervisor

[aime@slc06chu ~]$ perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o listvm -farm fastha | grep slcai552

Please check. It has alloted more than 300GB when the original size is 250 GB",,fastha,Open
635,BARUNACH,"2015-08-04 07:13:57",NULL,NULL,"Clean Up Failed","Clean up Failed with following reason.
Shared virtual disk ""27194989_2"" does not exist, or an error occurred while getting the shared disk resource.
Failed to Release Resources for Job",27194989,OPC_QA_SHARED,Closed
636,SBALASUN,"2015-08-04 07:21:15",NULL,NULL,"Cleanup of job 26327796 failed","Cleanup of job 26327796 fails when triggered from QA farm",26327796,FASAASQA,Closed
637,EKANTER,"2015-08-04 23:25:32",NULL,NULL,"cleanup failed","job 27190203 cleanup failed.",27190203,LCM_DEV_PROV,Closed
638,AMATHURA,"2015-08-05 05:56:52",NULL,NULL,"VM slc03wqr not released as part of cleanup","[aime@slc06chu ~]$ perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager_v7.pl -o listvm -farm fastha | grep 26390843
slc03wqr        10.245.145.44   slcah810        32768   4       10.245.144.1    REGISTERED      26390843        fastha  adcgdb05",26390843,fastha,Closed
639,PWARRIER,"2015-08-05 17:04:41",NULL,NULL,"Can you please help to get key for job 25205030","Can you please help to get key for job 25205030.",,OPC_QA_FARM,Closed
640,YOSAINI,"2015-08-06 09:16:18",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
641,YOSAINI,"2015-08-06 09:35:01",NULL,NULL,"Hypervisor slcaj457 Locked","Please unlock slcaj457 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
642,YOSAINI,"2015-08-06 10:03:08",NULL,NULL,"Hypervisor slcai658 Locked","Please unlock slcai658 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Request_adv Failed<br />Memory Mismatch (Farm:169939,Hyper:)<br />",HealthCheck,OPC_QA_FARM,Closed
643,SAMETHUK,"2015-08-06 06:33:37",NULL,NULL,"Please cleanup jobs 27048237,27105960","Could you please cleanup following jobs:
27048237
27105960
27109069
27171270
27190438
27195742
27246886",27105960,fastha,Closed
644,MULIEGAD,"2015-08-06 10:07:04",NULL,NULL,"Request to update correct memory for REL10 CRM/HCM/GSI OVM Topology","Please help us to update the memory for the OVM topologies specified in the mail sent regarding this.",,,Closed
645,YOSAINI,"2015-08-07 10:35:51",NULL,NULL,"Hypervisor slcan525 Locked","Please unlock slcan525 after fixing the issue(s): Ping VM Failed, ",HealthCheck,OPC_QA_SHARED,Closed
646,YOSAINI,"2015-08-07 10:36:01",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): OVS Processes = 9 , Ping VM Failed<br />",HealthCheck,OPC_QA_SHARED,Closed
647,YOSAINI,"2015-08-07 10:36:05",NULL,NULL,"Hypervisor adcgdc17 Locked","Please unlock adcgdc17 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:564000,Hyper:562844)<br />",HealthCheck,OPC_QA_SHARED,Closed
648,YOSAINI,"2015-08-07 10:36:10",NULL,NULL,"Hypervisor adcgdb03 Locked","Please unlock adcgdb03 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:564000,Hyper:562844)<br />",HealthCheck,OPC_QA_SHARED,Closed
649,YOSAINI,"2015-08-07 10:36:14",NULL,NULL,"Hypervisor adcgdb06 Locked","Please unlock adcgdb06 after fixing the issue(s): Memory Mismatch (Farm:564000,Hyper:562844), ",HealthCheck,OPC_QA_SHARED,Closed
650,YOSAINI,"2015-08-07 10:36:18",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:564000,Hyper:562844)<br />",HealthCheck,OPC_QA_SHARED,Closed
651,YOSAINI,"2015-08-07 10:36:22",NULL,NULL,"Hypervisor slcal070 Locked","Please unlock slcal070 after fixing the issue(s): OVS Processes = 9 , Memory Mismatch (Farm:171942,Hyper:149926)<br />",HealthCheck,OPC_QA_SHARED,Closed
652,YOSAINI,"2015-08-07 10:36:26",NULL,NULL,"Hypervisor slcal075 Locked","Please unlock slcal075 after fixing the issue(s): Memory Mismatch (Farm:171942,Hyper:149926), ",HealthCheck,OPC_QA_SHARED,Closed
653,ANHANUMA,"2015-08-07 08:00:54",NULL,VM,"jobs are failing on ovm_preflight pool","tried 4 times, but jobs are failing during deployer host startup
error message "" Please note that your job 27263087 has Failed while Starting DEPLOYER_HOST""",27263087,ovm_preflight,Closed
654,YOSAINI,"2015-08-09 08:09:53",NULL,NULL,"Hypervisor slcal064 Locked","Please unlock slcal064 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
655,YOSAINI,"2015-08-09 08:15:08",NULL,NULL,"Hypervisor slcal074 Locked","Please unlock slcal074 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
656,YOSAINI,"2015-08-09 08:20:23",NULL,NULL,"Hypervisor slcal065 Locked","Please unlock slcal065 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
657,YOSAINI,"2015-08-09 08:25:38",NULL,NULL,"Hypervisor slcal061 Locked","Please unlock slcal061 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
658,YOSAINI,"2015-08-09 08:30:23",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
659,YOSAINI,"2015-08-09 08:35:38",NULL,NULL,"Hypervisor slcal036 Locked","Please unlock slcal036 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
660,YOSAINI,"2015-08-09 08:40:52",NULL,NULL,"Hypervisor slcal039 Locked","Please unlock slcal039 after fixing the issue(s): Create VM Failed, ",HealthCheck,QAFARM,Closed
661,YOSAINI,"2015-08-10 08:08:40",NULL,NULL,"Hypervisor slcag414 Locked","Please unlock slcag414 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Create VM Failed<br />",HealthCheck,ovm_preflight,Closed
662,YOSAINI,"2015-08-10 09:51:31",NULL,NULL,"Hypervisor slcaj455 Locked","Please unlock slcaj455 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
663,YOSAINI,"2015-08-10 09:55:06",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Memory Mismatch (Farm:251859,Hyper:)<br />",HealthCheck,ovm_preflight,Closed
664,BARUNACH,"2015-08-10 05:40:15",NULL,Cleanup,"26794410, 26626876 cleanup failed","Cleanup has been failed for 26794410, 26626876 jobs under OPC_QA_FARM pool as well as saas_paas pool.",26794410,OPC_QA_FARM,Closed
665,MCAINE,"2015-08-10 16:00:03",NULL,NULL,"27278159 Cleanup failed","27278159 cleanup failed.
Any chance you can also let me know why the job creation itself failed?
-Thanks",27278159,BI,Closed
666,PPRSHAH,"2015-08-11 05:42:12",NULL,null,"Unable to cleanup DTE ID --> 27186511","Unable to cleanup DTE ID --> 27186511 once the job has completed. Entered the DTE ID as job key but says its invalid key.",27186511,QAFARM,Closed
667,PPRSHAH,"2015-08-11 06:52:20",NULL,NULL,"Clean up for DTE ID - 27186511 failed","Clean up for DTE ID - 27186511 failed. Seems like manual clean up is requested.",,QAFARM,Closed
668,YOSAINI,"2015-08-11 15:23:29",NULL,NULL,"Hypervisor slcap303 Locked","Please unlock slcap303 after fixing the issue(s): Memory Mismatch (Farm:656524,Hyper:653449), ",HealthCheck,FASAASQA,Closed
669,YOSAINI,"2015-08-11 15:23:33",NULL,NULL,"Hypervisor slcap299 Locked","Please unlock slcap299 after fixing the issue(s): Memory Mismatch (Farm:656524,Hyper:653449), ",HealthCheck,FASAASQA,Closed
670,YOSAINI,"2015-08-11 15:23:37",NULL,NULL,"Hypervisor slcap304 Locked","Please unlock slcap304 after fixing the issue(s): Memory Mismatch (Farm:656524,Hyper:653449), ",HealthCheck,FASAASQA,Closed
671,YOSAINI,"2015-08-11 15:23:41",NULL,NULL,"Hypervisor slcap305 Locked","Please unlock slcap305 after fixing the issue(s): Memory Mismatch (Farm:656524,Hyper:653449), ",HealthCheck,FASAASQA,Closed
672,SAMETHUK,"2015-08-11 09:08:25",NULL,NULL,"VM creation slc05jur has failed","Debug: create_vm slc05jur,stit_oel5u6,slcak526,slcak526,admin,OVMadmin,welcome1,3,32768
Virtual machine ""slc05jur"" is being created. Please check the status.
Maximum Memory Size has been changed to ""32768"".
Memory size changed to ""32768"".
Number of VCPUs changed to ""3"".
ERROR: subjobid 17795210153351  failed!",27372441,fastha,Closed
673,BARUNACH,"2015-08-11 16:18:26",NULL,VM,"PowerOn Failed for slc06xse OVMM.","Below error message is displayed.
Start - /OVS/running_pool/342478_slc06xse_us_oracle_com
PowerOn Failed : Result - failed:<Exception: return=>failed:<Exception: failed:<IOError: [Errno 2] No such file or directory: '/var/ovs/mount/3958BB5232A547FFB42402BA85E92394/running_pool/342478_slc06xse_us_oracle_com/vm.cfg'>",,,Closed
674,YOSAINI,"2015-08-11 16:47:16",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
675,YOSAINI,"2015-08-11 17:22:02",NULL,NULL,"Hypervisor slcai053 Locked","Please unlock slcai053 after fixing the issue(s): Create VM Failed, ",HealthCheck,OPC_QA_FARM,Closed
676,YOSAINI,"2015-08-11 17:27:02",NULL,NULL,"Hypervisor slcai052 Locked","Please unlock slcai052 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.10.el5xen, Create VM Failed<br />",HealthCheck,OPC_QA_FARM,Closed
677,SAMETHUK,"2015-08-11 18:45:32",NULL,NULL,"please remove following vms in fastha pool","No one is using following in fastha pool looks like they were created outside dte job and went into FORCE state , please remove these vms. 
 slc05jvh 
 slc05jvg 
 slc05jvf 
 slc05jve 
 slc05jup",,fastha,Closed
678,YOSAINI,"2015-08-12 08:07:38",NULL,NULL,"Hypervisor slcag248 Locked","Please unlock slcag248 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,prov_qa_automation,Closed
679,YOSAINI,"2015-08-12 08:12:54",NULL,NULL,"Hypervisor slcag051 Locked","Please unlock slcag051 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HealthCheck,prov_qa_automation,Closed
680,BKASHYAP,"2015-08-12 17:00:41",NULL,Others,"Request for Big Ip","Please allocate a Big IP to kick off ovm env in dev pool",,lifecycle_dev,Closed
681,SWSUDHAK,"2015-08-13 07:23:00",NULL,VM,"SIMhost2 Create VM failed","This is SDI base run.
SHARED_IDM_SETUP block failed as the shared idm host2 create vm failed.

Error:
Debug: create_vm slc04zio,stit_oel5u6,slcaj457,slcaj457,admin,OVMadmin,welcome1,6,32768
ERROR: Can't create slc04zio which already exist :: status is: Access denied. Please check username or password.",27414938,OPC_QA_FARM,Closed
682,VIKUKUMA,"2015-08-13 09:35:34",NULL,NULL,"Unable to login into VMs","We have setup SDI base using 15.3.2 SIMv3.1 

Issue -  unable to login in to below VMs with with aime/2cool which was working before .

1) slc09brv.us.oracle.com
2)slc09brw.us.oracle.com
3)slc09brx.us.oracle.com
4)slc09bsg.us.oracle.com
5)slc09bsh.us.oracle.com
6)slc09bsi.us.oracle.com
7)slc09bsk.us.oracle.com
8)slc09bsl.us.oracle.com

NOTE-  All the VMs associated with same hyps - slcap176",,,Open
683,JKUTTAPP,"2015-08-13 10:17:06",NULL,Hypervisor,"Request for shared server setup","can you please combine these two hypervisor from ovm_general and create a shared server pool so that we can keep a dedicated SDI env for upgrade.

slcap025        251811  251811  990     1000    1000    1000 
10.240.176.1    ovm_general     slc     slcap025        FORCE 
Memory_Mismatch Sat Aug  1 01:10:57 2015
slcap026        251811  251811  1000    1000    1000    1000 
10.240.176.1    ovm_general     slc     slcap026 UNLOCKED        OK      
Sat Jun 27 01:51:30 2015",,ovm_general,Closed
684,KRAMANJA,"2015-08-14 06:16:48",NULL,NULL,"Cleanup for job27220838  not completing","Cleanup of  job-27220838 is not getting completed.",27220838,warroom,Closed
685,MCAINE,"2015-08-14 09:22:55",NULL,NULL,"REL10 job failed for 3rd time this week","27454070 failed, I have not been able to run a REL10 job thus far",27454070,BI,Closed
686,PVKULKAR,"2015-08-14 11:02:50",NULL,Hypervisor,"OVM-2007 Master Oracle VM Agent (slcal070.us.oracle.com) is not active","HI,
Please help in resolving the OVMM server pool issue
Getting the following error while SDI task [slcal070] data from OVMM.] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:getServerPoolMetrics(169)] [javax.xml.ws.soap.SOAPFaultException: Internal Server Error (Caught exception while handling request: oracle.ovs.biz.server.ServerException: OVM-2007 Master Oracle VM Agent (slcal070.us.oracle.com) is not active, and the operation can not be performed. )

Thanks,
Priya",27386710,OPC_QA_SHARED,Closed
687,BKASHYAP,"2015-08-14 17:56:19",NULL,NULL,"Shared hypervisor","slcag795 and slcag797 are shown under the same pool slcag793, causing issues when env is getting created.  As a result both the hypervisors are not getting used at all.  Can you please help fixing this issue.",,lifecycle_dev,Open
688,AMALON,"2015-08-14 18:35:54",NULL,Hypervisor,"Job not picked up","Hi,

Job 27476315 in ovm_general isn't picked up despite having sufficient free h/w.",27476315,ovm_general,Closed
689,YOSAINI,"2015-08-15 08:31:50",NULL,NULL,"Hypervisor slcap026 Locked","Please unlock slcap026 after fixing the issue(s): Ping VM Failed, ",HealthCheck,ovm_general,Closed
690,YOSAINI,"2015-08-17 09:12:51",NULL,NULL,"Hypervisor slcai554 Locked","Please unlock slcai554 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.4.46.el5xen, Memory Mismatch (Farm:108499,Hyper:)<br />",HealthCheck,ovm_preflight,Closed
691,YOSAINI,"2015-08-17 09:12:56",NULL,NULL,"Hypervisor slcao573 Locked","Please unlock slcao573 after fixing the issue(s): Memory Mismatch (Farm:3587,Hyper:1537), ",HealthCheck,ovm_preflight,Closed
692,PWARRIER,"2015-08-17 07:22:09",NULL,NULL,"Please create login for  someshwar.dhayalan@oracle.com","Please create login for someshwar.dhayalan@oracle.com to access farmUI and submit jobs under OPC_QA_SHARED and OPC_QA_FARM pool

thanks",,,Closed
693,SAMETHUK,"2015-08-17 10:15:33",NULL,Hypervisor,"vms slc05jlk,slc05jlq are in creating status for ever","while creating a vm using following in dte job vm struck with creating status , vm creation has failed.
/private_farm_fwk/ovm_farm_manager_v7.pl -o create_vm -host_name slc05jlk -localstorage true -lv_size 200 >& /net/slc09mqr.us.oracle.com/scratch/aime/PFA_REMOTE/job_27512292/work/oracle/work/CREATE_SDI_DEPLOYER_HOST/logs/request_vm_create_16_08_2015_12_45_38_55.6010328074084.log
ERROR: Creation Failed : slc05jlk is stil in Creating status after 40 Minutes",27512292,fastha,Closed
694,AMATHURA,"2015-08-17 10:15:55",NULL,NULL,"Change Job status 26847348 as cleaned","Change Job status 26847348 as cleaned",26847348,fastha,Closed
695,YOSAINI,"2015-08-17 10:29:23",NULL,NULL,"Hypervisor slcap026 Locked","Please unlock slcap026 after fixing the issue(s): Ping VM Failed, ",HealthCheck,ovm_general,Closed
696,MCAINE,"2015-08-18 06:50:53",NULL,NULL,"ORA-00059: maximum number of DB_FILES exceeded","Amit has helped me get through some deployment issues however there is the above error in this log:
/tmp/FAOVMDB3117958892693922728_fusiondb/rmanWorkDir_cleanAndDuplicate/output-rman_dbutil.sh-19436.log 

Can someone please adjust the DB?",27475168,BI,Open
697,BKASHYAP,"2015-08-18 17:48:24",NULL,NULL,"Cleanup of 23918779","Cleanup is not proceeding and is showing the following message :

<INFO> The Timer --> Cleanup Job Invocation via Telnet has exceeded the pre-defined timeout period of 20 minutes


Please help cleanup the env.",23918779,lifecycle_dev,Work-in-Progress
698,SJANDHYA,"2015-08-19 06:04:02",NULL,NULL,"SETUP_RAC_FOR_DEPLOYMENT block failed","Description:
@ SETUP_RAC_FOR_DEPLOYMENT failed with the following error which is
@ intermittent.
@ .
@ Configuring slc06xop network
@ Configuring slc06xoq network
@ Configuring slc06xop shm
@ Configuring slc06xoq shm
@ building RAC ....
@ ERROR: subjobid 3543801711409  failed!
@ .
@ .
@ Job Details:
@ Job ID: 27526865
@ Results location:
@ /net/slc08bum.us.oracle.com/scratch/aime/PFA_REMOTE/job_27526865/work
@ Backup of results: /scratch/aime/backups/27526865_setupracfailure",27526865,QAFARM,Closed
699,SWSUDHAK,"2015-08-19 08:18:32",NULL,Hypervisor,"27546996 job had 3 pingable ip's","We triggered a job 27546996 on OPC_QA_FARM
And it had 3 pingable IP's before rehydration only.
Due to this our Rehydration will not proceed
10.244.160.205
10.244.160.216
[10.244.160.215
On checking in the farm db these Ip?s are used only for this job#27546996 
These VM?s are not seen in OVMM and and also not seen in Hypervisor to perform xm destroy.",27546996,OPC_QA_FARM,Closed
700,BARUNACH,"2015-08-20 06:30:57",NULL,NULL,"Cleanup Failed","Cleanup has been failed for the job 27214883.",27214883,OPC_QA_SHARED,Closed
701,BARUNACH,"2015-08-20 06:32:04",NULL,NULL,"Cleanup Failed","Cleanup has been failed for the job 26626876.",26626876,saas_paas,Closed
702,MULIEGAD,"2015-08-20 11:02:38",NULL,NULL,"Request to create OVM topologies for","As GSI Lite is a new Pillar supported  from 15.3.6 we need OVM topologies for the same . Here there is difference in memory assigned to Primary and secondary hosts.",,,Closed
703,SJANDHYA,"2015-08-20 11:49:44",NULL,Others,"Unable to find pool 'slcan523'","DEPLOY_TEMPLATE block failed with the above error",27605524,QAFARM,Closed
704,GSUNDARE,"2015-08-21 09:24:07",NULL,NULL,"Jobs not Auto-Cleaned","The following jobs have failed waiting for resources.

27605654
27615284

After timeout, the jobs are not getting AUTO CLEANED.  The status shows ""CLEANING""",27605654,OPC_QA_SHARED,Closed
705,GSUNDARE,"2015-08-22 16:03:20",NULL,NULL,"Cleanup of job 27356581 failed.  SDI cleanup is COMPLETED.  Please cleanup from UI","Cleanup of job 27356581 failed.  SDI cleanup is COMPLETED.  Please cleanup from UI

++ grep CLEANUP_FA_FROM_SDI1
+ return_text=
+ '[' '' '!=' '' ']'
+ echo 'Deploy Delete block has failed. Hence skipping the zfs storage deletion.'
Deploy Delete block has failed. Hence skipping the zfs storage deletion.",27356581,,Closed
706,SJANDHYA,"2015-08-24 06:59:49",NULL,NULL,"Unable to commit in slcnas570 due to insufficient space","Seeing error error: The action could not be completed because there is insufficient space in Clone_Ovab_Home block during commit in slcnas570.

Results archive - /net/slc08bxd.us.oracle.com/scratch/aime/PFA_REMOTE/job_27677315/work",27677315,QAFARM,Closed
707,BKASHYAP,"2015-08-25 08:23:09",NULL,Cleanup,"Cleanup didnt complete for job 23953452","Cleanup didnt complete for job 23953452",23953452,lifecycle_dev,Closed
708,BKASHYAP,"2015-08-25 08:24:12",NULL,Cleanup,"Cleanup didnt complete for job 25110879","Cleanup didnt complete for job 25110879",25110879,lifecycle_dev,Closed
709,APINNAMR,"2015-08-25 15:25:04",NULL,Others,"Rel10 P4FA OVM job failed with : Unable to find pool 'slcal039'","Rel10 P4FA OVM job failed during deploy template block execution due to below error:
oracle.apps.fnd.provisioning.ovm.sdk.cli.FAOVMCLIException: Unable to find pool 'slcal039'","27695575 ",QAFARM,Closed
710,SJANDHYA,"2015-08-25 18:47:52",NULL,Others,"DEPLOY_TEMPLATE failed with 'Downloading EM Agent from OMS - slc08bwi.us.oracle.com ... FAILED'","[10:10:25] Downloading EM Agent from OMS - slc08bwi.us.oracle.com ... FAILED. [1m19s]
An error occurred: An error occurred during command execution: OS command exited with an undesired exit code(3):
directory=/tmp/emcli22575.install
./emcli setup -dir=/tmp/emcli22575.install/conf -url=https://slc08bwi.us.oracle.com:7799/em -user=sysman -password=<oms_password> -localdirans=YES -licans=YES -nocertvalidate -trustall
Exiting with error",27714761,QAFARM,Closed
711,APINNAMR,"2015-08-26 06:55:11",NULL,Cleanup,"Job : 27715552 failed during CREATE_VM","jOB FAILED : 27715552 during CREATE_VM block execution.

Debug: create_vm slc06xyn,OVM_OL6U4_X86_64_11204DBRAC_PVM,slcal064,slcal064,admin,OVMadmin,welcome1,8,64000
Virtual machine ""slc06xyn"" is being created. Please check the status.
Maximum Memory Size has been changed to ""64000"".
Memory size changed to ""64000"".
Number of VCPUs changed to ""8"".
ERROR: subjobid 3665922511251  failed!","27715552  ",QAFARM,Closed
712,YOSAINI,"2015-08-26 09:25:24",NULL,Hypervisor,"Hypervisor adcgdb04 Locked","Please unlock adcgdb04 after fixing the issue(s): Ping Hyp Failed, ",HealthCheck,ovm_preflight,Closed
713,YOSAINI,"2015-08-26 09:42:42",NULL,NULL,"Hypervisor slcag414 Locked","Please unlock slcag414 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 9 <br />Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
714,MULIEGAD,"2015-08-27 05:47:08",NULL,Hypervisor,"Facing issue with VM creation","Virtual machine ""slc03pfk"" is being created. Please check the status.
Maximum Memory Size has been changed to ""32768"".
Memory size changed to ""32768"".
Number of VCPUs changed to ""6"".
ERROR: subjobid 978642611816  failed!

Please help us to resolve this issue",,OPC_QA_FARM,Closed
715,BKASHYAP,"2015-08-27 05:48:11",NULL,NULL,"Cleanup of 27713192 is stuck for last two days","Cleanup of 27713192 is stuck for last two days with the following message....

Please note that the running handler for this Job has to be terminated and then resources released. This operation will take some time.",27713192,lifecycle_dev,Closed
716,MULIEGAD,"2015-08-27 05:50:00",NULL,NULL,"Please reboot Hypervisor slcak529","As we are facing issue with starting few VM from this hypervisors please help us to reboot this.",,OPC_QA_SHARED,Closed
717,YOSAINI,"2015-08-27 08:51:20",NULL,NULL,"Hypervisor slcai058 Locked","Please unlock slcai058 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.4.46.el5xen, Create VM Failed<br />",HealthCheck,ovm_preflight,Closed
718,AMATHURA,"2015-08-27 09:15:22",NULL,Others,"Server Pool issue in outage ovmm","One of the server pool slcao124 in outage ovmm slc08dpe was not showing the memory details even though both servers under it was active as well the server pool itself was active. I tried bouncing the ovs-agent and hypervisors itself, but no luck. After the hypervisors were rebooted, the /var/ovs/mount is not there in both hypervisors. I am using sun storage mount for OVS. Is there a way we can reassign the repostories now as the folders in sun storage for the OVS is intact?",,,Closed
719,YOSAINI,"2015-08-27 10:57:17",NULL,NULL,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Repo Mismatch Detected, Ping VM Failed<br />",HealthCheck,prov_qa_automation,Closed
720,YOSAINI,"2015-08-27 11:29:03",NULL,Hypervisor,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Repo Mismatch Detected, Ping VM Failed<br />",HealthCheck,prov_qa_automation,Closed
721,PWARRIER,"2015-08-27 16:30:55",NULL,NULL,"JOB 27765848 appears to have hung. In preparing state itself","JOB 27765848 is in preparing state from hours together. Can you please help here? This is for REL11 ST9 CRM",27765848,OPC_QA_FARM,Closed
722,AMATHURA,"2015-08-27 17:15:40",NULL,Hypervisor,"ovs-agent issue in hypervisors slcao120,121,122,123","[root@slcao120 ~]# /sbin/service ovs-agent start
OVSAgentServer is now starting...
OVSAgentServer started.
TIMEOUT=600
Current host!=Server Pool Master. Server Pool Master will be pinged until 600 seconds. The ovs-agent will be started once Server Pool Master is running",,,Closed
723,YOSAINI,"2015-08-28 09:46:41",NULL,NULL,"Hypervisor slcan525 Locked","Please unlock slcan525 after fixing the issue(s): OVS Processes = 9 , Memory Mismatch (Farm:69414,Hyper:67363)<br />",HealthCheck,OPC_QA_SHARED,Closed
724,YOSAINI,"2015-08-28 09:46:46",NULL,NULL,"Hypervisor slcan526 Locked","Please unlock slcan526 after fixing the issue(s): Memory Mismatch (Farm:69414,Hyper:67363), ",HealthCheck,OPC_QA_SHARED,Closed
725,KRAMANJA,"2015-08-28 06:10:39",NULL,NULL,"cleanup of job -26251762 failed","Cleanup of job-26251762 in QAFarm pool failed.
Please help to cleanup it up",26251762,QAFARM,Closed
726,MCAINE,"2015-08-28 19:14:29",NULL,Others,"Environment not usable after job","After running 27776349 I see that there are many diffs.
I cannot access any of the managed servers, I assume because something is pretty seriously wrong with the environment.",27776349,BI,Work-in-Progress
727,YOSAINI,"2015-08-30 09:32:28",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): OVS Processes = 9 , Ping VM Failed<br />",HealthCheck,QAFARM,Closed
728,YOSAINI,"2015-08-31 09:36:55",NULL,NULL,"Hypervisor slcag413 Locked","Please unlock slcag413 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 10 <br />Ping VM Failed<br />",HealthCheck,ovm_preflight,Closed
729,SJANDHYA,"2015-08-31 09:10:02",NULL,NULL,"Unable to clean 27762271","The status of the job 27762271 is in Cleaning tsate fro ma very long time . Please look into it.",27762271,QAFARM,Closed
730,BKASHYAP,"2015-08-31 09:45:38",NULL,NULL,"OVMM IS SET TO NULL","faovm.ovmm.host=null in /net/slc02inp.us.oracle.com/scratch/aime/PFA_REMOTE/job_27847513/OVM_EM_JVMD_STORAGE.properties

I manually changed it to slc02oxi.us.oracle.com, looks like it was no late for the topo to pickup that value.",27847513,lifecycle_dev,Closed
731,ACHAYAPA,"2015-08-31 13:05:53",NULL,VM,"Unable to access deployer host for job 26747933","In QA Farm, the deployer host for job 26747933 i.e. slc03dot.us.oracle.com is not accessible, but I see rest of hosts are accessible. Could you please check once suggest.

Other than deployer host I am able to access the other hosts.",26747933,prov_qa_automation,Closed
732,PWARRIER,"2015-08-31 16:27:01",NULL,NULL,"141GB boxes locked due to memory mismatch in opc_qa_shared","4- 141 GB boxes are in the forced state with memory mismatch. These appear to be fully free.",,OPC_QA_SHARED,Closed
733,AMALON,"2015-08-31 20:03:24",NULL,NULL,"SIM 3.1 OHS Host-- slc07zun not accessible","SIM 3.1 OHS host is not accessible via SSH. Also, trying to bounce the host fails.

Hypervisor pool slcao381",27519623,OPC_QA_FARM,Closed
734,YOSAINI,"2015-09-01 08:22:54",NULL,NULL,"Hypervisor slcap174 Locked","Please unlock slcap174 after fixing the issue(s): Uname version lower: , Memory Mismatch (Farm:235427,Hyper:)<br />",HealthCheck,saas_paas,Closed
735,YOSAINI,"2015-09-01 08:23:01",NULL,NULL,"Hypervisor slcap175 Locked","Please unlock slcap175 after fixing the issue(s): Uname version lower: , Memory Mismatch (Farm:13011,Hyper:)<br />",HealthCheck,saas_paas,Closed
736,YOSAINI,"2015-09-01 08:23:07",NULL,NULL,"Hypervisor slcap176 Locked","Please unlock slcap176 after fixing the issue(s): Uname version lower: , Insufficient Avl. Memory<br />Memory Mismatch (Farm:723,Hyper:)<br />",HealthCheck,saas_paas,Closed
737,MULIEGAD,"2015-09-01 07:04:41",NULL,NULL,"Request to create OVM topologies for REL11_12c DB","Help us with creation of OVM topologies with the details mailed for 12c DB creation.",,,Closed
738,BARUNACH,"2015-09-01 07:35:17",NULL,Others,"Not able to login SHARED_IDM_HOST3 of SDI base slc03yzk.","Ping is working for the host slc07zun.us.oracle.com but not able to login into host.",27519623,OPC_QA_FARM,Closed
739,HealthChecker,"2015-09-01 16:51:51",NULL,NULL,"Hypervisor slcai051 Locked","Please unlock slcai051 after fixing the issue(s): OVS Repository Mismatch , OVS Processes = 9 <br />Start VM Failed<br />",HC,FASAASQA,Closed
740,SMADHU,"2015-09-01 09:52:17",NULL,NULL,"Move hyp ""slcai657"" from OPC_QA_FARM to ""slcai659"" pool in FASAASQA","Please move hyp ""slcai657"" from OPC_QA_FARM to ""slcai659"" pool in FASAASQA.

Additional Details:
This has one rac_DB VM, which can be cleaned, as the underlying base job is cleaned from farm UI.",,FASAASQA,Closed
741,HealthChecker,"2015-09-01 17:18:56",NULL,NULL,"Hypervisor slcaj502 Locked","Please unlock slcaj502 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Memory Mismatch (Farm:362781,Hyper:342347)<br />",HC,FASAASQA,Closed
742,HealthChecker,"2015-09-01 17:19:01",NULL,NULL,"Hypervisor slcaj457 Locked","Please unlock slcaj457 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Memory Mismatch (Farm:362781,Hyper:342347)<br />",HC,FASAASQA,Closed
743,HealthChecker,"2015-09-01 17:19:06",NULL,NULL,"Hypervisor slcaj514 Locked","Please unlock slcaj514 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:362781,Hyper:342347)<br />",HC,FASAASQA,Closed
744,HealthChecker,"2015-09-01 17:19:11",NULL,NULL,"Hypervisor slcaj499 Locked","Please unlock slcaj499 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:362781,Hyper:342347)<br />",HC,FASAASQA,Closed
745,HealthChecker,"2015-09-01 17:19:16",NULL,NULL,"Hypervisor slcap303 Locked","Please unlock slcap303 after fixing the issue(s): OVS Repository Mismatch , Memory Mismatch (Farm:503622,Hyper:200516)<br />",HC,FASAASQA,Closed
746,HealthChecker,"2015-09-01 17:19:21",NULL,NULL,"Hypervisor slcap299 Locked","Please unlock slcap299 after fixing the issue(s): OVS Repository Mismatch , Memory Mismatch (Farm:503622,Hyper:200516)<br />",HC,FASAASQA,Closed
747,PWARRIER,"2015-09-01 16:18:26",NULL,NULL,"REL10 CRM job on OPC_QA_SHARED WAITING_FOR_INTERNAL_FIX","27881353	OPC_QA_SHARED	barunach	WAITING_FOR_INTERNAL_FIX	 	SDI_FA_REL10_CRM_PFA_RAC	N/A	

This is critical for saas-paas validation.",,OPC_QA_SHARED,Closed
748,SJANDHYA,"2015-09-01 18:47:36",NULL,NULL,"Failed to get the available memory for VM pool 'slcal063' on OVMM host 'slc08dpg.us.oracle.com'","Env deployment failed with the following error:
Failed to get the available memory for VM pool 'slcal063' on OVMM host 'slc08dpg.us.oracle.com'
Exiting with error",27881044,QAFARM,Closed
749,BARUNACH,"2015-09-02 07:16:42",NULL,NULL,"Cleanup Failed or clean up is not started.","Following error message is displayed.
Cleanup Log for Job ID 27856333:
Could not find Cleanup Log for Jobid --> 27856333",27856333,OPC_QA_FARM,Closed
750,AMPANAMA,"2015-09-02 08:58:56",NULL,NULL,"Error seen during create rac - OVM-4105 MAC address exists","We are creating 12C NON-CDB and create rac step is completed but with below error message

DTEID: ampanama_12C_nonCDB1

NIC has been added to the virtual machine.
Caught exception while handling request: oracle.ovs.biz.exception.NetworkCardException: OVM-4105 MAC address exists.
Done",slcag794,OPC_QA_FARM,Closed
751,APINNAMR,"2015-09-02 11:12:34",NULL,NULL,"27897069 is failed during REGISTER_OVAB_TEMPLATES","Rel92 job# 27897069 is failed during REGISTER_OVAB_TEMPLATES
Reason of failure: 
To	Subject	Sent	Size	Categories	
Sandya Menon; fa_preflight_qa_ww_grp	RE: OVM jobs started for REL9.2 and REL10 - Oct Saas	4:39 PM	14 KB","27897069 ",QAFARM,Closed
752,APINNAMR,"2015-09-02 11:14:18",NULL,NULL,"Rel10 job#27897069 is failed during SETUP_RAC_FOR_DEPLOYMENT","Rel10 job# is failed during SETUP_RAC_FOR_DEPLOYMENT
Reason of failure: 
<ERROR> Timed out waiting for VM --> slc05ved to start",27897069,QAFARM,Closed
753,APINNAMR,"2015-09-02 13:19:30",NULL,NULL,"Cleanup failed for job: 27897159","Cleanup failed for job: 27897159",27897159,QAFARM,Closed
754,SJANDHYA,"2015-09-02 13:32:20",NULL,NULL,"FAILED Task: Create VM - slc08bue_us_oracle_com","During the env re-hydration, following error occurred:
An error occurred: An error occurred during an earlier execution of command 'Create VM - slc08bue_us_oracle_com': Execution Failed
Exiting with error",27903571,QAFARM,Closed
755,HealthChecker,"2015-09-03 09:30:55",NULL,NULL,"Hypervisor slcal060 Locked","Please unlock slcal060 after fixing the issue(s): OVS Processes = 10 , Start VM Failed<br />",HC,QAFARM,Closed
756,HealthChecker,"2015-09-03 09:38:48",NULL,NULL,"Hypervisor slcal061 Locked","Please unlock slcal061 after fixing the issue(s): Start VM Failed, ",HC,QAFARM,Closed
757,HealthChecker,"2015-09-03 09:56:16",NULL,NULL,"Hypervisor slcal037 Locked","Please unlock slcal037 after fixing the issue(s): OVS Repository Mismatch , Memory Mismatch (Farm:188883,Hyper:60882)<br />",HC,QAFARM,Closed
758,SJANDHYA,"2015-09-03 07:39:57",NULL,NULL,"Cleanup failed",27903571,27903571,QAFARM,Closed
759,APINNAMR,"2015-09-03 10:21:00",NULL,NULL,"Unable to access deployer host slc06yem.us.oracle.com","Unable to access
DEPLOYER_HOST=slc06yem.us.oracle.com",27792630,QAFARM,Closed
760,HealthChecker,"2015-09-03 12:51:40",NULL,Hypervisor,"Hypervisor adcgdb05 Locked","Please unlock adcgdb05 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HC,ovm_preflight,Closed
761,HealthChecker,"2015-09-04 10:30:04",NULL,NULL,"Hypervisor adcgdc17 Locked","Please unlock adcgdc17 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:187168,Hyper:186011)<br />",HC,OPC_QA_SHARED,Closed
762,HealthChecker,"2015-09-04 10:30:09",NULL,NULL,"Hypervisor adcgdb03 Locked","Please unlock adcgdb03 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:187168,Hyper:186011)<br />",HC,OPC_QA_SHARED,Closed
763,HealthChecker,"2015-09-04 10:30:14",NULL,NULL,"Hypervisor adcgdb06 Locked","Please unlock adcgdb06 after fixing the issue(s): Memory Mismatch (Farm:187168,Hyper:186011), ",HC,OPC_QA_SHARED,Closed
764,HealthChecker,"2015-09-04 10:30:19",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Insufficient Avl. Memory<br />Memory Mismatch (Farm:187168,Hyper:186011)<br />",HC,OPC_QA_SHARED,Closed
765,HealthChecker,"2015-09-04 10:30:24",NULL,NULL,"Hypervisor slcal070 Locked","Please unlock slcal070 after fixing the issue(s): OVS Repository Mismatch , Memory Mismatch (Farm:144806,Hyper:142757)<br />",HC,OPC_QA_SHARED,Closed
766,HealthChecker,"2015-09-04 10:30:30",NULL,NULL,"Hypervisor slcal075 Locked","Please unlock slcal075 after fixing the issue(s): Memory Mismatch (Farm:144806,Hyper:142757), ",HC,OPC_QA_SHARED,Closed
767,HealthChecker,"2015-09-04 10:30:35",NULL,NULL,"Hypervisor slcal041 Locked","Please unlock slcal041 after fixing the issue(s): Memory Mismatch (Farm:446374,Hyper:295846), ",HC,OPC_QA_SHARED,Closed
768,HealthChecker,"2015-09-04 10:30:40",NULL,NULL,"Hypervisor slcal040 Locked","Please unlock slcal040 after fixing the issue(s): Memory Mismatch (Farm:446374,Hyper:295846), ",HC,OPC_QA_SHARED,Closed
769,SMADHU,"2015-09-04 05:14:33",NULL,NULL,"Move hyp ""slcai658"" from OPC_QA_FARM to ""slcai659"" pool in FASAASQA","Please help in moving hypv slcai658 to farm pool slcai659 in FASAASQA",,FASAASQA,Closed
770,HealthChecker,"2015-09-05 08:50:45",NULL,NULL,"Hypervisor slcal067 Locked","Please unlock slcal067 after fixing the issue(s): OVS Processes = 11 , Start VM Failed<br />",HC,QAFARM,Closed
771,HealthChecker,"2015-09-05 09:01:42",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
772,MULIEGAD,"2015-09-06 03:32:06",NULL,Hypervisor,"slcal041 / slcal040  has issues even though HC succeeds","These hypervisors fails to allocate resource to a FA job due to below exception
OVM-1004 XML-RPC Client Call Oracle VM Agent API 'get_server_xm_info' error:",,,Closed
773,HealthChecker,"2015-09-07 08:34:06",NULL,NULL,"Hypervisor slcal066 Locked","Please unlock slcal066 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
774,HealthChecker,"2015-09-07 09:24:06",NULL,NULL,"Hypervisor slcag413 Locked","Please unlock slcag413 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:12997,Hyper:1447)<br />",HC,ovm_preflight,Closed
775,HealthChecker,"2015-09-08 08:39:40",NULL,NULL,"Hypervisor slcal074 Locked","Please unlock slcal074 after fixing the issue(s): Start VM Failed, ",HC,QAFARM,Closed
776,HealthChecker,"2015-09-08 08:48:00",NULL,NULL,"Hypervisor slcal074 Locked","Please unlock slcal074 after fixing the issue(s): Start VM Failed, Memory Mismatch (Farm:94496,Hyper:64530)<br />",HC,QAFARM,Closed
777,MULIEGAD,"2015-09-08 09:24:32",NULL,NULL,"Changes required in HCM/CRM topology for APEX hosts","Please help us to change the HCM and CRM OVM topology to add two new hosts",,,Closed
778,HealthChecker,"2015-09-09 08:30:33",NULL,NULL,"Hypervisor slcal064 Locked","Please unlock slcal064 after fixing the issue(s): OVS Repository Mismatch , Create VM Failed<br />",HC,QAFARM,Closed
779,HealthChecker,"2015-09-09 09:42:39",NULL,NULL,"Hypervisor slcal063 Locked","Please unlock slcal063 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
780,HealthChecker,"2015-09-09 09:56:40",NULL,NULL,"Hypervisor slcal061 Locked","Please unlock slcal061 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
781,HealthChecker,"2015-09-09 10:04:32",NULL,NULL,"Hypervisor slcan517 Locked","Please unlock slcan517 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
782,HealthChecker,"2015-09-09 10:19:32",NULL,NULL,"Hypervisor slcal037 Locked","Please unlock slcal037 after fixing the issue(s): OVS Repository Mismatch , Memory Mismatch (Farm:251859,Hyper:123859)<br />",HC,QAFARM,Closed
783,YOSAINI,"2015-09-09 05:50:08",NULL,NULL,"Hypervisor slcal064 Locked","Please unlock slcal064 after fixing the issue(s): OVS Processes = 10 , Ping VM Failed<br />",HealthCheck,QAFARM,Closed
784,MULIEGAD,"2015-09-09 05:50:28",NULL,null,"Few ips in farm_ov328 has dteid marked as INVALID","Please hlep su to know the command to cleanup the ips register with dteid as INVALID for farm farm_ovs328",,,Closed
785,HealthChecker,"2015-09-09 06:20:35",NULL,NULL,"Hypervisor slcal063 Locked","Please unlock slcal063 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
786,SWSUDHAK,"2015-09-09 06:39:15",NULL,Cleanup,"27793702 - Cleanup Failed","Job 27793702 is cleanup failed",27793702,farm_ovs328,Closed
787,HealthChecker,"2015-09-09 08:13:00",NULL,NULL,"Hypervisor slcal037 Locked","Please unlock slcal037 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
788,HealthChecker,"2015-09-09 08:27:34",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,QAFARM,Closed
789,SJANDHYA,"2015-09-09 14:03:54",NULL,VM,"Error: slc08bxi is alive.. please release it first","CDRM over OVM job failed at Create_EMGC block with the following error message:

hypervisor is slcan522
emgc template OVM_EM12R3_STIT is registered
Debug: create_vm slc08bxi,OVM_EM12R3_STIT,slcan522,slcan522,admin,OVMadmin,welcome1,4,10000
Error: slc08bxi is alive.. please release it first

Job ID: 28075044",28075044,QAFARM,Closed
790,HealthChecker,"2015-09-10 09:44:15",NULL,NULL,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): OVS Processes = 10 , Start VM Failed<br />",HC,OPC_QA_FARM,Closed
791,HealthChecker,"2015-09-10 10:03:49",NULL,NULL,"Hypervisor slcap172 Locked","Please unlock slcap172 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,OPC_QA_FARM,Closed
792,EKANTER,"2015-09-10 03:08:39",NULL,Others,"job consistently fails","cat /net/slc08bvy.us.oracle.com/scratch/aime/PFA_REMOTE/job_28095317/work/oracle/work/topology_runtime_error.dif
*** DTE TOPOLOGY RUNTIME ERROR *** 
.....",28095317,QAFARM,Closed
793,HealthChecker,"2015-09-10 07:25:29",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): Create VM Failed, ",HC,QAFARM,Closed
794,AMALON,"2015-09-10 07:27:12",NULL,VM,"VM slc03yzz Fail To Start","VM slc03yzz on slcai054 has been initializing for some time.",27744340,OPC_QA_FARM,Closed
795,APINNAMR,"2015-09-10 12:56:05",NULL,Cleanup,"Please cleanup the job: 28079606","seems like cleanup is hanging for long time (~ 6 hrs )for job: 28079606",28079606,QAFARM,Closed
796,EKANTER,"2015-09-10 15:23:39",NULL,Others,"jobs on lcm farm don't start","same job on qafarm starts
qafarm job is 28104321",28110138,LCM_DEV_PROV,Closed
797,EKANTER,"2015-09-10 16:10:05",NULL,Others,"jobs on lcm farm don't start","same job on qafarm starts
qafarm job is 28104321",28110138,LCM_DEV_PROV,Closed
798,SWSUDHAK,"2015-09-10 17:55:10",NULL,NULL,27875768,"27875768 job is cleaned from DTE
But the VM traces are still seen.
Manual clean up is failing
/usr/bin/perl /ade_autofs/gd14_fmw/INTGQA_MAIN_GENERIC.rdd/LATEST/intgqa/ovm/private_farm_fwk/ovm_farm_manager.pl -o cleanalldte -dteid 27875768 -key 27875768_402n5h31tb0kics9i6p8cg2965
Unknown option: key
DBI connect('ovm_farm_db:adc2120708.us.oracle.com','ovm_farm_user',...) failed: Can't connect to MySQL server on 'adc2120708.us.oracle.com' (111)",27875768,OPC_QA_SHARED,Closed
799,MCAINE,"2015-09-10 18:23:53",NULL,NULL,"REL10 GSI Job fails","I have seen this issue a couple of times and worked with Amit.  He has told me that there is an issue with the DB backup.",27475168,BI,Open
800,SJANDHYA,"2015-09-11 03:46:39",NULL,Cleanup,"IPs selected for rehydration are reachable before rehydration","During re-hydration, seeing the following error:
[08:13:49] Verifying Hosts for Deploy ... An error occurred: [HOST_PRIMARY=slc08buo.us.oracle.com should be not reachable]
[HOST_OHS=slc08buu.us.oracle.com should be not reachable]
[HOST_LDAP=slc08bux.us.oracle.com should be not reachable]
[HOST_OIM=slc08buv.us.oracle.com should be not reachable]",28104321,QAFARM,Closed
862,BARUNACH,"2015-10-13 05:24:49",NULL,Cleanup,"Cleanup failed due to ERROR: subjobid 3181041265735 failed!","Cleanup failed with following error message.

<INFO> 12/10/2015 06:58:06 ->

Error detected in Output Line --> ERROR: subjobid 3181041265735  failed!",28749378,OPC_QA_SHARED,Closed
801,HealthChecker,"2015-09-12 10:20:11",NULL,NULL,"Hypervisor slcan517 Locked","Please unlock slcan517 after fixing the issue(s): Memory Mismatch (Farm:85920,Hyper:2705), ",HC,QAFARM,Closed
802,HealthChecker,"2015-09-12 10:20:16",NULL,NULL,"Hypervisor slcan523 Locked","Please unlock slcan523 after fixing the issue(s): Memory Mismatch (Farm:246691,Hyper:178594), ",HC,QAFARM,Closed
803,HealthChecker,"2015-09-13 10:16:56",NULL,NULL,"Hypervisor slcal061 Locked","Please unlock slcal061 after fixing the issue(s): Ping VM Failed, ",HC,QAFARM,Closed
804,PWARRIER,"2015-09-14 05:14:59",NULL,NULL,"Request_adv_Failed for slcak529 , slcak530","Request_adv_Failed for slcak529 , slcak530  in opc_qa_shared",,OPC_QA_SHARED,Closed
805,BARUNACH,"2015-09-14 06:43:15",NULL,Hypervisor,"Rehydration is failed.","SDI Re-hydration is failed with following error message. 
2015-09-11 17:24:01,159] [DEBUG] [com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl] - [REQ 500044404_500045436] [Not able to fetch server pool [slcai056] data from OVMM.] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:getServerPoolMetrics(169)] [com.oracle.cloud9.sdi.common.exceptions.ovmm.OVMMPoolNotAccessibleException: Could not find pool null or pool is inactive
Please help me out to rectify.",28133983,OPC_QA_SHARED,Closed
806,PWARRIER,"2015-09-14 11:02:17",NULL,VM,"VMs slc03yzd,slc03yze pingable, but free as per farm manager","Health on slcai054  shows 16491290_slc03yzd 16491290_slc03yze OVS kvooka_SDI_SINGLE_DB_INSTANCE_TESTS_AUTO_11_04_2014_3315
kvooka_SDI_SINGLE_DB_INSTANCE_TESTS_AUTO_11_04_2014_52

slc03yzd and slc03yze are autopingable and locked.

slc03yzd        10.244.160.196  dummyhost       4096    2       10.244.160.1    AUTOLOCK-PING   INVALID OPC_QA_FARM
slc03yze        10.244.160.197  dummyhost       4096    2       10.244.160.1    AUTOLOCK-PING   INVALID OPC_QA_FARM

But xm list doesn;t show",,OPC_QA_FARM,Closed
807,HealthChecker,"2015-09-15 08:08:42",NULL,NULL,"Hypervisor slcaj499 Locked","Please unlock slcaj499 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,FASAASQA,Closed
808,SJANDHYA,"2015-09-15 11:41:27",NULL,NULL,"subjobid 649755151106  failed! during Create_EMGC","Seeing the following error in Create_EMGC:
Creating EMGC.. this will take around 2 hours ...please standby...
ERROR: subjobid 649755151106  failed!",28211642,LCM_DEV_PROV,Open
809,HealthChecker,"2015-09-15 11:52:00",NULL,Hypervisor,"Hypervisor adcgdb05 Locked","Please unlock adcgdb05 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HC,ovm_preflight,Closed
810,SJANDHYA,"2015-09-15 16:30:30",NULL,NULL,"DEPLOY_TEMPLATE failed","[07:18:50/osn] Starting Task: Create VM - slc08bwb_us_oracle_com ...
[07:19:21]        FAILED Task: Export File Set - wls_domains_bi_oracleoutsourcing_com [8m7s]
[07:19:21/fa]   Starting Task: Export File Set - fapatch ...
[07:19:21]        FAILED Task: Export File Set - fusionbhd [0m33s]
[07:19:22]        FAILED Task: Export File Set - wls_domains_secondary_oracleoutsourcing_com [6m22s]
[07:19:22]        FAILED Task: Export File Set - wls_domains_admin-apps_oracleoutsourcing_com",28215187,QAFARM,Open
811,HealthChecker,"2015-09-16 07:28:29",NULL,NULL,"Hypervisor slcap172 Locked","Please unlock slcap172 after fixing the issue(s): OVS Processes = 9 , Start VM Failed<br />",HC,OPC_QA_FARM,Closed
812,SJANDHYA,"2015-09-16 09:02:56",NULL,NULL,"CLONE_OVAB_HOME could not be completed because there is insufficient space","CLONE_OVAB_HOME block failed with the following error:
slcnas570:shares sdiqa/28222124 (uncommitted clone)> commit
error: The action could not be completed because there is insufficient space
       available. This may be due to a quota being exceeded, or all available
       storage may be exhausted. Check the quotas of any projects or shares. If
       there is no quota, or the quota cannot be changed, then destroy any
       unused shares or snapshots to free up space and try again.",28222124,QAFARM,Open
813,HealthChecker,"2015-09-17 08:04:24",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Ping Hyp Failed, ",HC,OPC_QA_FARM,Closed
814,HealthChecker,"2015-09-17 09:57:34",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Ping Hyp Failed, Memory Mismatch (Farm:11254,Hyper:)<br />",HC,OPC_QA_FARM,Closed
815,HealthChecker,"2015-09-18 09:47:38",NULL,NULL,"Hypervisor adcgdc17 Locked","Please unlock adcgdc17 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:381216,Hyper:380060)<br />",HC,OPC_QA_SHARED,Closed
816,HealthChecker,"2015-09-18 09:47:44",NULL,NULL,"Hypervisor adcgdb03 Locked","Please unlock adcgdb03 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:381216,Hyper:380060)<br />",HC,OPC_QA_SHARED,Closed
817,HealthChecker,"2015-09-18 09:47:49",NULL,NULL,"Hypervisor adcgdb06 Locked","Please unlock adcgdb06 after fixing the issue(s): Memory Mismatch (Farm:381216,Hyper:380060), ",HC,OPC_QA_SHARED,Closed
818,HealthChecker,"2015-09-18 09:47:54",NULL,NULL,"Hypervisor adcgbf23 Locked","Please unlock adcgbf23 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 13 <br />Memory Mismatch (Farm:381216,Hyper:380060)<br />",HC,OPC_QA_SHARED,Closed
819,SWSUDHAK,"2015-09-18 17:22:41",NULL,Hypervisor,"VM status in unknown state","Two VM's created for CDB's are in unknown state and not responding

slc03wfl and slc03wfm
Hyp: slcah766

From OVMM the both vm status are in unknown state
ID: swsudhak_12C_CDB",,,Closed
820,APINNAMR,"2015-09-21 09:39:21",NULL,Cleanup,"Cleanup has failed for job:28070374","Cleanup has failed for job:28070374. Please help us to release the environment.",28070374,QAFARM,Closed
821,GSUNDARE,"2015-09-21 10:34:16",NULL,VM,"Unable to login to VM slc03yzl.us.oracle.com","VM 10.244.160.204 (slc03yzl.us.oracle.com) used for APEX is pingable but unable to connect.  It is not prompting for login.

Please resolve",,,Closed
822,EKANTER,"2015-09-21 19:18:55",NULL,Cleanup,"cleanup failed","please cleanup manually.",28302413,LCM_DEV_PROV,Closed
823,BARUNACH,"2015-09-22 05:26:44",NULL,Hypervisor,"SDI Rehydration is failed with memory issue.","[2015-09-22 00:01:09,678] [DEBUG] [com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl] - [REQ 500044418_500045521] [Ovmm Server pool slc02oxi.us.oracle.com : slcak528, memoryRequired [350720] Total memory [524266], free memory [219204] memoryUsed according to SDI [268800]] [at com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl:checkResourceLimit(137)] []
[2015-09-22 00:01:09,679] [DEBUG] [com.oracle.cloud9.sdi.connectors.ovmm.OVMMConnectorImpl] - [REQ 500044418_500045521] [Server pool",28352022,OPC_QA_SHARED,Closed
824,MULIEGAD,"2015-09-22 06:03:16",NULL,Cleanup,"Cleanup of Jobs from farm_ovs328","Cleanup of jobs from farm_ovs328 fails as there is issue with OVMM used for creating VM on this farm","28104542 	",farm_ovs328,Closed
825,SJANDHYA,"2015-09-23 06:44:42",NULL,NULL,"Error occurred while trying to Create a Rac with Node 1 --> slc08bwd and Node 2 --> slc08bwe","SETUP_RAC_FOR_DEPLOYMENT failed with the following error:
Error occurred while trying to Create a Rac with Node 1 --> slc08bwd and Node 2 --> slc08bwe",28372252,QAFARM,Closed
826,SJANDHYA,"2015-09-23 06:48:59",NULL,NULL,"Error occurred while trying to BUILD emgc vm","CREATE_EMGC failed with the following error:
Error occurred while trying to BUILD emgc vm

Backup Location of the results:
/net/adc2140656/scratch/28365958_setupracfail_22sep15/work/",28365958,QAFARM,Closed
827,HealthChecker,"2015-09-23 09:17:09",NULL,NULL,"Hypervisor slcag795 Locked","Please unlock slcag795 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />OVS Space Low<br />Create VM Failed<br />",HC,lifecycle_dev,Open
828,HealthChecker,"2015-09-23 09:23:28",NULL,NULL,"Hypervisor slcag797 Locked","Please unlock slcag797 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />OVS Space Low<br />Create VM Failed<br />",HC,lifecycle_dev,Open
829,SJANDHYA,"2015-09-23 18:10:26",NULL,NULL,"Create EMGC: ERROR: subjobid 904642311917  failed!","CREATE_EMGC failed with the following error:
ERROR: subjobid 904642311917  failed!

Results backup location: /net/2140656/scratch/28393276_createemgcfail_23sep15",28393276,QAFARM,Closed
830,HealthChecker,"2015-09-24 10:18:07",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:250835), ",HC,QAFARM,Closed
831,HealthChecker,"2015-09-26 10:47:44",NULL,NULL,"Hypervisor slcal034 Locked","Please unlock slcal034 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:250835), ",HC,QAFARM,Closed
832,NKATARLA,"2015-09-26 04:36:33",NULL,NULL,"Job 28371844 Cleanup Failed","The 28371844 Job cleanup in OPC_QA_SHARED Farm pool failed. Can you pls. help us cleanup the same?",28371844,OPC_QA_SHARED,Closed
833,NKATARLA,"2015-09-26 06:21:02",NULL,NULL,"Job Failed while Installing OVMCLI on DEPLOYER_HOST","Job 28469179 has failed while installing OVMCLI on DEPLOYER_HOST slc06xxl.us.oracle.com.",28469179,OPC_QA_SHARED,Open
834,APINNAMR,"2015-09-26 16:59:38",NULL,VM,"Job: 28460749 failed during deploy template","spawn ssh -t root@slc06xyp echo ""Successfully Connected to Remote Machine""; exit
root@slc06xyp's password: 
Permission denied, please try again.

root@slc06xyp's password: 
Permission denied, please try again.

root@slc06xyp's password: 
Permission denied (publickey,password).",28460749,,Closed
835,GSUNDARE,"2015-09-29 05:39:53",NULL,Cleanup,"Job- 28458439  cleanup has hung","Job- 28458439  cleanup has hung.  Please help cleanup",28458439,,Closed
836,SJANDHYA,"2015-09-29 07:38:32",NULL,NULL,"Job in PREPARING FOR RUN status from 2 hours","My job is in status PREPARING FOR RUN from 2 hours. Could you please look into it asap?",28514132,QAFARM,Closed
837,HealthChecker,"2015-09-29 15:17:00",NULL,NULL,"Hypervisor slcai656 Locked","Please unlock slcai656 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:238732,Hyper:207820)<br />",HC,FASAASQA,Closed
838,HealthChecker,"2015-09-29 15:17:05",NULL,NULL,"Hypervisor slcai659 Locked","Please unlock slcai659 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:238732,Hyper:207820)<br />",HC,FASAASQA,Closed
839,HealthChecker,"2015-09-29 15:17:10",NULL,NULL,"Hypervisor slcai657 Locked","Please unlock slcai657 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:238732,Hyper:207820)<br />",HC,FASAASQA,Closed
840,HealthChecker,"2015-09-29 15:17:15",NULL,NULL,"Hypervisor slcai658 Locked","Please unlock slcai658 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:238732,Hyper:207820)<br />",HC,FASAASQA,Closed
841,HealthChecker,"2015-09-30 08:33:11",NULL,NULL,"Hypervisor slcag248 Locked","Please unlock slcag248 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,prov_qa_automation,Closed
842,AMATHURA,"2015-09-30 10:44:53",NULL,NULL,"ovs-agent issue in hypervisors slcao142,slcao144,slcao136","I have 3 servers combined into 1 server pool. Initially server pool slcao142 had only 2 servers (slcao142 and slcao144). Yesterday I have added slcao136 and it was working fine till yesterday after adding. From today, server pool status changed to Inactive and server status ""Unreachable"" even though ovs-agent is up and running. OVMM - slc08dpe",,,Closed
843,EKANTER,"2015-09-30 13:31:49",NULL,NULL,"lost key","please cleanup this job",28288238,LCM_DEV_PROV,Closed
844,HealthChecker,"2015-10-01 09:16:39",NULL,Hypervisor,"Hypervisor slcai054 Locked","Please unlock slcai054 after fixing the issue(s): Memory Mismatch (Farm:190603,Hyper:183011), ",HC,OPC_QA_FARM,Work-in-Progress
845,EKANTER,"2015-10-01 22:46:28",NULL,Cleanup,"job key is lost","please cleanup job 28288238 as its key was lost.",28288238,LCM_DEV_PROV,Closed
846,MULIEGAD,"2015-10-03 17:11:15",NULL,NULL,"OVM Topology for REL11 CRM/HCM with PFA","Please help us to create below OVM topologies for HCM/CRM PFA 12c

?	SDI_FA_REL11_HCM_PFA_12c
?	SDI_FA_REL11_CRM_PFA_12c",,,Closed
847,GSUNDARE,"2015-10-05 09:38:41",NULL,Hypervisor,"SDI base job - 28609886 in waiting state","SDI job - 28609886 is in Waiting state in saas_paas pool.  Please resolve

slcap173 - Memory mismatch issue ( please reset )
slcap176 - Memory not fully released",28609886,,Closed
848,HealthChecker,"2015-10-05 11:01:03",NULL,NULL,"Hypervisor slcap173 Locked","Please unlock slcap173 after fixing the issue(s): Ping VM Failed, ",HC,saas_paas,Closed
849,EKANTER,"2015-10-05 13:29:20",NULL,Cleanup,"lost key","please clean up job 28288238",28288238,LCM_DEV_PROV,Closed
850,GSUNDARE,"2015-10-06 08:46:58",NULL,NULL,"15.4.2 SDI Base failed with OVM error","job 28638927 has Failed while Installing OVMCLI on DEPLOYER_HOST

Please resolve",28638927,OPC_QA_SHARED,Open
851,HealthChecker,"2015-10-07 04:38:48",NULL,Hypervisor,"Hypervisor slcal071 Locked","Please unlock slcal071 after fixing the issue(s): Create VM Failed, ",HC,QAFARM,Closed
852,BARUNACH,"2015-10-07 07:41:20",NULL,Cleanup,"Cleanup failed due to timed out issue.","Cleanup failed due to timed out issue.
<INFO> The Timer --> Cleanup Job Invocation via Telnet has exceeded the pre-defined timeout period of 20 minutes",28009704,OPC_QA_SHARED,Closed
853,HealthChecker,"2015-10-09 09:15:08",NULL,Hypervisor,"Hypervisor adcgdb05 Locked","Please unlock adcgdb05 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Ping VM Failed<br />",HC,ovm_preflight,Work-in-Progress
854,EKANTER,"2015-10-09 22:49:33",NULL,Cleanup,"28646519 clean up failed","28646519 clean up failed",28646519,,Closed
855,HealthChecker,"2015-10-11 08:07:26",NULL,Hypervisor,"Hypervisor slcal039 Locked","Please unlock slcal039 after fixing the issue(s): Ping Hyp Failed, ",HC,QAFARM,Closed
856,HealthChecker,"2015-10-11 10:15:49",NULL,NULL,"Hypervisor slcal039 Locked","Please unlock slcal039 after fixing the issue(s): Ping Hyp Failed, Memory Mismatch (Farm:251859,Hyper:)<br />",HC,QAFARM,Closed
857,SJANDHYA,"2015-10-11 15:45:09",NULL,NULL,"subjobid 1376761155526  failed!","CREATE_EMGC failed consequently with the following error:

Creating EMGC.. this will take around 2 hours ...please standby...
ERROR: subjobid 1376761155526  failed!",28739421,QAFARM,Open
858,SJANDHYA,"2015-10-11 18:18:40",NULL,Others,"FAILED Task: Export File - /OVAB_HOME/idm/idm_oidmw_binaries.tar","Rel10 job - DEPLOY_TEMPLATE failed with the following error:
[11:02:37]             FAILED Task: Export File - /OVAB_HOME/idm/idm_oidmw_binaries.tar [4m45s]
                     ... FAILED. [4m50s]
An error occurred: An error occurred during command execution: Execution of command Export Standalone File Archives failed for 1 of 2 child commands.
Exiting with error",28736783,QAFARM,Closed
859,HealthChecker,"2015-10-12 09:06:11",NULL,Hypervisor,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:18120,Hyper:1447)<br />",HC,ovm_preflight,Closed
860,HealthChecker,"2015-10-12 04:09:06",NULL,NULL,"Hypervisor slcal039 Locked","Please unlock slcal039 after fixing the issue(s): OVS Repository Mismatch , OVS Processes = 12 <br />Start VM Failed<br />",HC,QAFARM,Closed
861,VIKUKUMA,"2015-10-12 18:02:19",NULL,Hypervisor,"JOB Timedout error with Sufficent resource free","Hi Sowmya,

     Our Job got timedout with status (JOBid - 2875190) ""waiting for free resource"" eventhough sufficient h/w resource was free and RAC Factory has been pickedup.

Required Memory ~180G 
Available memory - ~215 G in single hyps
     
Yogesh was looking into issue and as per him some issue with resource allocation .

We are blocked with some high priority tasks could you please take a look and help us to proceed.

Once you confirm will resubmit new job.

Thanks
Vinay",,,Closed
863,SWSUDHAK,"2015-10-13 12:12:57",NULL,NULL,"12C CDB Factory Resource not available","There is no 12C CDB Factory Resource available to trigger Rel11 ST11 CRM job",28768553,farm_ovs328,Open
864,PWARRIER,"2015-10-13 13:05:01",NULL,Cleanup,"Cleanup failures in farm_ovs328","Cleanup has failed in farm_ovs328 which is blocking the hypervisor. Need help to cleanup the same, as we need to try out Primary node scaleout which needs 87 GB.",,farm_ovs328,Closed
865,AMATHURA,"2015-10-14 07:16:21",NULL,NULL,"VMs running from a particular server is not reachable suddenly","slcao126 - server pool. it has following four servers (slcao126,slcao127,slcao132,slcao148). VMs running from slcao126 and slcao127 are not working. VMs running from slcao132 and slcao148 are working. Please help.

OVMM - http://slc08dpe.us.oracle.com:8888/OVS/faces/infrastructure/OVS_Login.jspx",,,Open
866,PWARRIER,"2015-10-15 10:45:12",NULL,Hypervisor,"move slcah811 to CDB factory resource","slcah811        250354  250354  979     1000    994     1000    10.245.144.1    farm_ovs328     slc     slcah811        UNLOCKED",,,Closed
867,PWARRIER,"2015-10-16 05:04:36",NULL,NULL,"job triggered on farmovs328 is not getting picked","The DTE job triggered is not getting picked up. Its waiting for resource. In the workerhost logs also, I couldn?t find  much as to why this waiting.

But I see only 7 free IPs with gateway as 10.248.24.1 . Looks like this is the reason that job is not getting picked.

Actually we have enough IPs, but Swaroop?s failed JOB 28747710  though cleaned , has still not released the IPs. Can you help to release or basically help to get this job picked\",,farm_ovs328,Closed
868,SDHAYALA,"2015-10-16 09:19:58",NULL,Cleanup,"Cleanup Issue","Cleanup is failing. Also the deployer host in not accessible.",27850363,OPC_QA_SHARED,Closed
869,MABOGEGO,"2015-10-19 06:43:29",NULL,VM,"slc03han - deployer host is down","slc03han - deployer host is down so could you please bring up as soon as possible

Regards,
Mahadev",,ovm_preflight,Closed
870,MULIEGAD,"2015-10-19 10:22:41",NULL,Cleanup,"Few OVM Topologies has deployer host login as aime/aime","Few OVM topologies like SDI_FA_REL11_GSI_PFA_RAC
SDI_FA_REL10_CRM_PFA_RAC has deployer host login as aime/aime, because of which cleaning is stuck.",27772388,OPC_QA_FARM,Closed
871,VIKUKUMA,"2015-10-20 06:11:03",NULL,Cleanup,"Cleanup job failed to cleanup RAC factory resource","DTE JOB -279593288

We have cleaned up the job with RAC factory resource still shows live against same job id .

Could you please help us to cleaned up factory resources.",279593288,,Closed
872,PWARRIER,"2015-10-20 15:24:15",NULL,Others,"Factory Resource CDB takes CDB character set WE8MSWIN1252.","Factory Resource CDB takes CDB character set WE8MSWIN1252. REL11 ST13 DBRestore fails as its looking for AL32UTF8",,,Closed
873,GSUNDARE,"2015-10-20 17:20:31",NULL,VM,"VM slc03yyh not listing in OVMM","The VM slc03yyh is not listing in OVMM.  The parent host is  displayed as dummyhost.

This is required for JCS provisioning and association to Rel11 ST11 CRM FA",,OPC_QA_FARM,Closed
874,PWARRIER,"2015-10-26 05:57:18",NULL,Hypervisor,"Please help to bring up slcai059","hypervisor slcai059 is down. Please help to bring this up.",,farm_ovs328,Closed
875,AMATHURA,"2015-10-26 12:39:18",NULL,NULL,"Please change the worker host of DRQA pool to slc03wlj","Please change the worker host of DRQA pool to slc03wlj",,DRQA,Closed
876,AMATHURA,"2015-10-26 13:28:04",NULL,Cleanup,"Please change the status of jobs - 29009492 , 29009547 as cleaned","Please change the status of jobs - 29009492 , 29009547 as cleaned",,DRQA,Closed
877,MULIEGAD,"2015-10-28 06:54:06",NULL,Hypervisor,"QAFARM Automation for OVS328 Hyp and CDB Factory resource","?	Hypervisor which belongs to OVS 328 used for FA don?t have proper entries in /etc/exports and nfs is not started.
?	 CDB used  from FACTORY RESOURCE don?t have below services up and running , because of which all remote execution fails.
service rpcbind start
service nfs start
service autofs start",,,Closed
878,AMATHURA,"2015-10-28 07:29:40",NULL,NULL,"Please provide unlimited job access for HPASHIKA,AMATHURA,UGODA,VINJOHN in DRQA Pool","Please provide unlimited job access for HPASHIKA,AMATHURA,UGODA,VINJOHN in DRQA Pool",,DRQA,Closed
879,HealthChecker,"2015-10-29 10:51:47",NULL,NULL,"Hypervisor slcan520 Locked","Please unlock slcan520 after fixing the issue(s): Memory Mismatch (Farm:251811,Hyper:235427), ",HC,QAFARM,Closed
880,HPASHIKA,"2015-11-02 16:56:27",NULL,Cleanup,"Cleanup Job 29107475","Job 29107475 again moved to  WAITING_FOR_INTERNAL_FIX. I requested a different job now. Please cancel this job as I am not able to do it.","29107475 ",DRQA,Closed
881,VIKUKUMA,"2015-11-03 06:45:01",NULL,NULL,"Hyps slcai054 is dead slow and casuing multiplke issue","Hyps slcai054  is very slow and casuing multiple issues while VM start /stop .

Please take a look.",,,Closed
882,HealthChecker,"2015-11-04 09:41:57",NULL,NULL,"Hypervisor slcag248 Locked","Please unlock slcag248 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 19 <br />Start VM Failed<br />",HC,prov_qa_automation,Closed
883,PWARRIER,"2015-11-04 02:55:19",NULL,NULL,"Need endpoints for SIM","Need additional endpoints for SIM 

 https://idmadmin.us1.vfarm.oraclecorp.com 
 https://login.us1.vfarm.oraclecorp.com
 http://idminternal.us1.vfarm.oraclecorp.com",,,Open
884,MULIEGAD,"2015-11-04 09:40:29",NULL,NULL,"Jobs are not picked on small_pool","Any jobs on small_pool is not picked and going into WAITING_FOR_INTRENAL_FIX status. Please help us with the fix.",,small_pool,Open
885,HealthChecker,"2015-11-05 10:00:08",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, OVS Processes = 18 <br />Start VM Failed<br />",HC,OPC_QA_FARM,Closed
886,MULIEGAD,"2015-11-06 07:40:00",NULL,NULL,"Issue with custom cleanup in OVS 328 environments","The cleanup triggered from UI of OVS328 pools don't identify the deployer host on which it will looks for cleanup.sh",,,Open
887,EKANTER,"2015-11-10 22:46:04",NULL,NULL,"job stuck in waiting state","usually it is quick to start. is it out of resources?",29265666,LCM_DEV_PROV,Closed
888,EKANTER,"2015-11-11 19:50:23",NULL,NULL,"job hasn't started properly","I am getting tons of messages
Could not determine Sub Job for DTE Job --> 29265666
Is there anyone in HQ to deal with it?",29265666,LCM_DEV_PROV,Closed
889,SJANDHYA,"2015-11-12 12:06:01",NULL,Others,"CLONE_OVAB_HOME failing always","While running the job to setup CDRM over OVM env, CLONE_OVAB_HOME block is failing always without any specific error",29298946,QAFARM,Closed
890,HPASHIKA,"2015-11-16 09:58:41",NULL,NULL,"Cleanup Job 28689441 Failed","Please Clean this job as it is holding up resources.","28689441 ",fastha,Open
891,AMALON,"2015-11-16 21:28:44",NULL,NULL,"Server Pools Missing In QAFARM Pool","Hi, 

I've noticed several VMs fail to create in QA farm jobs due to server pools missing.

For example:
Server pool ""slcal062"" does not exist.

This job might get cleaned in the mean time, but please double check hypervisors vs. pools.",29373829,QAFARM,Open
892,SALUKUR,"2015-11-18 09:22:28",NULL,NULL,"Factory resource held up even though the associated DTE job is cleaned up","This factory resource is with ""RECYCLING"" status
The associated DTE job was cleaned up

G01	G_RAC_DB_2473 	RECYCLING	28749913",,,Closed
893,AMATHURA,"2015-11-18 11:55:34",NULL,NULL,"Job Cleanup is stuck : JobID 29407466","Cleanup ID is 29407466_h61sttoqsl42hdvll77frqc0bn

We triggered cleanup but cleanup is not getting started and not releasing the hypervisors.
Job status is completed as of now. If cleanup is triggered again from UI then getting a warning message is cleanup is in progress.",29407466,fastha,Closed
894,KRAMANJA,"2015-11-18 13:59:11",NULL,NULL,"jobs are going in cleaning stage after submitting cleanup state","Many jobs in warroom pool are going in cleaning state after submitting a cleanup request.",,warroom,Closed
895,AMATHURA,"2015-11-19 08:47:21",NULL,NULL,"29423514	- Cleanup FAiled","Cleanup Key is  	29423514_f5l3t1gdm4u3kdmclgunf56chb

Cleanup is failing and not releasing the sources. This is happening consistently during Rel10 HA env cleanup.

Can you please look into this?","29423514	",fastha,Closed
896,AMATHURA,"2015-11-20 11:28:56",NULL,NULL,"Cleanup job failed  - JobID 29449448","Cleanup Key : 29449448_7sjl0e0hih93vj1mqkv7lc1nih",29449448,,Closed
897,HPASHIKA,"2015-11-20 12:24:51",NULL,NULL,"29451607 is waiting for free hv despite of having enough resources","Job 29451607 is waiting despite of having enough VM's and Memory",29451607,DRQA,Closed
898,AMATHURA,"2015-11-20 12:32:09",NULL,VM,"Please move below vms from outageqa pool to DRQA","Please move below vms from outageqa pool to DRQA:

slcae82bevm36
slcae82bevm26
slcae82bevm17
slcae82bevm16
slcae82bevm13
slcae82bevm11
slcae82bevm09
slcae82bevm07
slcae82bevm06
slcae82bevm05
slcae82bevm04
slcae82bevm03
slcae82bevm02
slcae82bevm01",,DRQA,Closed
899,AMATHURA,"2015-11-20 12:33:41",NULL,VM,"Please delete below vms  from QAFARM db","Please delete below vms  from QAFARM db

slcae82bfvm36
slcae82bfvm26
slcae82bfvm17
slcae82bfvm16
slcae82bfvm13
slcae82bfvm11
slcae82bfvm09
slcae82bfvm07
slcae82bfvm06
slcae82bfvm05
slcae82bfvm04
slcae82bfvm03
slcae82bfvm02
slcae82bfvm01",,outageqa,Closed
900,JKUTTAPP,"2015-11-20 15:21:57",NULL,Others,"QAFARM - No free bigips","For the referance env creation job triggered in QAFARM, getting no free bigip error message.
But when doing ""list_new_bigip"" I am seeing 3 bigips in AVAILABLE state.",,QAFARM,Closed
901,RUMMADI,"2015-11-20 16:57:57",NULL,Cleanup,"de-register hypervisor from OVMM","Hi, The following hypervisers are reimaged.
slcal691 and slcak620 

But looks like they are still occupied by QAFarm jobs. Could you plese deregister them from OVMM manually.
The DTE jobs using the above hypervisers are;

27288981
28524261",,vkarpura_org,Closed
902,EKANTER,"2015-11-20 19:02:39",NULL,Cleanup,"please clean up a job","job 28718565 cleanup key is lost",28718565,LCM_DEV_PROV,Closed
903,SALUKUR,"2015-11-23 01:34:52",NULL,NULL,"clean up 29386775","Help in cleanup this job",29386775,OPC_QA_FARM,Open
904,MCAINE,"2015-11-23 23:45:34",NULL,NULL,"Need root password for DB hosts","I am unable to ssh to the DB hosts as root and need to do this in order to create a mountpoint.",29462113,BI,Closed
905,SWSUDHAK,"2015-11-25 10:04:56",NULL,NULL,"Could not cleanup 27878227","Could not cleanup Job#27878227 even after bringing the cancel request to completion in SDI CANCEL_78b3327d-1a37-468b-95dd-321b059de35eCREATE2015-09-30T10:39:19",27878227,OPC_QA_SHARED,Closed
906,VIKUKUMA,"2015-11-27 12:59:22",NULL,Others,"JAVA Console & Service BIG IP urls are not working","Console & Service urls are not working .
Could you please check if BIGIP configuration is fine for same.
Map file==
JCSService                 sdiqatest-v0024-java.us1.vfarm.oraclecorp.com           v0024-external                  create       443              slc03yza        7795                https             External VIP Port and External OHS
# 
JCSConsole                sdiqatestconsole-v0024-java.us1.vfarm.oraclecorp.com     v0024-admin-log                  create       443",,,Closed
907,JKUTTAPP,"2015-12-02 09:13:48",NULL,Others,"Additional BIG IP end points","From Rel12 onwards there is one additional offering of HED and it require additional big ip end points as mentioned by Lyle.

Can you please make necessary configuration in LBR to have these extra end points.

As per our existing BIG IP settings, we are using distinct internal end points (10613, 10615, etc) and unique external port 443.
Please make similar changes (hed-int:10657) and (hed-ext:443)",,,Closed
908,EKANTER,"2015-12-03 23:24:11",NULL,NULL,"LCM_DEV_PROV needs at least 3 big ip slots.","Please ensure that there are at least 3 big ip slots for LCM_DEV_PROV",,LCM_DEV_PROV,Open
909,BKASHYAP,"2015-12-04 09:56:43",NULL,NULL,"Remove slcag795,slcag797 and slcac609 from the Pool","Please remove slcag795,slcag797 and slcac609 from the lifecycle_dev pool.",,,Closed
910,EKANTER,"2015-12-07 19:19:59",NULL,NULL,CREATE_EMGC1.dif,"jobs on lcm farm consistently fail on CREATE_EMGC1
same exact command submitted for QAFARM consistently pass.",29718002,LCM_DEV_PROV,Open
911,SDHAYALA,"2015-12-08 06:54:42",NULL,NULL,"Cleanup Issue","Cleanup has removed all the IPs but displaying CLEANUP_FAILED status.",28756968,,Closed
912,TCHUKKA,"2015-12-15 21:13:18",NULL,VM,"We are not able to login slc07bjx","We not able to login to slc07bjx host using our linux login credentails.",,FADRDev,Closed
913,HealthChecker,"2015-12-16 06:45:51",NULL,Hypervisor,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Create VM Failed<br />",HC,ovm_preflight,Closed
914,HealthChecker,"2015-12-16 07:28:57",NULL,NULL,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Create VM Failed<br />",HC,ovm_preflight,Closed
915,HealthChecker,"2015-12-16 19:33:44",NULL,Hardware,"Hypervisor slcag248 Locked","Please unlock slcag248 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,prov_qa_automation,Closed
916,HealthChecker,"2015-12-16 19:39:31",NULL,NULL,"Hypervisor slcae231 Locked","Please unlock slcae231 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,prov_qa_automation,Closed
917,HealthChecker,"2015-12-16 19:44:48",NULL,NULL,"Hypervisor slcag051 Locked","Please unlock slcag051 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,prov_qa_automation,Closed
918,HealthChecker,"2015-12-16 19:50:04",NULL,NULL,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Create VM Failed, ",HC,prov_qa_automation,Closed
919,HealthChecker,"2015-12-17 09:10:35",NULL,NULL,"Hypervisor slcai052 Locked","Please unlock slcai052 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.10.el5xen<br />Create VM Failed<br />",HC,OPC_QA_FARM,Closed
920,HealthChecker,"2015-12-17 09:15:55",NULL,NULL,"Hypervisor slcah764 Locked","Please unlock slcah764 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Create VM Failed<br />",HC,OPC_QA_FARM,Closed
921,HealthChecker,"2015-12-17 09:21:45",NULL,NULL,"Hypervisor slcah817 Locked","Please unlock slcah817 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,OPC_QA_FARM,Closed
922,HealthChecker,"2015-12-17 09:27:06",NULL,NULL,"Hypervisor slcah766 Locked","Please unlock slcah766 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Create VM Failed<br />",HC,OPC_QA_FARM,Closed
923,HealthChecker,"2015-12-17 09:32:56",NULL,NULL,"Hypervisor slcah816 Locked","Please unlock slcah816 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Create VM Failed<br />",HC,OPC_QA_FARM,Closed
924,HealthChecker,"2015-12-17 09:38:16",NULL,NULL,"Hypervisor slcap172 Locked","Please unlock slcap172 after fixing the issue(s): Create VM Failed, ",HC,OPC_QA_FARM,Closed
925,JKUTTAPP,"2015-12-17 05:38:27",NULL,NULL,"Factory RAC not getting recycled in FASAASQA_328","I have triggered cleanup of the job 29530095 from FASAASQA_328 pool.
The job is in CLEANING state for more than 12 hours. Still Factory Rac it uses is recycling",29530095,FASAASQA_328,Closed
926,PWARRIER,"2015-12-18 18:11:27",NULL,NULL,"need bigip endpoints for v0052","Need bigip endpoints for
idmadmin.us1.vfarm.oraclecorp.com 
idmlogin.us1.vfarm.oraclecorp.com
idminternal.us1.vfarm.oraclecorp.com

all mapped to host SHARED_IDM_HOST3=slc09bru.us.oracle.com , port 7777",,OPC_QA_FARM,Open
927,HealthChecker,"2015-12-19 10:47:12",NULL,NULL,"Hypervisor slcal037 Locked","Please unlock slcal037 after fixing the issue(s): Memory Mismatch (Farm:251859,Hyper:241859), ",HC,QAFARM,Closed
928,HealthChecker,"2015-12-21 09:00:21",NULL,NULL,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): Ping Hyp Failed, ",HC,ovm_preflight,Closed
929,HealthChecker,"2015-12-21 09:32:34",NULL,NULL,"Hypervisor adcgdb07 Locked","Please unlock adcgdb07 after fixing the issue(s): Ping Hyp Failed, Memory Mismatch (Farm:18120,Hyper:)<br />",HC,ovm_preflight,Closed
930,BARUNACH,"2015-12-22 09:56:19",NULL,Hypervisor,"Memory Mismatch For hypervisor","Following HV having memory mismatch.
slcai054        223371  251859  970     1000    994     1000    10.244.160.1    OPC_QA_FARM     slc     slcai054        FORCE       Memory_Mismatch Thu Oct  1 02:16:39 2015",,OPC_QA_FARM,Closed
931,HealthChecker,"2015-12-23 09:25:55",NULL,NULL,"Hypervisor slcal068 Locked","Please unlock slcal068 after fixing the issue(s): Create VM Failed, ",HC,prov_qa_automation,Open
932,HealthChecker,"2015-12-25 10:03:46",NULL,NULL,"Hypervisor slcak528 Locked","Please unlock slcak528 after fixing the issue(s): OVS Repository Mismatch , Uname version lower: 2.6.18-128.2.1.5.12.el5xen<br />Memory Mismatch (Farm:297670,Hyper:293669)<br />",HC,OPC_QA_SHARED,Open
933,HealthChecker,"2015-12-25 10:03:51",NULL,NULL,"Hypervisor slcak530 Locked","Please unlock slcak530 after fixing the issue(s): Uname version lower: 2.6.18-128.2.1.5.12.el5xen, Memory Mismatch (Farm:297670,Hyper:293669)<br />",HC,OPC_QA_SHARED,Open
